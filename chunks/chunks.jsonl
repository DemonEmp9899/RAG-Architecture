{"chunk_id": "2511.05489v1.pdf__chunk_000", "doc_id": "2511.05489v1.pdf", "text": "Preprint. Under review. TIMESEARCH-R: ADAPTIVE TEMPORAL SEARCH FOR\nLONG-FORM VIDEO UNDERSTANDING\nVIA SELF-\nVERIFICATION REINFORCEMENT LEARNING\nJunwen Pan1‚àó\nQizhe Zhang1,2 ‚àó\nRui Zhang1\nMing Lu2 ‚Ä†\nXin Wan1\nYuan Zhang1,2\nChang Liu1\nQi She1 B\n1 ByteDance\n2 School of Computer Science, Peking University\n{panjunwen,sheqi.roger}@bytedance.com\nABSTRACT\nTemporal search aims to identify a minimal set of relevant frames from tens of\nthousands based on a given query, serving as a foundation for accurate long-form\nvideo understanding. Existing works attempt to progressively narrow the search\nspace. However, these approaches typically rely on a hand-crafted search process,\nlacking end-to-end optimization for learning optimal search strategies. In this paper,\nwe propose TimeSearch-R, which reformulates temporal search as interleaved\ntext-video thinking, seamlessly integrating searching video clips into the reasoning\nprocess through reinforcement learning (RL). However, applying RL training\nmethods, such as Group Relative Policy Optimization (GRPO), to video reasoning\ncan result in unsupervised intermediate search decisions. This leads to insufficient\nexploration of the video content and inconsistent logical reasoning. To address\nthese issues, we introduce GRPO with Completeness Self-Verification (GRPO-\nCSV), which gathers searched video frames from the interleaved reasoning process\nand utilizes the same policy model to verify the adequacy of searched frames,\nthereby improving the completeness of video reasoning. Additionally, we construct\ndatasets specifically designed for the SFT cold-start and RL training of GRPO-CSV,\nfiltering out samples with weak temporal dependencies to enhance task difficulty\nand improve temporal search capabilities. Extensive experiments demonstrate that\nTimeSearch-R achieves significant improvements on temporal search benchmarks\nsuch as Haystack-LVBench and Haystack-Ego4D, as well as long-form video\nunderstanding benchmarks like VideoMME and MLVU.", "start_char": 0, "end_char": 1992, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_001", "doc_id": "2511.05489v1.pdf", "text": "at\nTimeSearch-R achieves significant improvements on temporal search benchmarks\nsuch as Haystack-LVBench and Haystack-Ego4D, as well as long-form video\nunderstanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R\nestablishes a new state-of-the-art on LongVideoBench with 4.1% improvement over\nthe base model Qwen2.5-VL and 2.0% over the advanced video reasoning model\nVideo-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R. 1\nINTRODUCTION\nLong-form video understanding requires models to navigate through tens of thousands of frames\nto identify the most relevant information for answering specific questions (Fu et al., 2024; Zhou\net al., 2024; Wu et al., 2024). Temporal search lies at the heart of making long-video understanding\nboth accurate and interpretable (Park et al., 2025; Li et al., 2023; Ye et al., 2025). In contrast to the\nhuman visual system, which conducts adaptive temporal search (Yarbus, 1967; Hayhoe & Ballard,\n2005), current large video-language models (LVLMs) primarily rely on hand-crafted search strategies\nwith static frame sampling (Lin et al., 2023; Bai et al., 2025a; Feng et al., 2025). Humans naturally\nalternate between broad scanning and targeted inspection, refining their focus iteratively based on\nintermediate findings (Castelhano & Henderson, 2007; Henderson & Hayes, 2017). In contrast,\nexisting methods are limited to a fixed set of frames established before the reasoning process begins. This design presents a fundamental contradiction: video reasoning is a dynamic process where\ntemporal search interleaves with video reasoning; however, the video frames accessible to the model\nremain fixed from the outset, ultimately hindering effective reasoning. ‚àóEqual contribution. ‚Ä†Project lead. BCorresponding author. 1\narXiv:2511.05489v1 [cs.CV] 7 Nov 2025\nPreprint. Under review. Question: Did I close the drawer?", "start_char": 1792, "end_char": 3676, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_002", "doc_id": "2511.05489v1.pdf", "text": ", ultimately hindering effective reasoning. ‚àóEqual contribution. ‚Ä†Project lead. BCorresponding author. 1\narXiv:2511.05489v1 [cs.CV] 7 Nov 2025\nPreprint. Under review. Question: Did I close the drawer? To determine whether the drawer was closed, I need to identify frames \nwhere the drawer is interacted with and check its state afterward. {\"start_time\": 0, \"end_time\": 606, \"query\": \"drawer interaction\"}\n‚Ä¶ I can see the drawer being interacted with at timestamps 72s, 80‚Ä¶ \n593s. However, I need to confirm its state at the end of the video. {\"start_time\": 580, \"end_time\": 606, \"query\": \"drawer\"}\n‚ãØ\n‚Ä¶ the drawer appears to remain open in the last visible interactions \n(590s to 594s). There is no evidence of the drawer being closed‚Ä¶ \n<answer>No</answer>\nUniform \nSampling\nEnd-to-End RL Training (Ours)\n(b) Text-Video Interleaved Thinking\n(a) Paradigms of Temporal Search \nText-Video \nInterleaved Thinking\nQuery-based Retrieval\n(CLIP)\nFrame Sampling\nCaption Generation\nAction Prediction\nFrame Sampling\nVideoAgent\nQuestion Grounding\nObject Detection\n(YOLO)\nFrame Sampling\nTask Completion\nT*\nFigure 1: (a) Different paradigms of temporal search. Previous works such as VideoAgent (Wang\net al., 2024) and T* (Ye et al., 2025) predominantly rely on handcrafted workflows, resulting in\nsuboptimal strategies. Our approach adopts end-to-end reinforcement learning, enabling the model\nto learn optimal search strategies directly from data. (b) Interleaved text-video thinking process. We reformulate the temporal search task as an interleaved text-video thinking process, where the\ntemporal search is seamlessly interleaved into the reasoning process. Inspired by the gap between human cognition and model reasoning, recent studies have explored\ninteractive video agents that attempt to bridge this divide through multi-turn temporal search, as\nillustrated in Figure 1 (a).", "start_char": 3476, "end_char": 5343, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_003", "doc_id": "2511.05489v1.pdf", "text": "etween human cognition and model reasoning, recent studies have explored\ninteractive video agents that attempt to bridge this divide through multi-turn temporal search, as\nillustrated in Figure 1 (a). VideoAgent (Wang et al., 2024) first employs a large language model\n(LLM) as the central agent, which iteratively calls tools like vision-language models (VLMs) and\nCLIP (Radford et al., 2021) for frame captioning and retrieval, and then aggregates information in the\ntextual modality to perform reasoning and predict answers. T* (Ye et al., 2025) extends this paradigm\nby introducing an object-oriented spatial-temporal search. It first leverages a VLM to extract target\nobjects from the question, then employs object detection models (e.g., YOLO (Cheng et al., 2024))\nto identify keyframes containing these objects, and finally uses the retrieved frame set to complete\nthe task. Moreover, strategies that introduce tree-structured search to improve efficiency have also\nbeen explored (Wang et al., 2025; Li et al., 2025; Pan et al., 2025). However, all of these approaches\ndepend on manually designed workflows, which lead to suboptimal search strategies. This motivates us to explore an end-to-end learning approach that discovers optimal temporal search\nstrategies directly from data. In this work, we reformulate the temporal search task as an interleaved\ntext-video thinking process, and propose TIMESEARCH-R, a model that learns to actively search for\nrelevant temporal clips through reinforcement learning (RL). As shown in Figure 1 (b), our model\nalternates between textual reasoning and temporal exploration, iteratively refining its understanding\nof the video. We refer to this dynamic process as Thinking with Videos‚Äîa paradigm where models\ngradually improves their comprehension by searching for relevant video content conditioned on\nintermediate reasoning states.", "start_char": 5143, "end_char": 7021, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_004", "doc_id": "2511.05489v1.pdf", "text": "fer to this dynamic process as Thinking with Videos‚Äîa paradigm where models\ngradually improves their comprehension by searching for relevant video content conditioned on\nintermediate reasoning states. This concept extends the recent advances in multimodal reasoning,\nThinking with Images (Su et al., 2025; Hu et al., 2024; Zheng et al., 2025), to the long-video domain. Although recent works have successfully applied RL algorithms like Group Relative Policy Optimiza-\ntion (GRPO) (DeepSeek-AI, 2025) to textual (Jin et al., 2025) and spatial search (Zheng et al., 2025),\ntemporal search in videos poses unique challenges. The original GRPO rewards only the final output\nwhile ignoring intermediate search decisions, leading to several failure modes illustrated in Figure 2. The first mode, termed insufficient temporal exploration, arises because the final output reward\nprovides no incentive for comprehensive exploration of video frames. LVLMs may arrive at correct\nanswers through partial evidence or language bias without proper visual grounding (Niu et al., 2021),\n2\nPreprint. Under review. Question: How many small tin cans did the protagonist take out one \nby one while packing a notebook with \"ASTRO CARDS\" written on it? ‚Ä¶ {\"start_time\": 672, \"end_time\": 703, \n\"query\": \"small tin cans and ASTRO \nCARDS\"}\n‚Ä¶ the protagonist takes out two small tin cans during the packing \nprocess involving the \"ASTRO CARDS.\" The cans are visible \nindividually in frames 697s, 700s, and 703s. <answer>2</answer>\nThe searched video frames are insufficient \nto answer the question correctly. A missing frame. ‚Ä¶ {\"start_time\": 35, \"end_time\": 45, \"query\": \"Christmas \ntree decorated with apples, candles, and berries\"}\nQuestion: When demonstrating Christmas tree is initially decorated \nwith apples, candles and berries, which kind has the largest number? ‚Ä¶ the Christmas tree is decorated with apples, candles, and berries.", "start_char": 6821, "end_char": 8735, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_005", "doc_id": "2511.05489v1.pdf", "text": "ion: When demonstrating Christmas tree is initially decorated \nwith apples, candles and berries, which kind has the largest number? ‚Ä¶ the Christmas tree is decorated with apples, candles, and berries. The number of apples is 5, the number of candles is 5, and the \nnumber of berries is 5. Therefore, the number of each kind of \ndecoration is the same. <answer>Berries.</answer>\nThe intermediate reasoning steps output by the model are \ndisconnected from the final answer. Figure 2: Two failure modes with the original GRPO reward. Left: Insufficient temporal\nexploration. The model misses critical frames required to correctly answer the question. Right:\nInconsistent logical reasoning. The intermediate reasoning process contradicts the final answer. missing critical frames required for reliable understanding. The second mode, termed inconsistent\nlogical reasoning, emerges when models produce plausible thinking processes disconnected from\nthe final answers, a phenomenon also observed in text-only reasoning (Lanham et al., 2023). These\ntwo failure modes hinder proper temporal search and diminish the benefits of video reasoning. To address these challenges, we propose Completeness Self-Verification (CSV) as a supplement to\nthe original GRPO algorithm, providing supervision over the intermediate steps of temporal search. GRPO-CSV tackles insufficient temporal exploration by ensuring the model to acquire sufficient\nvisual evidence through self-verification, and promotes consistency between intermediate reasoning\nand the final answer by re-answering the question using the searched frames. Besides, we construct a\nhigh-quality video reasoning dataset to support GRPO-CSV training. Existing datasets contain a large\nnumber of trivial samples solvable through prue linguistic bias, as well as noisy samples that remain\nunsolvable even with extensive search, severely hindering progress in long-video reasoning.", "start_char": 8535, "end_char": 10455, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_006", "doc_id": "2511.05489v1.pdf", "text": "large\nnumber of trivial samples solvable through prue linguistic bias, as well as noisy samples that remain\nunsolvable even with extensive search, severely hindering progress in long-video reasoning. We\nimplement a two-stage data filtering pipeline to curate high-quality samples tailored to the demands\nof video reasoning, ensuring that the model learns the correct process of temporal search. We evaluate our TimeSearch-R on both temporal search and long-form video understanding tasks,\ndemonstrating its superiority in long video reasoning. On temporal search tasks, TimeSearch-R im-\nproving the temporal F1 score on Haystack-LVBench by 5.6% and the accuracy on Haystack-Ego4D\nby 8.5%, compared to the previous state-of-the-art (SOTA) method. On long-form video understand-\ning tasks, TimeSearch-R establishes new SOTA results with 4.1% improvement over the base model\nQwen2.5-VL and 2.0% over the advanced reasoning model Video-R1 on LongVideoBench. In summary, our main contributions are three-fold:\n1. We propose the TimeSearch-R framework, which reformulates temporal search as inter-\nleaved text-video thinking and learns optimal search strategies directly from data. 2. We introduce GRPO-CSV, a novel RL algorithm, which ensures sufficient and accurate\nvideo exploration by supervising the intermediate steps of temporal search. To support\nGRPO-CSV training, we also construct a high-quality video reasoning dataset via a two-\nstage filtering pipeline, enabling the model to learn correct temporal search processes. 3. Extensive experiments demonstrate the superiority of our approach on both temporal search\nand long-form video understanding. Notably, TimeSearch-R establishes a new SOTA on\nLongVideoBench, outperforming the latest reasoning model Video-R1 by 2.0%. 2\nMETHODS\nIn this section, we first reformulate the temporal search task as an interleaved text-video thinking\nprocess, enabling the model to learn optimal search strategies directly from data.", "start_char": 10255, "end_char": 12225, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_007", "doc_id": "2511.05489v1.pdf", "text": "2.0%. 2\nMETHODS\nIn this section, we first reformulate the temporal search task as an interleaved text-video thinking\nprocess, enabling the model to learn optimal search strategies directly from data. To address the\nchallenges of insufficient temporal exploration and inconsistent logical reasoning, we introduce\nGRPO-CSV as a novel RL algorithm for long videos, which ensures both sufficient and accurate\nvideo exploration by supervising intermediate steps of temporal search. Finally, we describe the\nmodel training process, including the construction of a high-quality long-video reasoning dataset. 3\nPreprint. Under review. 2.1\nTASK FORMULATION\nTemporal Search within Thinking Process. To learn optimal search strategies directly from data,\nwe reformulate temporal search as a multi-turn thinking process interleaved with video clip retrieval. Given a video V and a corresponding question Q, an initial preview ÀúV is uniformly sampled from V\nfor subsequent reasoning. At each thinking step k, the policy model œÄŒ∏ generates a textual reasoning\nTk. If Tk contains a search instruction, the video environment executes it according to frame\ntimestamps, retrieving a clip Vk ‚äÜV that is appended to the ongoing chain of thought (CoT) as input\nfor later steps. The interleaved text-video CoT at reasoning step k is formalized as:\nCk ‚âú{ (T1, V1), (T2, V2), . . . , (Tk, Vk) }. (1)\nThis interaction process repeats until the model emits the final answer A or reaches the pre-defined\nreasoning budget. For further analysis, the entire reasoning chain can be decomposed into two\ncomponents: temporal search and answer prediction, which can be formulated as:\nPŒ∏(A, C | ÀúV , Q) = PŒ∏(C | ÀúV , Q)\n|\n{z\n}\nTemporal Search\n¬∑ PŒ∏(A | C, ÀúV , Q)\n|\n{z\n}\nAnswer Prediction\n. (2)\nDynamic Video Frames. During the interleaved thinking process, the model autonomously explores\nthe video by searching for additional clips.", "start_char": 12025, "end_char": 13924, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_008", "doc_id": "2511.05489v1.pdf", "text": "al Search\n¬∑ PŒ∏(A | C, ÀúV , Q)\n|\n{z\n}\nAnswer Prediction\n. (2)\nDynamic Video Frames. During the interleaved thinking process, the model autonomously explores\nthe video by searching for additional clips. At reasoning step k, if the model outputs a search\ninstruction, it is also required to specify the temporal boundaries tk\ns and tk\ne to be explored, along\nwith a corresponding textual query qk. The video environment then executes a frame retrieval\nfunction to obtain additional F frames Vk = search(V ; tk\ns, tk\ne, qk, F) = {f 1\nk, f 2\nk, . . . , f F\nk }. This\nfunction serves as an interface to the policy model œÄŒ∏, employing a small VLM (e.g., SigLIP (Zhai\net al., 2023)) to calculate the similarity among frames within the specified temporal clip [tk\ns, tk\ne], as\nwell as the relevance with the textual query qk. The most informative F frames are then sampled\nusing determinantal point process (DPP) (Kulesza & Taskar, 2012), which has been widely used\nfor information retrieval (Chen et al., 2018; Celis et al., 2018; Sun et al., 2025). This operation\nsignificantly improves the efficiency of temporal search, and more details can be found in Section A. 2.2\nGRPO WITH COMPLETENESS SELF-VERIFICATION\nEvaluating temporal search typically requires frame-level annotations (Ye et al., 2025), which are time-\nconsuming and labor-intensive. To circumvent this challenge, previous works (Yu et al., 2025; Sun\net al., 2025) have treated downstream video understanding task as a surrogate metric for assessing the\nsearched frame set. However, these approaches are limited to selecting an optimal subset from a pre-\ndefined pool of candidate frames, lacking interaction with and exploration of the video environment. Inspired by this, we design a Completeness Self-Verification (CSV) mechanism for GRPO, which\nis annotation-free and can be seamlessly integrated into RL training, serving as a complementary to\nthe original outcome reward. The overall pipeline of GRPO-CSV is illustrated in Figure 3.", "start_char": 13724, "end_char": 15718, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_009", "doc_id": "2511.05489v1.pdf", "text": "RPO, which\nis annotation-free and can be seamlessly integrated into RL training, serving as a complementary to\nthe original outcome reward. The overall pipeline of GRPO-CSV is illustrated in Figure 3. GRPO-CSV. We introduce CSV as a complement to GRPO with only outcome rewards. During the\nGRPO rollout phase, the policy model œÄŒ∏ generates a text-video interleaved CoT C and a final answer\nA. Applying rewards only to the final answer may reduce the effectiveness of intermediate search\nprocesses. To address this, we extract the video clips from C to form a dynamic frame set Vc as the\ninput for the CSV phase. In the CSV rollout phase, the same policy model is required to re-answer\nthe question Q using only Vc, yielding a CSV answer Ac. Critically, the model is prohibited from\nany further temporal searching and must rely solely on the currently searched frames to answer the\nquestion. The CSV answer Ac is expected to remain consistent with the original answer A:\nPŒ∏(Ac | Vc, Q) ‚âàPŒ∏(A | C, ÀúV , Q). (3)\nCompleteness Reward. We design a completeness reward for the CSV phase, which is computed\nusing the original answer A, the CSV answer Ac, and the ground-truth answer A‚àóas follows:\nRc = 1[Acc(A, A‚àó) > 0.5] ¬∑ Acc(Ac, A‚àó). (4)\nwhere Acc(A, A‚àó) and Acc(Ac, A‚àó) denote the correctness scores of the original answer and the\nCSV answer, respectively, and 1[¬∑] is an indicator function activated only when the original answer A\nis correct. This conditional design ensures that the CSV reward is applied only to promising reasoning\ntrajectories, encouraging meaningful temporal search while verifying both the sufficiency of acquired\nvisual evidence and the consistency between the reasoning process and the final answer. 4\nPreprint. Under review. Policy\nModel\nFrame \nSampling\n‚ãØ\nReference \nModel\nAnswer \nVerifier\nùëÖ! ùëÖ\"\nùëÖ#\n‚ãØ\nGroup \nComputation\nùëé!", "start_char": 15518, "end_char": 17363, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_010", "doc_id": "2511.05489v1.pdf", "text": "ence and the consistency between the reasoning process and the final answer. 4\nPreprint. Under review. Policy\nModel\nFrame \nSampling\n‚ãØ\nReference \nModel\nAnswer \nVerifier\nùëÖ! ùëÖ\"\nùëÖ#\n‚ãØ\nGroup \nComputation\nùëé! ùëé\"\nùëé#\n‚ãØ\nKL\n‚ãØ\nPolicy\nModel\nAnswer \nVerifier\n+\nùëÑ\n‚ãØ\nRollout\nùëÑ\nùëâ\nCSV Answer\nGRPO\nPolicy Update\nDynamic \nFrame Set\nCSV\nCompleteness \nReward\nAdvantage\nInput\nAnswer Reward\nRollout\nText Thinking\nVideo Clips\nFinal Answer\nùëÖ! $\nùëÖ\"\n$\nùëÖ#\n$\n‚ãØ\n! Text-Video Interleaved CoT\nFigure 3: Overall pipeline of GRPO-CSV. Building upon the original GRPO, CSV extracts a\ndynamic frame set from the multi-modal CoT and constructs a vision-only CoT for re-answering. This design verifies that the searched dynamic frames provide sufficient evidence for correct reasoning,\nensuring completeness and consistency without requiring explicit frame-level supervision. Format Reward. The format reward enforces adherence to a predefined schema through-\nout the multi-turn reasoning process, validating the structural integrity of the entire tra-\njectory rather than individual steps. During reasoning, each step must follow either the\n<think>...</think><tool_call>...</tool_call> pattern for temporal search or the\n<think>...</think><answer>...</answer> pattern for the final response. We assign a\nbinary score to the full trajectory: 1 if all steps are structurally valid, and 0 otherwise. Accuracy Reward. We evaluate answer accuracy for two task types. For multiple-choice questions,\nwe extract the option letter from the model‚Äôs output and perform an exact match with the ground-truth\noption. For open-ended questions, we adopt an LLM-as-a-Judge approach (Zheng et al., 2023) to\nassess the semantic agreement between the model‚Äôs final answer and the reference answer. The scores\nfor both cases are given in binary form, with 1 indicating alignment with the standard answer. Overall Reward. The total reward is the sum of completeness, format, and accuracy components:\nR = Rc + Rfmt + Racc.", "start_char": 17163, "end_char": 19123, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_011", "doc_id": "2511.05489v1.pdf", "text": "ases are given in binary form, with 1 indicating alignment with the standard answer. Overall Reward. The total reward is the sum of completeness, format, and accuracy components:\nR = Rc + Rfmt + Racc. (5)\nThis composition encourages sufficient temporal exploration (Rc), consistent reasoning structures\n(Rfmt), and correct final answers (Racc), enhancing the model‚Äôs ability to understand long-form videos. 2.3\nMODEL TRAINING\nDataset Construction. A fundamental challenge in RL for long-video reasoning lies in the fact that\na large number of samples in the existing datasets can be solved through pure linguistic bias, reducing\nthe reliance on temporal search. Moreover, some noisy samples remain unsolvable even under\nideal temporal search, preventing the model from effectively exploring the video. To address these\nchallenges, we implement a two-stage data filtering pipeline to construct a high-quality dataset tailored\nto video reasoning. In the first stage, we remove samples that the policy model can solve correctly\nusing only 4 uniformly sampled frames, thereby discouraging reliance on linguistic shortcuts. In the\nsecond stage, we further discard samples that remain unsolvable even with multiple temporal searches\nand numerous video frames, ensuring active video exploration. Additional details of this filtering\npipeline are provided in Section B.1. And we enhance dataset diversity through the incorporation\nof samples sourced from Haystack-Ego4D (Ye et al., 2025), VideoMarathon (Lin et al., 2025), and\nCinePile (Rawal et al., 2024). A detailed analysis of the dataset is presented in Section B.2. 5\nPreprint. Under review. Table 1: Temporal search performance. We report temporal similarity, visual similarity, and\nquestion-answering (QA) accuracy on Haystack-LVBench, as well as QA accuracy on Haystack-\nEgo4D test-tiny subset. Baseline results are directly cited from Ye et al. (2025). ‚Ä† indicates the\naverage number of keyframes determined by the model adaptively.", "start_char": 18923, "end_char": 20907, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_012", "doc_id": "2511.05489v1.pdf", "text": ", as well as QA accuracy on Haystack-\nEgo4D test-tiny subset. Baseline results are directly cited from Ye et al. (2025). ‚Ä† indicates the\naverage number of keyframes determined by the model adaptively. Method\nBase Model\n# Frame\nTemporal\nVisual\nQA\nP\nR\nF1\nP\nR\nF1\nLVBench\nEgo4D\nStatic Frame Sampling\nUniform\nQwen2.5VL-7B\n8\n1.4\n6.3\n2.2\n56.0\n72.0\n62.7\n33.7\n32.0\nUniform\nGPT-4o\n8\n1.4\n6.3\n2.2\n56.0\n72.0\n62.7\n47.1\n41.5\nUniform\nGPT-4o\n32\n1.4\n24.9\n2.7\n58.7\n81.6\n67.3\n50.5\n45.5\nAdaptive Temporal Search\nVideoAgent (Wang et al., 2024)\nGPT-4\n10.1‚Ä†\n1.2\n8.5\n2.1\n58.8\n73.2\n64.7\n-\n-\nRetrieval-based (Ye et al., 2025)\nGPT-4o\n8\n1.5\n6.3\n2.3\n63.1\n65.5\n64.1\n-\n-\nT* (Ye et al., 2025)\nGPT-4o\n8\n1.6\n7.1\n2.5\n58.4\n72.7\n64.3\n51.9\n45.0\nRetrieval-based (Ye et al., 2025)\nGPT-4o\n32\n1.3\n21.8\n2.4\n59.9\n80.8\n67.8\n-\n-\nT* (Ye et al., 2025)\nGPT-4o\n32\n1.7\n28.2\n3.1\n58.3\n83.2\n67.8\n53.1\n46.5\nText-Video Interleaved Reasoning\nTimeSearch-R (Ours)\nQwen2.5VL-7B\n8.8‚Ä†\n5.4\n22.3\n8.1\n63.2\n76.4\n69.2\n52.1\n53.5\nModel Training. We employ a two-stage training scheme for our TimeSearch-R. In the first stage,\nsupervised fine-tuning (SFT) serves as a cold start, guiding the model to follow the correct reasoning\nformat and enabling effective policy learning in the subsequent RL stage. SFT training adopts\nthe above dataset construction pipeline, using GPT-4o (OpenAI, 2024) to generate the text-video\ninterleaved reasoning processes and the corresponding final answers. Following practices in the text\ndomain (Jin et al., 2025), we mask the temporal search results during training to force the model to\nlearn meaningful temporal windows and textual queries. The objective in this stage is to minimize\nthe standard cross-entropy loss over reasoning tokens, while excluding masked video tokens from\ngradient computation. Building on this cold-start, we further conduct RL post-training based on the\nproposed GRPO-CSV algorithm to stimulate the temporal reasoning capability of the model. 3\nEXPERIMENTS\n3.1\nEXPERIMENTAL SETUP\nBaselines.", "start_char": 20707, "end_char": 22688, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_013", "doc_id": "2511.05489v1.pdf", "text": "is cold-start, we further conduct RL post-training based on the\nproposed GRPO-CSV algorithm to stimulate the temporal reasoning capability of the model. 3\nEXPERIMENTS\n3.1\nEXPERIMENTAL SETUP\nBaselines. To comprehensively evaluate the effectiveness of TimeSearch-R, we compare it against\nthree types of baselines: (1) Advanced foundation models with static frame sampling, including\nboth API models (OpenAI, 2024; Team et al., 2024) and open-source models (Bai et al., 2025a). (2)\nState-of-the-art temporal search agents, such as VideoAgent (Wang et al., 2024), T* (Ye et al., 2025),\nand VideoTree (Wang et al., 2025). (3) Video reasoning models like Video-R1 (Feng et al., 2025). Datasets. We evaluate TimeSearch-R on two tasks: (1) Temporal search on Haystack-LVBench\nand Haystack-Ego4D (Ye et al., 2025), where the task is modeled as long video needle-in-a-haystack,\nmeasuring temporal and visual similarity as well as QA accuracy. (2) Long-form video understanding\non VideoMME (Fu et al., 2024), MLVU (Zhou et al., 2024), and LongVideoBench (Wu et al., 2024). Evaluation Metrics. Besides the original metrics used in the benchmarks, we additionally introduce\ntwo metrics to assess the quality of the text-video interleaved thinking process for ablation study. Among them, completeness measures whether the searched frame set is sufficient for the correct\nanswer, while consistency measures the alignment between intermediate reasoning and the final\nanswer. Further details on these two metrics are provided in Section D. Implementation Details. We train TimeSearch-R based on Qwen2.5-VL-7B-Instruct (Bai et al.,\n2025a). In the RL training, we use the AdamW (Loshchilov & Hutter, 2017) optimizer with a learning\nrate of 1e-6, a KL penalty coefficient Œ≤ = 0.005, and a batch size of 4 with 8 rollouts per prompt. We\nlimit each search operation to retrieving at most 8 frames from a specified temporal clip, with up to 8\nsearch steps in total. Training is conducted on 32 A100 GPUs.", "start_char": 22488, "end_char": 24469, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_014", "doc_id": "2511.05489v1.pdf", "text": "f 4 with 8 rollouts per prompt. We\nlimit each search operation to retrieving at most 8 frames from a specified temporal clip, with up to 8\nsearch steps in total. Training is conducted on 32 A100 GPUs. See more details in Section F. 6\nPreprint. Under review. Table 2: Video understanding performance. E2E stands for end-to-end optimization. # Frame\nrepresents the number of input frames. ‚Ä† indicates the keyframes produced by temporal search. Model\nE2E\n# Frame\nVideoMME (w/o sub)\nMLVU\nLVB\nshort\nmedium\nlong\noverall\nm-avg\nval\nStatic Frame Sampling\nQwen2.5VL-7B (Bai et al., 2025b)\n‚úì\n768\n76.3\n66.0\n54.6\n65.1\n70.2\n56.0\nGPT-4o (OpenAI, 2024)\n‚úì\n384\n80.0\n70.3\n65.3\n71.9\n64.6\n66.7\nGemini-1.5-Pro (Team et al., 2024)\n‚úì\n1 fps\n81.7\n74.3\n67.4\n75.0\n‚Äì\n64.0\nAdaptive Temporal Search\nVideoAgent (GPT-4) (Wang et al., 2024)\n‚úó\n87‚Ä†\n‚Äì\n‚Äì\n49.0\n56.0\n‚Äì\n‚Äì\nVideoTree (GPT-4) (Wang et al., 2025)\n‚úó\n128‚Ä†\n67.8\n59.9\n54.2\n‚Äì\n‚Äì\n‚Äì\nT‚àó(GPT-4o) (Ye et al., 2025)\n‚úó\n32‚Ä†\n69.5\n63.5\n59.3\n64.1\n‚Äì\n‚Äì\nText-only Reasoning\nVideo-R1-7B (Feng et al., 2025)\n‚úì\n32\n71.1\n59.0\n49.4\n59.9\n61.6\n56.4\nVideo-R1-7B (Feng et al., 2025)\n‚úì\n768\n74.1\n65.1\n55.6\n65.7\n68.4\n58.1\nText-Video Interleaved Reasoning\nQwen2.5VL-7B + Search\n‚úó\n768\n53.4\n53.8\n48.2\n51.8\n58.9\n49.1\nTimeSearch-R-7B (Ours)\n‚úì\n768\n76.8\n67.1\n56.0\n66.6\n71.5\n60.1\n‚àÜ(v.s. Qwen2.5VL-7B)\n‚Äì\n‚Äì\n+0.5\n+1.1\n+1.4\n+1.5\n+1.3\n+4.1\n3.2\nMAIN RESULTS\nTemporal Search. On the temporal search task, TimeSearch-R establishes a new state-of-the-art\non LV-Haystack, as shown in Table 1. Under a budget of 8 keyframes, our method achieves an F1\nscore of 8.1 in temporal similarity, more than three times the previous best result of 2.5 obtained by\nT*. In visual similarity, TimeSearch-R reaches an F1 score of 69.2, surpassing the previous SOTA\nmethod VideoAgent by 5.5, and even outperforming the retrieval-based method and T* with larger\nkeyframe budgets.", "start_char": 24269, "end_char": 26102, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_015", "doc_id": "2511.05489v1.pdf", "text": "sual similarity, TimeSearch-R reaches an F1 score of 69.2, surpassing the previous SOTA\nmethod VideoAgent by 5.5, and even outperforming the retrieval-based method and T* with larger\nkeyframe budgets. For the needle-in-a-haystack QA, our TimeSearch-R consistently outperforms\nthe advanced API model GPT-4o, achieving 52.1% accuracy on Haystack-LVBench and 53.5% on\nHaystack-Ego4D. These results demonstrate the superiority of end-to-end learned temporal search\nstrategies over handcrafted workflows based on human heuristics. Long-Form Video Understanding. Our TimeSearch-R also achieves strong performance on the\nlong-form video understanding task, which is shown in Table 2. On VideoMME, our method reaches\nan overall accuracy of 66.6%, surpassing the base model Qwen2.5-VL by 1.5%. As the duration of\nthe video increases, our method can achieve more gains, from 0.5% on short videos to 1.4% on long\nvideos, demonstrating that temporal search becomes more valuable when the video length increases. On MLVU and LongVideoBench, TimeSearch-R achieves 71.5% and 60.1%, improving over the\nbase model by 1.3% and 4.1%, respectively. Compared with video search agents, TimeSearch-R\noutperforms VideoAgent and T* on VideoMME by 10.6% and 2.5%, highlighting the advantage\nof end-to-end optimization. Notably, our method consistently surpasses the latest video reasoning\nmodel Video-R1 across all benchmarks, validating that text-video interleaved reasoning is more\neffective than text-only reasoning for long-form video understanding. Moreover, directly applying\ntemporal search to Qwen2.5-VL through CoT prompting without additional training actually degrades\nperformance, underscoring the necessity of RL post-training with the proposed GRPO-CSV. 3.3\nABLATION STUDIES\nTraining Scheme.", "start_char": 25902, "end_char": 27681, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_016", "doc_id": "2511.05489v1.pdf", "text": "2.5-VL through CoT prompting without additional training actually degrades\nperformance, underscoring the necessity of RL post-training with the proposed GRPO-CSV. 3.3\nABLATION STUDIES\nTraining Scheme. We explore the impact of different training stages in Table 4a, from zero-shot\nCoT to SFT and finally RL, yielding two key findings: (1) SFT enables search capability: The\nmodel cannot perform the search well only through zero-shot CoT prompts. SFT allows the model\nto rapidly acquire temporal search skills, dramatically improving temporal F1 from 0.0 to 7.8 and\nsearched frame completeness from 44.2% to 60.5%. (2) RL enhances video reasoning: While RL\nprovides modest improvements to temporal similarity and search completeness, its primary advantage\nlies in boosting overall understanding performance. The post-training stage improves reasoning\nconsistency by 2.6%, which in turn raises QA accuracy from 59.2% to 66.6%. GRPO-CSV Component. We further conduct an ablation study on the components of GRPO-CSV\nin Figure 4, and obtain three key findings: (1) GRPO reduces search completeness. Without CSV\n7\nPreprint. Under review. Method\nHaystack-LVBench\nVideoMME\nP\nR\nF1\nComp. Cons. Acc. Qwen2.5-VL w/ search\n0.0\n0.0\n0.0\n44.2\n59.4\n51.8\nSFT\n7.4\n11.6\n7.8\n60.5\n69.2\n59.2\nGRPO (Before Collapse)\n5.2‚àí2.2\n18.8+7.2\n7.4‚àí0.4\n57.2‚àí3.3\n69.3+0.1\n65.1+5.9\nGRPO-CSV w/o Acc. Rwd\n6.1‚àí1.3\n19.8+8.2\n8.2+0.4\n61.2+0.7\n75.3+6.1\n64.8+5.6\nGRPO-CSV w/ Acc. Rwd\n5.4‚àí2.0\n22.3+10.7\n8.1+0.3\n60.2‚àí0.3\n71.8+2.6\n66.6+7.4\n(a) Ablation Results\n0\n50\n100 150 200 250 300 350 400\nSteps\n0\n1\n2\n3\n4\n5\n6\n7\n8\nNumber of Search Turns\nGRPO\nGRPO-CSV\nGRPO-CSV w/o Acc. Reward\n(b) Training Dynamics\nFigure 4: Ablation study of GRPO-CSV. (a) Comparison of different training schemes on temporal\nsearch and long-form video understanding. (b) When CSV is removed, training begins to collapse. The model gradually reduces the number of search calls and eventually stops searching altogether. Table 3: Ablation study of data composition.", "start_char": 27481, "end_char": 29468, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_017", "doc_id": "2511.05489v1.pdf", "text": "ng. (b) When CSV is removed, training begins to collapse. The model gradually reduces the number of search calls and eventually stops searching altogether. Table 3: Ablation study of data composition. Line 1 shows the accuracy of original Qwen2.5-VL. Ego\nExo\nFilter\nGeneral\nReasoning\nshort\nmedium\nlong\noverall\ntemporal\nspatial\naction\nobject\n‚Äì\n‚Äì\n‚Äì\n76.3\n66.0\n54.6\n65.1\n51.4\n76.8\n56.8\n59.5\n‚úì\n‚úì\n74.2\n62.7\n51.3\n62.8\n40.1\n67.9\n60.0\n56.2\n‚úì\n‚úì\n76.4\n64.7\n54.9\n65.3\n54.8\n73.2\n58.2\n59.0\n‚úì\n‚úì\n‚úì\n76.8\n67.1\n56.0\n66.6\n58.8\n75.0\n62.5\n61.9\nas a complement, GRPO drops completeness from 60.5% to 57.2% and temporal F1 from 7.8 to 7.4,\ndemonstrating that outcome-only rewards lead to insufficient temporal exploration. (2) GRPO-CSV\nimproves training stability. As illustrated in Figure4b, removing CSV causes training to collapse\naround step 300, after which the model ceases to make search calls and completeness drops to zero. (3) GRPO-CSV with accuracy reward achieves the best QA performance. While completeness\nreward alone achieves the highest completeness and consistency, it slightly reduces QA accuracy by\n0.3%. Combining GRPO-CSV with accuracy reward leads to the best overall QA performance. Data Composition. We also analyze the data composition in RL training, as shown in Table 3,\nrevealing the contributions of data filtering and domain diversity. Without data filtering, RL training\nleads to a substantial performance drop compared to the original Qwen2.5-VL. This degradation arises\nbecause linguistic biases induce zero advantage in GRPO group computation: when questions can be\ntrivially answered through linguistic shortcuts, all rollouts achieve perfect accuracy and completeness,\nyielding no learning signal and severely hindering RL efficiency and training stability. After applying\ndata filtering, the model trained solely on egocentric data recovers baseline performance, but the\nlack of diversity weakens the benefits of RL.", "start_char": 29268, "end_char": 31197, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_018", "doc_id": "2511.05489v1.pdf", "text": "ng RL efficiency and training stability. After applying\ndata filtering, the model trained solely on egocentric data recovers baseline performance, but the\nlack of diversity weakens the benefits of RL. By incorporating exocentric data to enhance domain\ndiversity, the model achieves its best general QA accuracy of 66.6%. Notably, although the training\ndata only includes general long-video QA tasks, RL training significantly boosts the model‚Äôs temporal\nand action reasoning capabilities, improving them by 7.4% and 5.7%, respectively. This remarkable\nperformance demonstrates that TimeSearch-R learns fundamental cognitive patterns through end-to-\nend policy optimization, validating the strong generalization of our proposed GRPO-CSV algorithm. 3.4\nCASE STUDIES\nWe analyze the search patterns that emerge during end-to-end RL training, demonstrating how\nthe model executes temporal search within its reasoning process in a manner analogous to human\ncognition. These search patterns exhibit adaptability and flexibility across different task types:\nHypothesis-driven search. The model formulates hypotheses based on limited context and executes\ntargeted searches to gather additional video frames as supporting evidence. (Figure 5)\nConfirmation or elimination. When the initially sampled dynamic frame set provides insufficient\nsupport for an answer, the model employs multi-faceted search strategies or elimination methods to\ncollect additional evidence and reduces uncertainties. (Figure 13 and 14)\nSequential search. The model performs segment-by-segment analysis to accomplish temporal\nreasoning tasks that require understanding sequential relationships. (Figure 15)\n8\nPreprint. Under review. Question: Why do dogs lie in a row by a window as depicted in the video? A. Begging for food. B. Preparing to sleep. C. Taking a photo with a person. D. Ready for water. <think> To answer the question, I need to identify the context in which dogs are lying in a row by the window.", "start_char": 30997, "end_char": 32975, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_019", "doc_id": "2511.05489v1.pdf", "text": "ing for food. B. Preparing to sleep. C. Taking a photo with a person. D. Ready for water. <think> To answer the question, I need to identify the context in which dogs are lying in a row by the window. I will search for frames where \ndogs are lined up near a window and analyze the surrounding environment and actions to determine the reason for their positioning. </think>\n<tool_call> {\"start_time\": 0, \"end_time\": 2370, \"query\": \"dogs lying in a row by a window\", \"num_frames\": 8} </tool_call>\n<think> From the selected frames, it is evident that the dogs are lying in a row by the window and a person is taking a photo of them. This strongly suggests that the dogs are posing for a photo. Therefore, the answer to the question is clear. </think> <answer>C</answer>\n<think> Based on the provided frames, it is clear that the dogs are lying in a row by the window in multiple scenes. In the first frame, they are \nlined up near a window, and in the subsequent frames, they are seen in similar positions. The context suggests that they are likely posing \nfor a photo with a person, as indicated by the presence of a person taking a photo in the second frame. Therefore, I will analyze the frames \nwhere the dogs are lined up by the window to determine if they are taking a photo. </think> <tool_call> {\"start_time\": 749, \"end_time\": \n2353, \"query\": \"dogs lying in a row by a window and a person taking a photo\", \"num_frames\": 8} </tool_call>\nFigure 5: Hypothesis-driven search. Given the context that dogs are lying in a row across multiple\nscenes and remain still, the model hypothesizes that they are waiting to be photographed. It then\nsearches for the person taking a photo to gather supporting evidence and provides the final answer. 4\nRELATED WORK\nTemporal Search for Long-Video Understanding.", "start_char": 32775, "end_char": 34573, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_020", "doc_id": "2511.05489v1.pdf", "text": "are waiting to be photographed. It then\nsearches for the person taking a photo to gather supporting evidence and provides the final answer. 4\nRELATED WORK\nTemporal Search for Long-Video Understanding. Traditional video understanding methods rely\non static frame sampling, such as uniform sampling or heuristic-based strategies (Li et al., 2024;\nChen et al., 2024; Bai et al., 2025a), which fail to adapt to varying information density and evolving\nreasoning contexts. Recent work has explored more sophisticated mechanisms. Similarity-based\nmethods like KeyVideoLLM (Liang et al., 2024) achieve significant compression while maintaining\nperformance , while learning-based approaches such as Frame-Voyager (Yu et al., 2025) rank frame\ncombinations based on prediction losses, emphasizing task-specific selection. Advanced semantic\nframeworks have emerged to address temporal dependencies. Logic-in-Frames (Guo et al., 2025)\ndefines logical relations including spatial co-occurrence and temporal proximity to guide dynamic\nframe sampling. T* (Ye et al., 2025) reframes temporal search as spatial search with adaptive zooming\nmechanisms. Interactive agents like VideoAgent (Wang et al., 2024) and VideoTree (Wang et al.,\n2025) enable multi-turn temporal exploration through prompt-driven orchestration. However, none of\nthe aforementioned methods adopt end-to-end optimization, resulting in suboptimal search strategies. Reinforcement Learning for Multimodal Reasoning. Recent advances have explored RL to\nenhance reasoning capabilities in LLMs. GRPO (DeepSeek-AI, 2025) demonstrates that outcome-\nbased rewards can effectively elicit complex reasoning. Search-R1 (Jin et al., 2025) extends this\nparadigm to text-based search tasks, showing that RL can facilitate adaptive information retrieval.", "start_char": 34373, "end_char": 36165, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_021", "doc_id": "2511.05489v1.pdf", "text": "e-\nbased rewards can effectively elicit complex reasoning. Search-R1 (Jin et al., 2025) extends this\nparadigm to text-based search tasks, showing that RL can facilitate adaptive information retrieval. Approaches like MM-Eureka (Meng et al., 2025) and LMM-R1 (Peng et al., 2025) have successfully\napplied RL to enhance multimodal reasoning, but focus primarily on static image understanding rather\nthan dynamic video interaction. Video-R1 (Feng et al., 2025) applies GRPO to video reasoning but\nlimits the thinking process to prue text without visual interaction, while DeepEyes (Zheng et al., 2025)\nuses RL for high-resolution image understanding through adaptive cropping operations but focuses\non spatial rather than temporal exploration. Despite these advances, applying RL to interactive long\nvideo understanding remains largely unexplored and presents unique challenges. 5\nCONCLUSION\nIn this work, we propose TimeSearch-R, a framework that reformulates temporal search as text-video\ninterleaved thinking to learn optimal search strategies directly from data. To enhance temporal search\nthrough RL, we propose CSV as a complement to the outcome-only reward of GRPO, addressing the\nchallenges of insufficient temporal exploration and inconsistent logical reasoning. TimeSearch-R\nachieves strong performance on both temporal search and long-form video understanding tasks,\nwhile exhibiting distinct search patterns across different task types. We hope this work contributes\nmeaningful progress toward advancing long video understanding powered by reinforcement learning. 9\nPreprint. Under review. REFERENCES\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan,\nPengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025a.", "start_char": 35965, "end_char": 37937, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_022", "doc_id": "2511.05489v1.pdf", "text": "Li, Jianqiang Wan,\nPengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025a. URL https://arxiv.org/abs/2502.13923. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923,\n2025b. Monica S. Castelhano and John M. Henderson. Initial scene representations facilitate eye move-\nment guidance in visual search. Journal of Experimental Psychology: Human Perception and\nPerformance, 33(4):753‚Äì763, 2007. doi: 10.1037/0096-1523.33.4.753. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria, and Nisheeth Vishnoi. Fair and diverse dpp-based data summarization. In International conference on machine learning,\npp. 716‚Äì725. PMLR, 2018. Laming Chen, Guoxin Zhang, and Eric Zhou. Fast greedy map inference for determinantal point\nprocess to improve recommendation diversity. Advances in Neural Information Processing Systems,\n31, 2018. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 24185‚Äì24198, 2024. Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world:\nReal-time open-vocabulary object detection. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 16901‚Äì16911, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\n2025. URL https://arxiv.org/abs/2501.12948. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou\nWang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms.", "start_char": 37737, "end_char": 39706, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_023", "doc_id": "2511.05489v1.pdf", "text": "2025. URL https://arxiv.org/abs/2501.12948. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou\nWang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint\narXiv:2503.21776, 2025. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu\nZhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation\nbenchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Yifan Guo, Liqiang Zou, Yang Li, Jia Chen, and Joey Tianyi Zhou. Logic-in-frames: Logical depen-\ndency modeling for frame selection in video question answering. arXiv preprint arXiv:2501.00212,\n2025. Mary M. Hayhoe and Dana H. Ballard. Eye movements in natural behavior. Trends in Cognitive\nSciences, 9(4):188‚Äì194, 2005. doi: 10.1016/j.tics.2005.02.009. John M. Henderson and Taylor R. Hayes. Meaning-based guidance of attention in scenes: Evidence\nfrom meaning maps. Journal of Vision, 17(6):23, 2017. doi: 10.1167/17.6.23. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and\nRanjay Krishna. Visual sketchpad: Sketching as a visual chain of thought for multimodal language\nmodels. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=GNSMl1P5VR. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and\nJiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement\nlearning. arXiv preprint arXiv:2503.09516, 2025. 10\nPreprint. Under review. Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Found. Trends\nMach. Learn., 5(2-3):123‚Äì286, 2012. doi: 10.1561/2200000044. URL https://doi.org/\n10.1561/2200000044.", "start_char": 39506, "end_char": 41336, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_024", "doc_id": "2511.05489v1.pdf", "text": "ew. Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Found. Trends\nMach. Learn., 5(2-3):123‚Äì286, 2012. doi: 10.1561/2200000044. URL https://doi.org/\n10.1561/2200000044. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Her-\nnandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina\nNguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam\nMcCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy\nMaxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner,\nSamuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-of-thought reasoning. CoRR, abs/2307.13702, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei\nLi, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint\narXiv:2408.03326, 2024. Chenglin Li, Qianglong Chen, Yin Zhang, et al. Iterative zoom-in: Temporal interval exploration for\nlong video understanding. arXiv preprint arXiv:2507.02946, 2025. Yicong Li, Junbin Xiao, Chun Feng, Xiang Wang, and Tat-Seng Chua. Discovering spatio-temporal\nrationales for video question answering. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 13869‚Äì13878, 2023. Dengxin Liang, Yixiao Shu, Ruobing Zhang, Songyang Chen, Xuanmo Li, and Minheng Wang. Keyvideollm: Towards large-scale video keyframe selection. arXiv preprint arXiv:2407.03104,\n2024. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual\nrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Hao Chen, Jiebo Luo, Zicheng Liu,\nand Emad Barsoum. Unleashing hour-scale video training for long video-language understanding. arXiv preprint arXiv:2506.05332, 2025. Ilya Loshchilov and Frank Hutter.", "start_char": 41136, "end_char": 43129, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_025", "doc_id": "2511.05489v1.pdf", "text": "Hao Chen, Jiebo Luo, Zicheng Liu,\nand Emad Barsoum. Unleashing hour-scale video training for long video-language understanding. arXiv preprint arXiv:2506.05332, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng\nHan, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang,\nand Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based\nreinforcement learning, 2025. URL https://arxiv.org/abs/2503.07365. Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counter-\nfactual vqa: A cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 12700‚Äì12710, 2021. OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024. Junwen Pan, Rui Zhang, Xin Wan, Yuan Zhang, Ming Lu, and Qi She. Timesearch: Hierarchical\nvideo search with spotlight and reflection for human-like long video understanding. arXiv preprint\narXiv:2504.01407, 2025. Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryu, Donghyun Kim, and\nMichael S. Ryoo. Too many frames, not all useful: Efficient strategies for long-form video qa,\n2025. URL https://arxiv.org/abs/2406.09396. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang,\nXingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning\nabilities through two-stage rule-based rl, 2025. URL https://arxiv.org/abs/2503. 07536. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 11\nPreprint. Under review.", "start_char": 42929, "end_char": 44836, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_026", "doc_id": "2511.05489v1.pdf", "text": "Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 11\nPreprint. Under review. Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: A long video question answering dataset and benchmark. CoRR, abs/2405.08813, 2024. Zhaochen Su, Peng Xiang, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu\nLi, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, and Yi R. (May)\nFung. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. CoRR, abs/2506.23918, 2025. doi: 10.48550/ARXIV.2506.23918. URL https://doi.org/\n10.48550/arXiv.2506.23918. Hui Sun, Shiyin Lu, Huanyu Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Ming\nLi. Mdp3: A training-free approach for list-wise frame selection in video-llms. arXiv preprint\narXiv:2501.02885, 2025. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett\nTanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Leandro von Werra, Younes Belkada, Lewis Tunstall, and Hugging Face. Trl: Transformer reinforce-\nment learning. https://github.com/huggingface/trl, 2020. Accessed: 2025-09-16. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video\nunderstanding with large language model as agent. ECCV, 2024. Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and\nMohit Bansal. Videotree: Adaptive tree-based video representation for llm reasoning on long\nvideos, 2025. URL https://arxiv.org/abs/2405.19209. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context\ninterleaved video-language understanding, 2024. Alfred L. Yarbus.", "start_char": 44636, "end_char": 46612, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_027", "doc_id": "2511.05489v1.pdf", "text": "25. URL https://arxiv.org/abs/2405.19209. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context\ninterleaved video-language understanding, 2024. Alfred L. Yarbus. Eye Movements and Vision. Plenum Press, New York, 1967. Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyza-\nguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, et al. Re-thinking temporal\nsearch for long-form video understanding. In CVPR, pp. 8579‚Äì8591, 2025. Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu,\nZhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, and Qianru Sun. Frame-voyager: Learning\nto query frames for video large language models, 2025. URL https://arxiv.org/abs/\n2410.03226. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\nimage pre-training. In Proceedings of the IEEE/CVF international conference on computer vision,\npp. 11975‚Äì11986, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir\nGloberson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing\nYu. Deepeyes: Incentivizing \"thinking with images\" via reinforcement learning. arXiv preprint\narXiv:2505.14362, 2025. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang,\nTiejun Huang, and Zheng Liu. Mlvu: A comprehensive benchmark for multi-task long video\nunderstanding. arXiv preprint arXiv:2406.04264, 2024. 12\nPreprint. Under review.", "start_char": 46412, "end_char": 48404, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_028", "doc_id": "2511.05489v1.pdf", "text": "Yang, Yongping Xiong, Bo Zhang,\nTiejun Huang, and Zheng Liu. Mlvu: A comprehensive benchmark for multi-task long video\nunderstanding. arXiv preprint arXiv:2406.04264, 2024. 12\nPreprint. Under review. TIMESEARCH-R: ADAPTIVE TEMPORAL SEARCH FOR\nLONG-FORM VIDEO UNDERSTANDING\nVIA SELF-\nVERIFICATION REINFORCEMENT LEARNING\nAPPENDIX\nThis appendix provides more details about our methods, dataset, training, more case studies, broader\nimpacts, as well as the LLM usage, organized as follows:\n‚Ä¢ Section A: Search Function\n‚Ä¢ Section B: Dataset Details\n‚Ä¢ Section C: Prompt Design\n‚Ä¢ Section D: Evaluation Metrics\n‚Ä¢ Section E: Efficiency Analysis\n‚Ä¢ Section F: Training Details\n‚Ä¢ Section G: More Case Studies\n‚Ä¢ Section H: Boarder Impacts\n‚Ä¢ Section I: LLM Usage\nA\nSEARCH FUNCTION\nA.1\nFRAME SELECTION\nThe video search function selects the most informative frames within predicted temporal clips. Specifically, we leverage determinantal point process (DPP) (Kulesza & Taskar, 2012) as the search\noptimization for its ability to naturally balance query relevance and diversity that penalizes redundancy,\nwhich has been widely applied in information retrieval (Celis et al., 2018; Sun et al., 2025). Recall the definition of search in Sec. 2.1, it aims to select F optimal frames guided by a temporal\nclip [ts, te] and a query q from the original video V . First, the function first subsamples N candidate\nframes F[ts,te] = {vi}N\ni=1 within the temporal clip. Subsequently, we obtain a visual embedding\nhi ‚ààRd for each candidate frame in F[ts,te], and a query embedding q ‚ààRd for q. Then we define the\npairwise cosine similarity for candidate frames as Sij = h‚ä§\ni hj and compute an unnormalized query\nrelevance score for each frame as Àúri = q‚ä§hi, which is rescaled to [0, 1] by min-max normalization\nri =\nÀúri‚àímin Àúr\nmax Àúr‚àímin Àúr+œµ, where œµ is a small constant to avoid division by zero.", "start_char": 48204, "end_char": 50075, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_029", "doc_id": "2511.05489v1.pdf", "text": "malized query\nrelevance score for each frame as Àúri = q‚ä§hi, which is rescaled to [0, 1] by min-max normalization\nri =\nÀúri‚àímin Àúr\nmax Àúr‚àímin Àúr+œµ, where œµ is a small constant to avoid division by zero. The kernel is constructed\nby diagonal conditioning with these relevance weights:\nÀúL = diag(r) S diag(r),\n(6)\nwhich is equivalent to ÀúLij = rirj h‚ä§\ni hj. The optimal subset V ‚àó‚äÇF[ts,te] with |V ‚àó| = F is then\nobtained through fast greedy MAP inference (Chen et al., 2018):\nV ‚àó= arg\nmax\nS‚äÜF[ts,te],|S|=F det(ÀúLS). (7)\nThis formulation ensures that selected frames are both diverse and relevant to the query. When\navailable frames are fewer than F, the search function degrades to uniform temporal sampling. A.2\nFRAME REPRESENTATION\nThe selected clip frames are sparse and non-uniform. To maintain the temporal pace, we attach an\nexplicit absolute timestamp to each frame by inserting a short text token with the time in seconds\n(e.g., ‚Äú12.3s‚Äù) immediately before the image. This simple interleaving of timestamp text and the\ncorresponding image maintains absolute temporal grounding when inter-frame intervals vary and\n13\nPreprint. Under review. Stage-1\nDirect\nAnswer\n4 frames\nLLM as a Judge\nCorrect? CoT &\nAnswer\n64 frames\nLLM as a Judge\nCorrect? Stage-2\nFor SFT\nFor RL\nSearch\nN\nY\nN\nHuman selection\n(Only for synthetic noisy QA pairs)\nQwen2.5-VL\nTimeSearch-R \n(Early version)\nGPT-4o\nFigure 6: Illustration of the proposed two-stage data filtering pipeline. complements the native temporal ids. Explicit absolute timestamp augmented frame representation\nhas also been observed to improve temporal capability in prior work on long-video temporal ground-\ning (Pan et al., 2025). For uniformly sampled preview frames, we employ the native dynamic-FPS\nand absolute time encoding following Qwen2.5-VL (Bai et al., 2025a), which bind image token\nsequences to temporal ids aligned with real absolute timestamps.", "start_char": 49875, "end_char": 51778, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_030", "doc_id": "2511.05489v1.pdf", "text": "ew frames, we employ the native dynamic-FPS\nand absolute time encoding following Qwen2.5-VL (Bai et al., 2025a), which bind image token\nsequences to temporal ids aligned with real absolute timestamps. B\nDATASET DETAILS\nB.1\nDATASET CONSTRUCTION\nTo ensure high-quality training data, we implement a two-stage filtering pipeline as shown in Fig. 6. Stage 1: Visual Dependency Filtering. We uniformly sample 4 frames from each video and\nfeed them along with the question to Qwen2.5-VL for inference. Questions that can be correctly\nanswered with this limited visual information are considered to have low visual dependency and are\nsubsequently filtered out. Only questions requiring richer visual context proceed to the next stage. Stage 2: Search Usefulness Filtering. We increase the frame input to up to 64 frames and em-\nploy different LVLMs to perform dynamic temporal search for question-relevant video segments. Specifically, we use GPT-4o to generate SFT (Supervised Fine-Tuning) data and an early version\nof TimeSearch to obtain RL (Reinforcement Learning) training data. Although this stage produces\nCoT, only the CoT generated by GPT-4o is used for SFT training, while RL training utilizes only the\nquestion-answer pairs. To avoid search format errors, we implement format validation for LVLMs‚Äô\nresponses, automatically retrying the model until obtaining properly formatted answers. Human Selection for VideoMarathon (Panda-70M). Given that VideoMarathon‚Äôs training set\ncontains automatically generated question-answer pairs with potential unanswerable questions or\nincorrect ground-truth answers, we conduct manual annotation to ensure data quality. To minimize\nannotator bias in model answer evaluation, we establish a structured annotation protocol. First,\nannotators assess question reasonableness based on video content, filtering out unanswerable or\nambiguous questions.", "start_char": 51578, "end_char": 53461, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_031", "doc_id": "2511.05489v1.pdf", "text": "s in model answer evaluation, we establish a structured annotation protocol. First,\nannotators assess question reasonableness based on video content, filtering out unanswerable or\nambiguous questions. Subsequently, annotators provide manual answers and compare them with\nsynthetic ground-truth labels, removing data samples that are inconsistent with human responses. B.2\nDATASET ANALYSIS\nThe dataset exhibits a pronounced long-tail distribution in video duration with a mean length of 1,659\nseconds. Most videos are shorter than 2,000 seconds, while a nontrivial tail extends beyond one hour,\nposing significant challenges for static frame sampling. This distribution motivates adaptive temporal\nsearch and multi-turn interaction to progressively retrieve evidence under tight keyframe budgets. 14\nPreprint. Under review. 0\n2000\n4000\n6000\n8000\n10000\nDuration (seconds)\n0\n100\n200\n300\n400\n500\n600\n700\n800\nFrequency\nVideo Duration Distribution\nMean: 1659s\nEgo4D\n49.5%\nOthers\n5.4%\nCinePile\n9.5%\nPanda-70M\n35.6%\nData Source Distribution\nmultiple_choice\n39.7%\nopen_ended\n60.3%\nQuestion Type Distribution\nFigure 7: Dataset analysis. (1) The training set is mainly composed of long videos. The average\nlength is 1659 seconds, and the maximum length exceeds 10,000 seconds. (2) Egocentric QA pairs\ncome from Haystack-Ego4D, and Exocentric QA data mainly from VideoMarathon and Cinepile,\nwhere VideoMarathon employs Panda-70M as the video source. (3) Question types include multiple-\nchoice and open-ended questions. To obtain open-ended QA pairs, we convert some multiple-choice\ntasks into open-ended questions. We curate data from four major sources to ensure coverage of diverse visual domains and camera\nstyles. As shown in Fig. 7, Ego4D from Haystack-Ego4D (Ye et al., 2025) training set contributes\n49.5% of samples, providing egocentric daily activities with frequent viewpoint changes.", "start_char": 53261, "end_char": 55145, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_032", "doc_id": "2511.05489v1.pdf", "text": "ins and camera\nstyles. As shown in Fig. 7, Ego4D from Haystack-Ego4D (Ye et al., 2025) training set contributes\n49.5% of samples, providing egocentric daily activities with frequent viewpoint changes. Panda-70M\nfrom VideoMarathon (Lin et al., 2025) accounts for 35.6%, expanding the variety of internet videos\nwith heterogeneous motion patterns and scene dynamics. CinePile (Rawal et al., 2024) provides 9.5%\nof short videos with narrative structure and rapid scene transitions. The remaining 5.4% are from\nother sources and serve to reduce distributional bias. Question types are intentionally imbalanced toward open-ended reasoning to better evaluate generative\ncapabilities. Open-ended questions make up 60.3% of the data and emphasize step-by-step analysis,\ntemporal grounding, and explanation quality. Multiple-choice questions comprise 39.7% and offer\nreliable automatic evaluation signals that complement outcome rewards in RL. This composition yields wide coverage over motion intensity, scene diversity, and narrative structure\nwhile maintaining sufficient automatic evaluability. The mixture of long-tail durations and open-\nended questions creates a setting where end-to-end RL and adaptive temporal search offer clear\nbenefits over single-shot heuristics. 15\nPreprint. Under review. C\nPROMPT DESIGN\nWe design prompts to standardize interaction formats, minimize ambiguity, and provide explicit\npriors for temporal reasoning. Fig. 8‚Äì11 show the templates used during training and evaluation. System Prompt. We follow the tool-use specification of the base Qwen2.5-VL family (Bai et al.,\n2025a) and adopt its tool_call schema for invoking temporal search. This design ensures\ndeterministic parsing by the environment and stable credit assignment for RL, as illustrated in Fig. 8. System Prompt\nYou are a helpful video assistant. # Tools\nYou may call one or more functions to assist with the user query.", "start_char": 54945, "end_char": 56857, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_033", "doc_id": "2511.05489v1.pdf", "text": "he environment and stable credit assignment for RL, as illustrated in Fig. 8. System Prompt\nYou are a helpful video assistant. # Tools\nYou may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{\"type\": \"function\", \"function\": {\"name\": \"seek_video_frames\", \"description\": \"Search and\nselect video frames according to textual query and temporal window. Time is in seconds.\",\n\"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\":\n\"The query is used to describe the object, scene, or event of interest in the video\nthoroughly and clearly. \"}, \"start_time\": {\"type\": \"number\", \"description\": \"Start time of\nthe segment of interest. \"}, \"end_time\": {\"type\": \"number\", \"description\": \"End time of\nthe segment of interest. \"}, \"num_frames\": {\"type\": \"integer\", \"description\": \"Number of\nframes to sample (maximum 8). Default is 8.\"}}, \"required\": [\"query\"]}}}\n</tools>\nFor each function call, return a json object with function name and arguments within <\ntool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call>\nFigure 8: The system prompt with tools. Question Answering Prompt. The QA template enforces thorough reasoning inside <think>\nbefore any tool call or final answer. It restricts the output to exactly one of two formats and allows at\nmost eight rounds of <tool_call>. It explicitly provides the line \"The video duration: {duration}\nseconds.\" to help the model produce absolute timestamps better. See Fig. 9. Question Answering\nYou must ALWAYS conduct thorough reasoning inside <think> and </think> tags BEFORE calling\nany tool or answering the question. You must invoke tools to explore any video content you are interested in within <tool_call\n> </tool_call> tags. You are allowed to use <tool_call></tool_call> tags for a maximum of 8 rounds.", "start_char": 56657, "end_char": 58592, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_034", "doc_id": "2511.05489v1.pdf", "text": "question. You must invoke tools to explore any video content you are interested in within <tool_call\n> </tool_call> tags. You are allowed to use <tool_call></tool_call> tags for a maximum of 8 rounds. When you have enough information to answer the question, provide your answer within <\nanswer> </answer> tags. Your answer should be supported by evidence from the video. Your output must follow the format: <think>Your reasoning process</think><tool_call>\nParameters</tool_call> or <think>Your reasoning process</think><answer>Your answer</answer\n>Question: {question}\nThe video duration: {duration} seconds. Figure 9: The template for question answering. Clip Frame Sampling and Search Response. After a search, the template returns the selected\nframes and their corresponding timestamps. If the frames are sufficient, the model must place the final\nanswer in <answer>. Otherwise, the template asks the model to call the tool again with different\nparameters in JSON, thereby encouraging reflection and re-query. See Fig. 10. 16\nPreprint. Under review. Temporal Search Response\nHere are selected frames. They are located at {timestamps}. If the frames provided above are sufficient to answer the user‚Äôs question, please put your\nfinal answer within <answer></answer>. Otherwise invoke the tool again with different parameters in JSON format. Figure 10: The response template of the temporal search. Completeness Self-Verification Prompt. The CSV template asks the model to answer as briefly as\npossible and to say \"I don‚Äôt know\" when the visual evidence is insufficient. No tools are available in\nthis stage, which prevents new searches and ensures the answer is grounded only on the dynamic\nframe set gathered earlier. See Fig. 11. Completeness Self-Verification\nYou are a helpful assistant. Please answer visual questions as briefly as possible. When\nyou don‚Äôt have enough visual information, please say ‚ÄôI don‚Äôt know‚Äô. Figure 11: The template for CSV reasoning.", "start_char": 58392, "end_char": 60356, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_035", "doc_id": "2511.05489v1.pdf", "text": "You are a helpful assistant. Please answer visual questions as briefly as possible. When\nyou don‚Äôt have enough visual information, please say ‚ÄôI don‚Äôt know‚Äô. Figure 11: The template for CSV reasoning. D\nEVALUATION METRICS\nCompleteness Rate. We measure the proportion of cases where the dynamic visual context alone\nsuffices to produce the correct answer. Concretely, after the multi-turn search, we re-answer the\nquestion using only the gathered dynamic frame set and disallow further search, following the CSV\nprocedure in Sec. 2.2 and the prompt illustrated in Fig. 11. The resulting correctness is computed\nwith the same task-specific accuracy used elsewhere, averaged over the whole dataset. Consistency Rate. Consistency evaluates whether the intermediate reasoning coherently sup-\nports the final answer under the given question. We prompt a LLM model (GPT-4o) with the\nquestion, the reasoning text extracted from <think>...</think>, and the final answer from\n<answer>...</answer>, using the format in Fig. 12 that requires a structured output: a short\nanalysis in <think> followed by <answer> equal to ‚ÄúYes‚Äù or ‚ÄúNo‚Äù. In implementation, we parse\nthe LLM‚Äôs output to obtain the binary decision; ‚ÄúYes‚Äù is counted as 1 and ‚ÄúNo‚Äù as 0, and any parsing\nfailure is treated as 0. The Consistency Rate is the dataset average of these binary outcomes. Consistency Score Evaluation\n<system prompt>\nYou are a careful and logical reviewer. Your task is to verify whether the given reasoning\nprocess and the final answer are consistent in addressing the given question. Please carefully read the following information:\nQuestion: <Question>\nReasoning Process: <Reasoning>\nFinal Answer: <Answer>\nPlease follow this format strictly:\n<think> Your analysis here </think> <answer> Yes/No </answer>\nFigure 12: The template for calculating consistency. 17\nPreprint. Under review. E\nEFFICIENCY ANALYSIS\nTable 4: Efficiency evaluation on Haystack-Ego4D. Baseline results are directly cited from Ye et al. (2025).", "start_char": 60156, "end_char": 62150, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_036", "doc_id": "2511.05489v1.pdf", "text": "The template for calculating consistency. 17\nPreprint. Under review. E\nEFFICIENCY ANALYSIS\nTable 4: Efficiency evaluation on Haystack-Ego4D. Baseline results are directly cited from Ye et al. (2025). We report the overall latency of temporal search and answering. Evaluations are conducted\non the Haystack-Ego4D using A100 GPUs. Temporal search metrics are reported in Tab. 1. Method\nQuestion Grounding\nFrame Retrieval\nLatency (sec) ‚Üì\nVideoAgent\nGPT4\nCLIP-1B\n34.9\nRetrieval-based\n‚Äì\nYOLO-world-110M\n32.2\nT‚àó(Detector-based)\nLLaVA-OV-7B\nYOLO-world-110M\n11.1\nTimeSearch-R\n‚Äì\nSigLIP-400M\n13.4\nTimeSearch-R attains an end-to-end latency of 13.4 seconds on the Haystack-Ego4D test set, yielding\na 61.6% speed-up over the 34.9-second latency of VideoAgent. Despite T‚àóoperating with the\nlightweight YOLO-World-110M detector and completing inference in 11.1 seconds, our method\nmaintains a comparable runtime while avoiding the complexity of hand-crafted scheduling. As shown\nin Tab. 1, TimeSearch-R markedly surpasses these baselines in temporal search metrics and QA\naccuracy, underscoring the effectiveness of reinforcement-driven temporal policies. F\nTRAINING DETAILS\nTable 5: Training hyperparameters of TimeSearch-R. Category\nParameter\nValue\nVideo Processing\nMax FPS\n2\nMax Frames per Video\n768\nTotal Video Tokens\n10,240\nMin Tokens per Frame\n12\nMax Tokens per Frame\n256\nInteraction Settings\nMax Search Turns\n8\nMax Completion Length per Turn\n256\nGRPO Training\nNumber of Generations\n8\nKL Penalty Coefficient (Œ≤)\n0.005\nScale Rewards\nfalse\nBatch Size per GPU\n1\nGradient Accumulation Steps\n2\nInfrastructure\nDeepSpeed Configuration\nZeRO-3 Offload\nVLLM Mode\ncolocate\nReplay Buffer\ntrue\nWe summarize the key hyperparameters in Table 5 for reproducibility. Training Configuration. TimeSearch-R employs a distributed training setup using PyTorch‚Äôs\nnative distributed data parallel framework with ZeRO-3 memory optimization through DeepSpeed.", "start_char": 61950, "end_char": 63876, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_037", "doc_id": "2511.05489v1.pdf", "text": "reproducibility. Training Configuration. TimeSearch-R employs a distributed training setup using PyTorch‚Äôs\nnative distributed data parallel framework with ZeRO-3 memory optimization through DeepSpeed. The training process leverages gradient accumulation to simulate larger batch sizes while maintaining\nmemory efficiency on GPU clusters. We utilize mixed precision training with bfloat16 to accelerate\ncomputation while preserving numerical stability, coupled with Flash Attention 2.0 for efficient\nattention computation. GRPO Training Setup. The reinforcement learning phase uses Group Relative Policy Optimization\nwith 8 generations per prompt to provide sufficient policy gradient estimates. The KL divergence\npenalty coefficient Œ≤ is set to 0.005 to balance between reward optimization and policy regularization. We employ VLLM in colocate mode for efficient inference during rollout generation, enabling faster\n18\nPreprint. Under review. policy updates. This RL training stage is implemented on top of the TRL library (von Werra et al.,\n2020), following standard practice for outcome-driven policy optimization in large language models. Video Processing Configuration. The model processes videos with a maximum of 768 frames\nand allocates up to 10,240 tokens for video content representation. Each interaction turn is limited to\n8 search operations, with a maximum of 8 interaction turns per question to ensure comprehensive\ntemporal exploration while maintaining computational efficiency. Frame tokens are dynamically\nallocated between 12 and 256 tokens per frame based on content complexity and relevance. G\nMORE CASE STUDIES\nThis section provides more case studies of TimeSearch-R, including successful cases and failed cases. Successful Cases. These representative success cases illustrate how TimeSearch-R conducts multi-\nturn exploration to accumulate decisive visual evidence while maintaining alignment between the\nreasoning trace and the final answer. They encompass confirmation (Fig.", "start_char": 63676, "end_char": 65675, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_038", "doc_id": "2511.05489v1.pdf", "text": "how TimeSearch-R conducts multi-\nturn exploration to accumulate decisive visual evidence while maintaining alignment between the\nreasoning trace and the final answer. They encompass confirmation (Fig. 13), elimination (Fig. 14),\nand sequential exploration patterns (Fig. 15), collectively demonstrating that the policy preserves the\nhigh completeness and consistency reported in Sec. D. Failed Cases. Figure 16 illustrates a residual failure where the policy halts after reviewing only\ntwo of four candidate segments, leading to an incorrect answer. Figure 17 illustrates a failure where\nthe model hallucinates information related to riding. H\nBROADER IMPACTS\nTimeSearch-R contributes to several important areas beyond the immediate technical contributions:\nAdvancing Video Interpretability and Explainability. TimeSearch-R introduces interleaved\ntext-video reasoning traces that provide transparent insights into the model‚Äôs decision-making process. The completeness and consistency criteria we propose enable quantitative assessment of long-form\nvideo explanations, making temporal search decisions auditable and interpretable. This advance-\nment represents a significant step toward more explainable AI systems in the video domain, where\nunderstanding the reasoning process is crucial for building trust and ensuring reliability. Transforming Video Reasoning from Static to Dynamic Paradigms. Our approach fundamen-\ntally shifts the paradigm from static frame sampling to dynamic, interactive reasoning in video\nunderstanding. By operationalizing hypothesis-driven exploration through iterative temporal search,\nwe promote a new methodology that emphasizes transparent, stepwise evidence gathering. This con-\ntrasts sharply with traditional one-shot inference over fixed visual contexts, encouraging researchers\nto develop more adaptive and interactive AI systems.", "start_char": 65475, "end_char": 67342, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_039", "doc_id": "2511.05489v1.pdf", "text": "rent, stepwise evidence gathering. This con-\ntrasts sharply with traditional one-shot inference over fixed visual contexts, encouraging researchers\nto develop more adaptive and interactive AI systems. The demonstrated effectiveness of our approach\nmay inspire broader adoption of similar interactive paradigms across various multimodal tasks. Exploring Scalable Weakly-Supervised Process Rewards. We introduce outcome-based process\nsupervision that eliminates the need for costly process annotations. Through the integration of weak\nsupervision and reinforcement learning via completeness self-verification, our method successfully\naligns intermediate search decisions with correct outcomes. This approach offers a scalable solution\nfor training complex interactive systems without requiring fine-grained procedural labels, potentially\nreducing annotation costs and enabling broader application across diverse domains. I\nTHE USE OF LARGE LANGUAGE MODELS\nThe authors declare that Large Language Models were used in this paper for polishing the writing. Specifically, the LLM assisted with tasks such as grammar checking, sentence simplification, and\nimproving the overall fluency of the text. It is important to note that the LLM was not used for any\nliterature review or research ideation. All research ideas and experimental analyses presented in the\npaper were solely conducted by the authors. 19\nPreprint. Under review. Question: What is the pregnant woman wearing in the video? A. A strapless, form-fitting dress adorned with sequins. B. A mint green dress with a strapless design. C. An elegant black dress with long sleeves. D. A striking green dress with a large, ruffled detail. <think>To answer the question, I need to identify the pregnant woman in the video and determine what she is wearing. I will search for frames \nwhere a pregnant woman is featured and analyze the attire.", "start_char": 67142, "end_char": 69030, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_040", "doc_id": "2511.05489v1.pdf", "text": "think>To answer the question, I need to identify the pregnant woman in the video and determine what she is wearing. I will search for frames \nwhere a pregnant woman is featured and analyze the attire. </think>\n<tool_call>{\"start_time\": 0, \"end_time\": 115, \"query\": \"pregnant woman on red carpet\", \"num_frames\": 8} </tool_call>\n<think> Based on the provided frames, the pregnant woman is consistently wearing a black dress with long sleeves. I will now provide the \nanswer based on the evidence. </think>\n<answer>C</answer>\n<think> Based on the provided frames, the pregnant woman is consistently wearing a black dress with long sleeves. I will now analyze the \nframes to confirm this. </think>\n<tool_call>{\"start_time\": 0, \"end_time\": 115, \"query\": \"black dress with long sleeves\", \"num_frames\": 8} </tool_call>\nFigure 13: Search pattern: search confirmation. Question: Based on the visual cues provided by the video, where did not the magician perform his magic? A. On the street. B. In a church. C. In a hospital. D. On a small boat on the lake. <think>To determine where the magician did not perform his magic, I need to analyze the video frames for scenes where magic is performed. I \nwill search for frames where the magician is shown performing tricks and identify the locations. I will then compare these locations to the given \noptions to find the one that is not present in the video. </think>\n<tool_call>{\"start_time\": 0, \"end_time\": 83, \"query\": \"magic performance\", \"num_frames\": 10} </tool_call>\n<think> Based on the selected frames, the magician did not perform magic in a hospital setting. The frames clearly show the magician \nperforming in various locations such as a yoga studio, a restaurant, a park, and a studio, but not in a hospital. Therefore, the answer to the \nquestion is clear. </think>\n<answer>C</answer>\n<think> Based on the selected frames, the magician performed magic in various locations such as a yoga studio, a restaurant, a park, and \na studio.", "start_char": 68830, "end_char": 70811, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_041", "doc_id": "2511.05489v1.pdf", "text": "the \nquestion is clear. </think>\n<answer>C</answer>\n<think> Based on the selected frames, the magician performed magic in various locations such as a yoga studio, a restaurant, a park, and \na studio. However, there is no frame showing a hospital setting. Therefore, I will search for a frame that does not include a hospital to \nconfirm the answer. </think>\n<tool_call>{\"start_time\": 0, \"end_time\": 83, \"query\": \"hospital setting\", \"num_frames\": 10} </tool_call>\nFigure 14: Search pattern: elimination method. 20\nPreprint. Under review. Question: Which of the following options correctly orders the sequence of the boy's daily itinerary? A. Go to class, go home, go to study room and continue studying. B. Attend classes, go to private tutoring facilities for counselling, go to study halls to continue studying. C. Go to class, go home, go to private tutoring facility to continue learning. D. Neither. <think>To determine the correct sequence of the boy's daily itinerary, I need to analyze the video frames to identify the key activities and their \norder. The video frames suggest a daily routine that includes waking up, going to school, attending classes, going to a private tutoring facility, \nand studying in a study hall. I will search for frames that depict these activities to establish the sequence. </think>\n<tool_call>{\"start_time\": 0, \"end_time\": 30, \"query\": \"boy waking up\", \"num_frames\": 8} </tool_call>\n<think> The frames clearly show the boy studying in a study hall. Based on the selected frames, the sequence of the boy's daily itinerary is as \nfollows: go to class, go to a private tutoring facility for counseling, and go to a study hall to continue studying. Therefore, the correct option is B. </think>\n<answer>B</answer>\n<think> The frames clearly depict the boy waking up, getting dressed, and leaving the room. This suggests the beginning of his daily \nroutine.", "start_char": 70611, "end_char": 72501, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_042", "doc_id": "2511.05489v1.pdf", "text": "ore, the correct option is B. </think>\n<answer>B</answer>\n<think> The frames clearly depict the boy waking up, getting dressed, and leaving the room. This suggests the beginning of his daily \nroutine. I need to continue searching for frames that show him going to school, attending classes, going to a private tutoring facility, and \nstudying in a study hall. </think>\n<tool_call>{\"start_time\": 30, \"end_time\": 100, \"query\": \"boy going to school\", \"num_frames\": 8} </tool_call>\n<think> The frames clearly show the boy leaving home, walking to school, and entering the school. This indicates the continuation of his \ndaily routine. I need to search for frames that depict him attending classes, going to a private tutoring facility, and studying in a study hall. </think>\n<tool_call>{\"start_time\": 100, \"end_time\": 150, \"query\": \"boy attending classes\", \"num_frames\": 8} </tool_call>\n<think> The frames clearly show the boy attending classes, which is a part of his daily routine. I need to search for frames that depict him \ngoing to a private tutoring facility and studying in a study hall. </think>\n<tool_call>{\"start_time\": 150, \"end_time\": 200, \"query\": \"boy going to private tutoring facility\", \"num_frames\": 8} </tool_call>\n<think> The frames clearly show the boy going to a private tutoring facility. I need to search for frames that depict him studying in a study \nhall. </think>\n<tool_call>{\"start_time\": 200, \"end_time\": 300, \"query\": \"boy studying in a study hall\", \"num_frames\": 8} </tool_call>\nFigure 15: Search pattern: sequential search. 21\nPreprint. Under review. Question: Which of the following contents was NOT mentioned in the video? A. NBA investigates Raptors player for gambling. B. The price of Bitcoin plummeting. C. Breakdown of Israel-Hamas ceasefire talks. D. Journalist describes the chaos in Haiti. <think>To determine which content was NOT mentioned in the video, I will review the main topics discussed in the video clips.", "start_char": 72301, "end_char": 74255, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_043", "doc_id": "2511.05489v1.pdf", "text": "Israel-Hamas ceasefire talks. D. Journalist describes the chaos in Haiti. <think>To determine which content was NOT mentioned in the video, I will review the main topics discussed in the video clips. The video covers \nmultiple segments, including bridge collapse, bridge safety concerns in Canada, whale deaths at Marineland, Supreme Court arguments, NBA \ninvestigations, and news from Haiti. I will analyze each segment to identify the topics covered and then compare them to the given options. </think>\n<tool_call>{\"start_time\": 0, \"end_time\": 2656, \"query\": \"NBA investigates Raptors player for gambling\", \"num_frames\": 8} </tool_call>\n<think> From the frames provided, the content related to the NBA investigation is consistently mentioned. I will now review the other options to \nensure none of them were mentioned in the video. </think>\n<answer>C</answer>\n<think> Based on the frames provided, the content related to the NBA investigation is consistently mentioned. I will now review the other \noptions to ensure none of them were mentioned in the video. </think>\n<tool_call>{\"start_time\": 0, \"end_time\": 2656, \"query\": \"The price of Bitcoin plummeting\", \"num_frames\": 8} </tool_call>\nFigure 16: Failure case: insufficient search. There were 4 options in total, but only 2 were reviewed\nbefore the search was terminated. Question: In the video after feeding the ducks, what did the male protagonist do after riding his bike? A. Check on the cattle herd. B. Pick up duck eggs. C. Check the growth of apple and chestnut trees. D. None of the above. <think>To answer this question, I need to identify the sequence of events related to feeding the ducks and then determine what the male \nprotagonist did after riding his bike. I will search for the relevant segments in the video where the ducks are being fed and then look for the \nscene where the protagonist rides his bike afterward.", "start_char": 74055, "end_char": 75944, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05489v1.pdf__chunk_044", "doc_id": "2511.05489v1.pdf", "text": "protagonist did after riding his bike. I will search for the relevant segments in the video where the ducks are being fed and then look for the \nscene where the protagonist rides his bike afterward. </think>\n<tool_call>{\"start_time\": 120, \"end_time\": 200, \"query\": \"boy waking up\", \"num_frames\": 8} </tool_call>\n<think> Based on the provided frames, the male protagonist is seen checking on the cattle herd after riding his bike. The frames clearly show \nhim walking in a field with cattle. Therefore, the answer to the question is A. </think>\n<answer>A</answer>\n<think> The frames clearly show the sequence of events where the male protagonist feeds the ducks and then rides his bike. After feeding \nthe ducks, he is seen riding his bike. I will now search for the scene where he rides his bike. </think>\n<tool_call>{\"start_time\": 200, \"end_time\": 300, \"query\": \"riding bike\", \"num_frames\": 8} </tool_call>\n<think> Based on the provided frames, the male protagonist is seen riding his bike and then walking in a field. I will now search for the scene \nwhere he checks on the cattle herd. </think>\n<tool_call>{\"start_time\": 300, \"end_time\": 400, \"query\": \"check on cattle herd\", \"num_frames\": 8} </tool_call>\nFigure 17: Failure case: visual hallucination. No information related to riding was found in the\nsearch results. 22", "start_char": 75744, "end_char": 77070, "source_path": "data\\2511.05489v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_000", "doc_id": "2511.05491v1.pdf", "text": "Visual Spatial Tuning\nRui Yang1‚àó, Ziyu Zhu3‚àó, Yanwei Li2‚Ä†, Jingjia Huang2, Shen Yan2, Siyuan Zhou2, Zhe\nLiu1, Xiangtai Li2, Shuangye Li2, Wenqian Wang2, Yi Lin2¬ß, Hengshuang Zhao1¬ß\n1The University of Hong Kong, 2ByteDance Seed, 3Tsinghua University\nAbstract\nCapturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models\n(VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general\ncapabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial\nTuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities,\nfrom spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by\nconstructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19\nskills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset\nwith 135K samples that instruct models to reason in space. In particular, we adopt a progressive\ntraining pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by\nreinforcement learning to further improve spatial reasoning abilities. Without the side-effect to\ngeneral capabilities, the proposed VST consistently achieves state-of-the-art results on several\nspatial benchmarks, including 34.8% on MMSI-Bench and 61.2% on VSIBench. It turns out that\nthe Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning\nparadigm, paving the way for more physically grounded AI.", "start_char": 0, "end_char": 1673, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_001", "doc_id": "2511.05491v1.pdf", "text": "ench and 61.2% on VSIBench. It turns out that\nthe Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning\nparadigm, paving the way for more physically grounded AI. Project Leader ‚Ä†: Yanwei Li at yanwei.li@bytedance.com\nCorrespondence ¬ß: Yi Lin at linyi.james@bytedance.com, Hengshuang Zhao at hszhao@cs.hku.hk\nCode: https://github.com/Yangr116/VST\n1\nIntroduction\nVision-Language Models (VLMs) [1, 13, 17, 27, 62] have achieved remarkable success across a wide range of\ndomains, such as visual question answering [38, 79], document understanding [39, 44], and autonomous GUI\nagents [67]. However, these models exhibit limitations in capturing spatial relationships from sequential visual\nobservations [72, 75]. This spatial understanding ability is a foundational component of general intelligence,\npresents across a broad spectrum of animals, including humans [28, 48]. The deficiency significantly constrains\ncurrent VLMs to effectively interact with the physical world, thereby limiting their application in fields such\nas robotics [8, 86], autonomous driving [59], and augmented/virtual reality (AR/VR) [25]. To mitigate this\nissue, several studies have explored the incorporation of additional expert encoders [6, 22]. However, this\napproach often introduces extra complexity and can negatively impact the general capabilities of the models. Alternatively, other research efforts have focused on the development of specialized datasets [11, 19, 47, 69,\n77, 82], aiming to enhance the spatial understanding abilities of VLMs. ‚àóEqual contribution. 1\narXiv:2511.05491v1 [cs.CV] 7 Nov 2025\nMethod\nData Type\nData Usage\nSI\nMI\nVideo\nSFT\nRL\nSpatialVLM [11]\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\nSAT [49]\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\nMM-Spatial [19]\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\nSPAR [82]\n‚úì\n‚úì\n‚úì\n‚úì\n‚úó\nSpace-R [47]\n‚úó\n‚úó\n‚úì\n‚úì\n‚úì\nVLM-3R [22]\n‚úó\n‚úó\n‚úì\n‚úì\n‚úó\nVST (ours)\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\nTable 1 Comparison with spatial dataset.", "start_char": 1473, "end_char": 3344, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_002", "doc_id": "2511.05491v1.pdf", "text": "RL\nSpatialVLM [11]\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\nSAT [49]\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\nMM-Spatial [19]\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\nSPAR [82]\n‚úì\n‚úì\n‚úì\n‚úì\n‚úó\nSpace-R [47]\n‚úó\n‚úó\n‚úì\n‚úì\n‚úì\nVLM-3R [22]\n‚úó\n‚úó\n‚úì\n‚úì\n‚úó\nVST (ours)\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\nTable 1 Comparison with spatial dataset. VST Dataset\n3D Object Detection\nDepth Estimation\nDistance Estimation\nPosition Relationship\nSpatial Reasoning\n‚Ä¶\nVLM\nVST for Spatial Understanding\nVST for Robot Action\nLanguage Model\nSI/MI/Video\nViT\nTokenizer\nText\nResponse\nFigure 1 Overview of our VST framework. Nevertheless, these arts have typically concentrated on limited or isolated aspects of spatial understanding. As\nsummarized in Table 1, some studies focus only on the supervised fine-tuning stage, while others are restricted\nto the single scenario, overlooking the diversity of visual input. To this end, we introduce a comprehensive\nand integrated framework, termed Visual Spatial Tuning (VST), which is designed to holistically cultivate\nhuman-like visuospatial abilities in VLMs. As illustrated in Figure 1, VST effectively augments the spatial\ncapabilities of existing VLMs through the construction of an extensive and carefully curated dataset. This\nenhancement proves advantageous for downstream Vision-Language-Action (VLA) tasks. To develop the VST, we deconstruct spatial ability into two key components: spatial perception and spatial\nreasoning. We define spatial perception as the ability to discern the spatial relationships between objects,\nand spatial reasoning as the ability to build and mentally manipulate an internal model of an environment. These two components correspond to the concepts of perceptual and conceptual spatial ability, respectively,\nas proposed in cognitive science [48]. Effective spatial perception requires the model to possess foundational\nspatial knowledge‚Äîspecifically, the ability to identify both \"what is it?\" and \"where is it?\" within its\nperipersonal space.", "start_char": 3144, "end_char": 5010, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_003", "doc_id": "2511.05491v1.pdf", "text": "8]. Effective spatial perception requires the model to possess foundational\nspatial knowledge‚Äîspecifically, the ability to identify both \"what is it?\" and \"where is it?\" within its\nperipersonal space. While existing VLMs can accurately recognize objects and locate them within pixel\nspace using 2D points or bounding boxes [3, 20, 62], their ability to determine object positions in 3D space\nremains limited [41, 60]. Therefore, we introduce the VST-Perception (VST-P) dataset, comprising 4.1 million\nsamples across 19 diverse tasks. The dataset incorporates single-image data to facilitate VLMs in discerning\nspatial relationships beyond the pixel level, which is an essential step towards bridging the gap between pixel\nspace and 3D space. In addition, multi-image data is included to enhance the ability to comprehend spatial\nrelationships from multiple viewpoints, and video data enables the capture of spatiotemporal relationships. Collectively, this dataset provides a comprehensive foundation for advancing spatial perception in VLMs. Beyond foundational spatial perception, we expect the model to mentally represent spatial relationships\nbeyond its own body, thereby engaging in advanced spatial reasoning. To this end, we introduce the VST-\nReasoning (VST-R) dataset, which comprises samples featuring chain-of-thought (CoT) processes to facilitate\nthe spatial reasoning ability, as well as samples with rule-checkable answers to further enhance its reasoning\ncapabilities. In spatial reasoning, we place particular emphasis on multi-image scenarios, as these necessitate\nthe model‚Äôs ability to identify connections among objects and cameras, and to mentally reconstruct spatial\nlayouts. However, when generating spatial CoT, the limited multi-view spatial understanding of current large\nVLMs [72, 75] poses challenges for directly synthesizing accurate layout descriptions and coherent reasoning\nchains.", "start_char": 4810, "end_char": 6723, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_004", "doc_id": "2511.05491v1.pdf", "text": "nerating spatial CoT, the limited multi-view spatial understanding of current large\nVLMs [72, 75] poses challenges for directly synthesizing accurate layout descriptions and coherent reasoning\nchains. Drawing inspiration from human cognition, we propose prompting with Bird‚Äôs-Eye View (BEV)\nannotation. It leverages a top-down perspective to explicitly convey spatial relationships between objects,\nthereby improving the quality of both generated layout descriptions and CoT reasoning process. Building upon the introduced VST-P and VST-R datasets, we demonstrate that it is unnecessary to incorporate\na special encoder with 3D inductive biases into VLMs for achieving strong spatial capabilities. Instead, we\npropose to inject visual spatial knowledge into VLMs through supervised fine-tuning and further enhance\nspatial reasoning capabilities via reinforcement learning. This progressive approach mirrors the development\nof human spatial intelligence [48], i.e., establishing a foundation in spatial perception before developing higher-\nlevel spatial reasoning abilities. As a result, our proposed VST framework consistently achieves state-of-the-art\n2\nVST-P\n4.1M\nSingle-Image (64.8%)\n3D Object Detection (1586K)\n3D Grounding (170K)\nDepth Comparison (412K)\nDistance Prediction (251K)\nScene Caption (106K)\nMeasurement (175K)\nMulti-Image (33.1%)\nCorrespondence (361K)\n3D Object Detection-MI (600K)\nObject-Object (230K)\nCamera-Camera (63K)\nCamera Motion (88K)\nScene Caption (35K)\nVideo (2.1%)\nMeasurement (3K)\nObject-Object (19K)\nObject Counting (12K)\nRoute Plan (4K)\nSpatiotemporal (10K)\nScene Caption (5K)\nOthers (35K)\n(a) Perception data distribution.", "start_char": 6523, "end_char": 8176, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_005", "doc_id": "2511.05491v1.pdf", "text": "(88K)\nScene Caption (35K)\nVideo (2.1%)\nMeasurement (3K)\nObject-Object (19K)\nObject Counting (12K)\nRoute Plan (4K)\nSpatiotemporal (10K)\nScene Caption (5K)\nOthers (35K)\n(a) Perception data distribution. VST-R\n135K\nCoT (77.8%)\nSR-SI (15K)\nSR-Camera Motion (6K)\nSR-Object-Object (15K)\nSR-Camera-Camera (6K)\nGR-Math (31K)\nGR-OCR (9K)\nGR-Knowledge (24K)\nRL (22.2%)\nSR-SI (2K)\nSR-Camera Motion (1K)\nSR-Object-Object (5K)\nSR-Camera-Camera (2K)\nGR-Math (8K)\nGR-OCR (2K)\nGR-Knowledge (10K)\n(b) Reasoning data distribution\nFigure 2 Overview of the VST dataset. (a) The distribution of VST-P, which is used for SFT. (b) The distribution of\nVST-R, which is used for CoT cold start and RL. ‚ÄòSR‚Äô denotes spatial reasoning, and ‚ÄòGR‚Äô denotes general reasoning. performance on multiple spatial benchmarks, attaining 87.8% on CVBench [60], 34.8% on MMSI-Bench [75],\nand 61.2% on VSIBench [72], while preserving the general multi-modal capabilities. Furthermore, the spatial\nproficiency from VST demonstrably enhances broader VLA tasks. For instance, Qwen2.5VL-3B [3] fine-tuned\non our VST yields an 8.6% improvement on the LIBERO benchmark [35]. 2\nDataset\nIn this section, we introduce the VST dataset, specifically developed to enhance the spatial perception\nand reasoning capabilities of VLMs. First, we construct a large-scale dataset, VST-Perception (VST-P), to\nequip VLMs with comprehensive spatial knowledge. Building upon this foundation, we further create the\nVST-Reasoning (VST-R) dataset to enable VLMs to reason in space. 2.1\nVST-Perception\nAs illustrated in Figure 2a, the VST-P dataset contains 4.1 M samples across 19 different tasks for supervised\nfine-tuning, covering three primary vision scenarios, i.e., single-image, multi-image, and video. Specifically,\nsingle-image data constitutes the majority (64.8%), multi-image data accounts for 33.1%, and video data\nmakes up the remaining small portion (2.1%). Single-image.", "start_char": 7976, "end_char": 9894, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_006", "doc_id": "2511.05491v1.pdf", "text": "multi-image, and video. Specifically,\nsingle-image data constitutes the majority (64.8%), multi-image data accounts for 33.1%, and video data\nmakes up the remaining small portion (2.1%). Single-image. Since monocular images are easily obtainable, single-image data constitutes the largest category. This category primarily encompasses tasks such as relative depth estimation (2.5D), 3D object detection, and\ndistance estimation. These tasks bridge the gap between 2D pixel coordinates and the 3D physical world,\nthereby facilitating the acquisition of spatial knowledge and the development of spatial awareness in VLMs. To collect this data, we create dedicated data engines to gather data with depth maps and data with 3D\nbounding box annotations, as shown in the top left of Figure 3. The depth data mainly comes from public\ndatasets and synthetic data. The open-source data originates from ScanNet++ [76], which is collected using\nreal-world devices, and Hypersim [50], which is generated by a simulator. To increase the diversity of depth\ndata, we use a depth expert model [73] to create pseudo labels for wild images from the COCO dataset [34]. After obtaining the depth maps, we convert them to the same coordinate system and generate depth-related\nvisual instruction samples. The reference formats for depth-related samples encompass text-based, point-based,\nbox-based, and visual-prompt-based representations. These diverse formats enable VLMs to infer the relative\ndistance from objects to the camera plane. For the 3D data engine, we use two main approaches. The first approach relies on open-source datasets,\nincluding ScanNet [18], ARKitScenes [4], Hypersim [50], SUN-RGBD [54], Matterport3D [10], and Objectron [2]. Since the 3D bounding boxes from ScanNet and Matterport3D are axis-aligned, we use the corrected versions\nfrom EmbodiedScan [63] to ensure greater accuracy.", "start_char": 9694, "end_char": 11579, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_007", "doc_id": "2511.05491v1.pdf", "text": "[54], Matterport3D [10], and Objectron [2]. Since the 3D bounding boxes from ScanNet and Matterport3D are axis-aligned, we use the corrected versions\nfrom EmbodiedScan [63] to ensure greater accuracy. Notably, each dataset is designed for distinct applications,\nprovides visual data in either video or image format, and annotates objects in varying coordinate systems. Therefore, we standardize all collected 3D bounding boxes to a unified camera coordinate system and process\nthe raw visual data to reduce repetition and occlusion. The second approach is generating data using a\n3\nSingle-image Engine (2.5D)\nRGB-D Image\nFiltering\nWeb RGB Images\nExpert\nModel\nCoT Engine\nBEV Images\nLarge VLM\nQA data\n3D boxes\nVST Data Engine\nFiltering\nRGB-D Scan\nSimulator\nSingle-image Engine (3D)\nMulti-image and Video Engine\nUnified\nFormat\nRaw Data\n‚Ä¶\nSampling\nVST Model Ability\nQ: Which one in boat-A (bbox_2d: [207, \n118, 416, 306]) and boat-B (bbox_2d: [0, \n120, 220, 390]) is farthest from the camera? A: boat-A. Q: Based on your analysis of the image, \nwhich object‚Äîbottle (red point) and trash \ncan (green point)‚Äîdo you think is farther \nfrom the camera? A: trash can\nQ: There are several points in the image. Present the object represented by these \npoints in an order that goes from far to \nclose. A: D, C, A, E, B\n2.5D Spatial Perception from Monocular Images\nQ: If the small metal trash can on the floor near the white storage cabinets in the first image is north of the \nsmall printer on the white cabinet near the door in the second image, what direction is the clear plastic bottle \non the white desk with two monitors in the second image from the printer? Options: A. southeast, B. northeast, \nC. south, D. northwest\nA: <think>First, I look at the first image. The scene has a long row of white storage cabinets along the left \nwall a white desk with shelves holding folders on the right, and a small metal trash can sitting on the floor \nbetween the cabinets and the desk.", "start_char": 11379, "end_char": 13349, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_008", "doc_id": "2511.05491v1.pdf", "text": "s a long row of white storage cabinets along the left \nwall a white desk with shelves holding folders on the right, and a small metal trash can sitting on the floor \nbetween the cabinets and the desk. There's also a coat hanger in the corner and some boxes stacked nearby. ‚Ä¶\nSo, from the printer's view: the bottle is to the left (the same direction as north, since north is toward the trash \ncan on the left) and forward (since the desk is closer than the trash can). Combining these, the bottle is in the \nnorthwest direction from the printer. That matches option D.</think>D. northwest\nSpatial Reasoning\n3D Spatial Perception from Monocular Images\nMulti-image and Video Spatial Perception\nQ: In the first image, a point is circled in \npink. The second image presents several \npoints labeled '1, 2, 3, 4‚Äô. Which \ncorresponds to the original? A: point-4\nCorrespondence\nQ: Images are recorded from a first-person \nperspective. Determine the camera's \ndirection of sideways translation (left or \nright) shown in these images. A: Left\nCamera Motion\nQ: At the camera position of the second \nimage, the direction toward backpack (gold \npoint) is south, which direction is blanket \n(blue point) relative to me? A: southwest\nPosition Relationship\nQ: Detect the 3D bounding box of the chair \npillow, ‚Ä¶\nA:{‚Äúchair‚Äù: 1.23, 0.84, 4.55, 0.98, 0.70, \n0.94, 0.01, 0.00, 0.00], ‚Äúpillow‚Äù: ‚Ä¶}\n3D Object Detection\nQ: Provide the height of the table (pink \npoint-1)? Specify the measurement in \ncentimeters. A: 71 cm\nAttribute Measurement\nQ: By estimating the real-world distances, \nidentify which of the mirror, or the blinds is \nnearest to the counter. A: blinds. Distance Estimation\nDepth\nSynthetic Scene\nRendering\nUnified\nFormat\nUnified\nFormat\nQuestion\nTemplate\nQuestion\nTemplate\nQuestion\nTemplate\nVLM Auto\nWrite\nFigure 3 Data engines of VST (left) and the capabilities they enable in VST-Model (right). simulator. Specifically, we use Isaac Sim1 to synthesize data, and scenes are from the GUTopia [61].", "start_char": 13149, "end_char": 15138, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_009", "doc_id": "2511.05491v1.pdf", "text": "to\nWrite\nFigure 3 Data engines of VST (left) and the capabilities they enable in VST-Model (right). simulator. Specifically, we use Isaac Sim1 to synthesize data, and scenes are from the GUTopia [61]. With the\nlarge-scale data with 3D bounding boxes, we create visual instruction samples for 3D object detection, 3D\ngrounding, attribute measurement, and distance estimation tasks. In the 3D object detection task, we predict the 9-DoF bounding box in the camera coordinate system. Specifically, the 3D bounding box is defined by (x, y, z, xl, yl, zl, p, y, r), where (x, y, z) are the box center,\n(xl, yl, zl) are the length along the X, Y, and Z axes, and (p, y, r) are the rotation angles. However, a significant\nchallenge in utilizing datasets aggregated from disparate sources is the inherent variability in camera intrinsics,\nwhich introduces geometric inconsistencies that can hinder model generalization and scalability. To mitigate\nthis issue, we introduce a Field of View (FoV) unification strategy. This approach normalizes the input\ndata by projecting all images onto a virtual camera with a predefined, uniform FoV. This process creates a\nstandardized visual input, akin to data captured by a single virtual camera, thereby eliminating intrinsic-\nrelated discrepancies for the 3D object detection task. In addition, when creating the instruction data, we\nmix single-turn and multi-turn formats. The multi-turn format data allows each subsequent box to reference\nthe previous one during training, helping the model learn the layout information. Furthermore, if we rely solely on template-based 3D object detection data for training, the VLM may overfit\nto the specific numerical values and fail to generalize spatial understanding. Therefore, to help the VLM\nbetter comprehend spatial information at the language level, we introduce the scene caption.", "start_char": 14938, "end_char": 16800, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_010", "doc_id": "2511.05491v1.pdf", "text": "t\nto the specific numerical values and fail to generalize spatial understanding. Therefore, to help the VLM\nbetter comprehend spatial information at the language level, we introduce the scene caption. Unlike general\n1https://developer.nvidia.com/isaac/sim\n4\ncaptions, which primarily describe image content, scene captions focus on the layout information and spatial\nrelationships within the image. To obtain such scene captions, we prompt a large VLM [27] with ground-truth\n3D bounding boxes and object relationships extracted from the scene graph [85]. The resulting captions\nnot only describe the objects present in the image, but also provide detailed layout information and spatial\narrangements. Multi-image. The second category comprises multi-image data, which supports tasks such as multi-view 3D\nobject detection, multi-view correspondence, object-object relationship understanding, and camera motion\nanalysis. These tasks are designed to enhance VLMs to comprehend spatial relationships across different\nviewpoints. As illustrated in the third data engine of Figure 3, we sample multi-image data from RGB-D\nscans sourced from ScanNet [18], ScanNet++ [76], and ARKitScenes [4]. For correspondence tasks, we utilize\npoint clouds and depth maps from various viewpoints to identify matched points. To unify object information\nacross multiple images, we transform all objects into the camera coordinate system of the first image. For\ncamera motion data, we represent camera poses using Euler angles. After these unification steps, we generate\ntemplate-based visual instruction samples. In the multi-image scenario, we also create the scene caption to\nreconstruct the scene layout by text and describe the spatial information represented by multiple RGB images. Video. The third category consists of video data, which enables the model to capture spatiotemporal\nrelationships through tasks such as identifying the order of appearances and counting objects.", "start_char": 16600, "end_char": 18560, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_011", "doc_id": "2511.05491v1.pdf", "text": "ages. Video. The third category consists of video data, which enables the model to capture spatiotemporal\nrelationships through tasks such as identifying the order of appearances and counting objects. To construct\nthe video dataset, we employ the same data engine used for multi-image data. The only difference is that we\nadd the appearance time for each object. Furthermore, we enhance the video dataset by sampling two-thirds\nof the data from VLM-3R [22], reorganizing it into a multi-turn format rather than a single-turn format. With the introduction of the VST-P dataset, the VLM exhibits significantly enhanced fundamental capabilities\nin comprehending spatial relationships. Notably, there is a ‚àº20% improvement on CVBench-3D [60], a ‚àº5%\nincrease on BLINK [23], and a ‚àº16% gain on VSIBench [72], as illustrated in Tables 5, 6, and 7. 2.2\nVST-Reasoning\nAs shown in Figure 2b, the VST-Reasoning (VST-R) dataset contains 135K samples with two parts: one part\nincludes CoT steps to teach the model how to reason, and the other part provides rule-checkable data used in\nonline RL to improve the reasoning ability. Besides spatial data, both parts include general data to preserve\nthe original general abilities. Most spatial reasoning samples come from multi-image scenarios, which require\nreconstructing scene details and inferring spatial relations. For the spatial reasoning samples with the CoT process, we develop a data engine, as illustrated in the\nbottom left of Figure 3. Specifically, we sample data from template-based question-answer pairs and employ a\nlarge VLM [27] as the teacher to generate detailed CoT reasoning steps. Recognizing that the multi-view\nspatial understanding of the current large VLMs remains limited relative to their general multi-modal abilities,\nwe introduce a novel strategy named prompting with BEV annotation. Specifically, this method leverages\nground-truth 3D bounding boxes to visualize the BEV image of the scene represented by multiple images.", "start_char": 18360, "end_char": 20349, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_012", "doc_id": "2511.05491v1.pdf", "text": "troduce a novel strategy named prompting with BEV annotation. Specifically, this method leverages\nground-truth 3D bounding boxes to visualize the BEV image of the scene represented by multiple images. During generation, we provide RGB images, the corresponding BEV visualizations, detailed object information,\nand question-answer pairs to prompt the teacher VLM. The BEV images serve as an auxiliary spatial prompt,\nallowing the teacher model to better capture spatial relationships compared to using only RGB images. As a\nresult, the generated reasoning processes are more coherent and accurate. For the CoT format, we adopt a\ntextual representation rather than utilizing 3D bounding boxes or cognition maps [77], as the textual format\noffers greater generality. In particular, during the reasoning process, the model first reconstructs the spatial\nlayout by text and subsequently infers the correct answer. With the VST-R dataset, the VLM demonstrates significantly enhanced spatial reasoning abilities. As\nillustrated in Table 11, there is an 8.9% improvement on MMSI-Bench [75]. 3\nMethod\nOur target is to equip general VLMs with 3D knowledge for better spatial understanding and reasoning from\ncommon visual inputs. Therefore, we chose Qwen2.5-VL [3] as the base model because it can accurately identify\n5\n(a) VST Model\n(b) VST-based VLA Model\nLanguage Model\nInput Image\nVision Encoder\nTokenizer\nLanguage Instruction\nOutput Text\nAction De-Tokenizer\nLanguage Model\nInput Image\nVision Encoder\nTokenizer\nLanguage Instruction\nFigure 4 (a) The VST model, which incorporates spatial perception and reasoning capabilities. (b) The VST-based\nVLA model, capable of generating action sequences through an action de-tokenizer. objects and locate them in pixel space. As shown in Figure 4 (a), it follows the widely used \"ViT-MLP-LLM\"\nparadigm: a pre-trained Vision Transformer (ViT) is combined with a large language model (LLM) via an\nMLP merger.", "start_char": 20149, "end_char": 22089, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_013", "doc_id": "2511.05491v1.pdf", "text": "hem in pixel space. As shown in Figure 4 (a), it follows the widely used \"ViT-MLP-LLM\"\nparadigm: a pre-trained Vision Transformer (ViT) is combined with a large language model (LLM) via an\nMLP merger. 3.1\nTraining Strategy\nWe continued training the base model to endow it with spatial perception and reasoning capabilities. The\ntraining process can be divided into three stages. Stage 1: Supervised Fine-tuning. At this stage, we incorporate the foundational spatial understanding into\nthe base model by utilizing the proposed VST-P dataset. To maintain the original capabilities of the base\nmodel, we also incorporate a portion of general multi-model data from open-source datasets [20, 33]. Assume\nthe base model is parameterized by Œ∏, which can simultaneously process text, images, and video. For any\ngiven training sample x = [x1, . . . , xL] of length L, we employ visual tokens as the conditioning context for\ntext prediction and adopt the standard auto-regressive objective:\nLŒ∏(x) = ‚àí\nL\nX\ni=2,xi‚ààtext\nwi log pŒ∏(xi | x1, . . . , xi‚àí1),\n(1)\nThe resulting model exhibits foundational spatial perception abilities, as illustrated in Table 5, 6, and 7. Stage 2: CoT Cold Start. This stage leverages chain-of-thought (CoT) data to instruct the model utilizing\nreasoning patterns. For example, in spatial reasoning scenarios with limited viewpoints, the model first\nreconstructs the layout of the scene using text, and then reasons through the given question. To preserve the\nmodel‚Äôs reasoning ability on general tasks, we also take some general reasoning data. The training objective\nremains the same as in Equation 1. The resulting model from this stage has basic spatial reasoning capabilities\n(Table 10), which serves as the initial RL actor. Stage 3: Reinforcement Learning. In this stage, we employ RL to further enhance the spatial reasoning\ncapabilities of the stage-2 model.", "start_char": 21889, "end_char": 23772, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_014", "doc_id": "2511.05491v1.pdf", "text": "capabilities\n(Table 10), which serves as the initial RL actor. Stage 3: Reinforcement Learning. In this stage, we employ RL to further enhance the spatial reasoning\ncapabilities of the stage-2 model. For this purpose, we utilize the Group Relative Policy Optimization (GRPO)\nalgorithm [51], which bypasses the need for a value model by computing the relative advantage of each\nresponse within a group of responses to the same question. To facilitate this process, we curated a verification\ndataset comprising tasks related to spatial understanding, 3D object detection, and general multi-modal\nunderstanding. This dataset is categorized into four task types: multiple-choice, open-ended, OCR, and 3D\ndetection. In the GRPO framework, we employ a mixed rule-based reward to evaluate the generated responses. For a given response ÀÜy and its corresponding ground truth y, the overall reward function is defined as:\nR(y, ÀÜy) = Racc(y, ÀÜy) + Rformat(y, ÀÜy). (2)\nThis function combines an accuracy reward, Racc(¬∑, ¬∑), which scores the correctness of the response, with a\nformat reward, Rformat(¬∑, ¬∑), which incentivizes adherence to a specified output format. For multiple-choice,\n6\nopen-ended, and OCR tasks, the accuracy reward is calculated using standard evaluation protocols [24, 43,\n44, 53]. For 3D object detection tasks, the reward is a linear combination of the 3D Intersection over Union\n(IoU) score and the F1 score:\nR3d(y, ÀÜy) = Œ±Riou(y, ÀÜy) + (1 ‚àíŒ±)RF1(y, ÀÜy)),\n(3)\nwhere Œ± is a hyperparameter that defaults to 0.5. In detail, given N predicted and M ground-truth 3D\nbounding boxes, we first establish a bipartite matching [32] between the predictions and the ground truth;\nRiou(¬∑, ¬∑) is then calculated as the average IoU of the successfully matched pairs. To calculate RF1(¬∑, ¬∑), we\ndefine a true positive as a match with an IoU score exceeding a threshold of 0.25.", "start_char": 23572, "end_char": 25447, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_015", "doc_id": "2511.05491v1.pdf", "text": "truth;\nRiou(¬∑, ¬∑) is then calculated as the average IoU of the successfully matched pairs. To calculate RF1(¬∑, ¬∑), we\ndefine a true positive as a match with an IoU score exceeding a threshold of 0.25. Following this stage, the\nmodel exhibits superior spatial reasoning abilities relative to the cold-start model, as shown in Table 11. 3.2\nExpanding to Vision-Language Action Model\nWith the spatial-enhanced model, a natural question emerges: can the integration of spatial priors improve\nthe performance of Vision-Language-Action (VLA) models in robotic manipulation tasks? To this end, we\nadapt the pretrained VLM into a VLA model, following the methodology of OpenVLA [31], as illustrated in\nFigure 4. Specifically, we formulate the action prediction problem as a vision-language task where, given an\nobservation image and a natural language instruction, the model auto-regressively predicts the actions. To\naccomplish this, we discretize the action space into 256 bins, where each bin corresponds to a special token in\nthe language tokenizer. With the actions tokenized, the entire model is fine-tuned using the objective function\ndefined in Eq 1. 4\nExperiment\nIn this section, we first outline our experimental setup, detailing the training and evaluation protocols. Following this, we compare our model‚Äôs performance against leading methods across several key benchmarks. Finally, we conclude with a detailed analysis of data effectiveness, the impact of data and model scaling, and\nother ablation studies. 4.1\nImplementation Details\nStage 1. The initial training stage aims to establish a strong foundation of spatial understanding capabilities. For this stage, we use a global batch size of 128, a sequence length of 16, 384, and a dynamic data packing\nstrategy to accelerate the training process. We employ the AdamW [40] optimizer, setting the base learning\nrate to 5 √ó 10‚àí5 and the vision encoder‚Äôs learning rate to 5 √ó 10‚àí6.", "start_char": 25247, "end_char": 27182, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_016", "doc_id": "2511.05491v1.pdf", "text": "d a dynamic data packing\nstrategy to accelerate the training process. We employ the AdamW [40] optimizer, setting the base learning\nrate to 5 √ó 10‚àí5 and the vision encoder‚Äôs learning rate to 5 √ó 10‚àí6. During this phase, we combine our VST\ndata with general multi-modal data from LLaVA-OneVision [33]. This approach allows the model to learn\nnew spatial understanding knowledge while mitigating catastrophic forgetting of its original capabilities. For\nour ablation studies, we use Qwen2.5-VL-3B [3] as the base model, training it on a mixture of one-third of\nthe VST data and 800K general multi-modal samples. For our final models, we employ Qwen2.5-VL-3B,\nQwen2.5-VL-7B, and Qwen2.5-VL-32B as base models, utilizing the entire VST dataset combined with 2.4M\ngeneral multi-modal samples. Stage 2. In the CoT cold-start stage, we continue training the model from the initial foundation stage. The\nhyper-parameters are adjusted to a global batch size of 128, a base learning rate of 1 √ó 10‚àí5, a vision encoder\nlearning rate of 1 √ó 10‚àí6, and a sequence length of 16, 384. In this stage, the training data is a mixture\nof spatial reasoning data and general multimodal reasoning data. We train the model for 2 epochs, as we\nobserved that smaller-scale models require extended training to effectively master the long-form CoT reasoning\nprocess. Stage 3. In the RL stage, we further refine the model from the second stage using the VeRL [52] framework. For the training objective, we adopt a revised version of the GRPO algorithm [78]. This stage utilizes the\nAdamW [40] optimizer with a constant learning rate of 1 √ó 10‚àí6 and a global batch size of 128. Expanding to VLA Model. When expanding to the VLA model, we still use the AdamW [40] optimizer, but\nwith a modified learning rate schedule: a base learning rate of 5 √ó 10‚àí5 and a vision encoder learning rate of\n5 √ó 10‚àí6. We set the global batch size to 128 and the max sequence length of data packing to 1024.", "start_char": 26982, "end_char": 28939, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_017", "doc_id": "2511.05491v1.pdf", "text": "dified learning rate schedule: a base learning rate of 5 √ó 10‚àí5 and a vision encoder learning rate of\n5 √ó 10‚àí6. We set the global batch size to 128 and the max sequence length of data packing to 1024. This\n7\nMethods\nCV\n3DSR MMSI BLINK VSI MMStar MMB RealworldQA MMMU OCRB AI2D\nGPT-4o [1]\n76.0\n45.3\n30.3\n65.9\n34.0\n65.1\n84.3\n76.2\n70.7\n80.6\n84.9\nGemini-2.5-Pro [17]\n-\n-\n36.9\n70.6\n-\n77.5\n90.1\n78.0\n81.7\n86.6\n88.4\nSeed1.5-VL [27]\n85.2\n61.6\n29.7\n72.1\n41.5\n77.8\n89.9\n78.4\n77.9\n86.1\n87.3\nLLava-OneVision-7B [33]\n61.9\n54.4\n26.6\n48.2\n32.4\n61.7\n80.8\n66.3\n48.8\n62.2\n81.4\nQwen2.5-VL-3B [3]\n71.8\n50.2\n26.5\n47.6\n29.6\n55.9\n79.9\n65.4\n47.9\n79.7\n81.6\nQwen2.5-VL-7B [3]\n75.4\n53.2\n25.9\n56.4\n38.9\n63.9\n83.5\n68.5\n58.6\n86.4\n83.9\nQwen2.5-VL-32B [3]\n81.9\n56.7\n27.7\n59.9\n40.9\n70.3\n84.0\n71.2\n68.9\n85.6\n85.4\nInternVL3-8B [84]\n81.0\n55.7\n25.7\n55.5\n42.1\n68.2\n83.4\n70.8\n62.7\n88.0\n85.2\nInternVL3-38B [84]\n84.9\n59.0\n26.3\n64.0\n48.9\n71.5\n87.6\n75.6\n70.1\n88.6\n88.9\nMiMo-VL-7B-RL [58]\n82.3\n50.8\n29.3\n62.4\n37.2\n65.1\n84.4\n68.2\n66.7\n86.6\n83.5\nSpaceR-7B [47]\n74.8\n53.3\n20.1\n55.4\n43.5\n61.6\n84.3\n64.7\n53.1\n85.9\n85.5\nSPAR-8B [82]\n80.7\n57.5\n-\n43.9\n41.1\n-\n79.9\n64.7\n-\n-\n-\nVST-3B-SFT (ours)\n84.4\n54.1\n30.2\n59.1\n57.9\n58.0\n80.9\n68.4\n45.2\n83.7\n82.5\nVST-3B-RL (ours)\n84.2\n56.5\n31.3\n57.2\n57.7\n58.9\n80.5\n68.5\n49.8\n80.9\n82.4\nVST-7B-SFT (ours)\n85.5\n54.6\n32.0\n62.1\n60.6\n63.1\n83.3\n72.2\n50.6\n85.5\n84.9\nVST-7B-RL (ours)\n86.5\n60.1\n34.8\n62.6\n61.2\n63.5\n83.0\n68.5\n49.4\n86.1\n83.5\nTable 2 Comparison with state-of-the-art VLMs on spatial benchmarks and general benchmarks. adjustment is necessary to compensate for the relatively short action sequences and the small resolution of the\ntraining images (256 √ó 256). The model is finetuned for 2.5K steps and 10K on the LIBERO benchmark [35]\nin total. Evaluation.", "start_char": 28739, "end_char": 30481, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_018", "doc_id": "2511.05491v1.pdf", "text": "for the relatively short action sequences and the small resolution of the\ntraining images (256 √ó 256). The model is finetuned for 2.5K steps and 10K on the LIBERO benchmark [35]\nin total. Evaluation. For our evaluation, we assess spatial understanding across three distinct modalities: single-image\nability is benchmarked with CVBench [60] and 3DSRBench [41], multi-image ability with BLINK [23] and\nMMSI-Bench [75], and video-based ability with VSIBench [72]. The average score (S-AVG) across these\nbenchmarks is then calculated to quantify the model‚Äôs overall spatial understanding capabilities. To verify its\ngeneral multi-modal understanding, we also report the average score (MM-AVG) across a suite of standard\nbenchmarks: MMStar [12], MMBench [38], RealworldQA [66], MMMU [79], OCRBench [39], and AI2D [30]. For 3D object detection, we evaluate the model on both the SUN RGB-D [54] validation set (Total3D\nversion [46]) and the ARKitScenes [4] test set (Omni3D version [7]). Performance is measured using standard\nmetrics: average precision (AP) at IoU thresholds of 0.15, 0.25, and 0.50 (denoted as AP@15, AP@25, and\nAP@50, respectively), and average recall for the top 100 predictions (AR@100). 4.2\nMain Results\nAs shown in Table 2, our VST models achieve competitive results across both spatial and general benchmarks. Notably, VST-7B-SFT and VST-7B-RL deliver leading performance on mainstream spatial understanding\ntasks. On the CVBench [55], VST-7B-SFT attains 85.5, surpassing the proprietary Seed1.5-VL [27] with 85.2. On MMSI-Bench [75], VST-7B-SFT achieves 32.0, outperforming GPT-4o [1] at 30.3, while RL further boosts\nVST-7B-RL to 34.8, approaching the proprietary state of the art Gemini-2.5-Pro [17] at 36.9. On BLINK [23],\nVST-7B-SFT yields 62.1, surpassing Qwen2.5-VL-7B [71] at 56.4. Notably, the VSI-Bench [72] highlights\nthe strength of our models in video spatial understanding.", "start_char": 30281, "end_char": 32187, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_019", "doc_id": "2511.05491v1.pdf", "text": "ni-2.5-Pro [17] at 36.9. On BLINK [23],\nVST-7B-SFT yields 62.1, surpassing Qwen2.5-VL-7B [71] at 56.4. Notably, the VSI-Bench [72] highlights\nthe strength of our models in video spatial understanding. VST-7B-SFT reaches 60.6, and VST-3B-SFT\nachieves 57.9, substantially ahead of GPT-4o with 34.0. Detailed results are reported in Table 3. Without\nany specialized 3D encoder, VST-7B-RL delivers the best overall average among comparable VLMs, achieving\n61.2. Although VLM-3R-7B [22] attains a similar score, it relies on an additional expert 3D encoder, whereas\nVST operates with a standard vision backbone. Beyond the overall average, VST shows clear strengths on\nfine-grained spatial sub-tasks: it leads in Object Size and Room Size estimation, and performs strongly in\nRelative Direction and Appearance Order. These gains reveal that VST‚Äôs visuospatial training enables robust\nmetric and ordinal reasoning over scenes, even without explicit 3D features. Although VST-SFT models\nobtain only moderate results on general benchmarks‚Äîscoring 83.3 for the 7B size and 80.9 for the 3B size\non MMBench [38], and 84.9 and 82.5 on AI2D [30], slightly below proprietary state of the art‚Äîthey still\nprovide a well-balanced overall performance. These outcomes highlight the clear advantage of VST in spatial\nperception and reasoning while maintaining strong competitiveness in multi-modal understanding. 8\nMethods\nAvg. Obj. Count\nAbs. Dist. Obj. Size\nRoom\nSize\nRel. Dist\nRel. Dir. Route\nPlan\nAppr.", "start_char": 31987, "end_char": 33473, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_020", "doc_id": "2511.05491v1.pdf", "text": "in spatial\nperception and reasoning while maintaining strong competitiveness in multi-modal understanding. 8\nMethods\nAvg. Obj. Count\nAbs. Dist. Obj. Size\nRoom\nSize\nRel. Dist\nRel. Dir. Route\nPlan\nAppr. Order\nGPT-4o [1]\n34.0\n46.2\n5.3\n43.8\n38.2\n37.0 41.3\n31.5\n28.5\nGemini-1.5-Pro [57]\n45.4\n56.2\n30.9\n64.1\n43.6\n51.3 46.3\n36.0\n34.6\nLLaVA-OneVision-7B [33] 32.4\n47.7\n20.2\n47.4\n12.3\n42.5 35.2\n29.4\n24.4\nLLaVA-Video-7B [83]\n35.6\n48.5\n14.0\n47.8\n24.2\n43.5 42.4\n34.0\n30.6\nQwen2.5-VL-7B [3]\n32.7\n34.5\n19.4\n47.6\n40.8\n32.8 24.5\n32.5\n29.4\nSAT-7B [49]\n-\n-\n-\n-\n-\n47.3 41.1\n37.1\n36.1\nInternVL-Spatial-8B [21]\n-\n68.7\n40.9\n63.1\n54.3\n47.7\n-\n29.9\n60.5\nSpaceR-7B [47]\n43.5\n61.9\n28.6\n60.9\n35.2\n38.2 46.0\n31.4\n45.6\nVILASR-7B [64]\n45.4\n63.5\n34.4\n60.6\n30.9\n48.9 45.2\n30.4\n49.2\nVLM-3R-7B [22]\n60.9\n70.2\n49.4 69.2\n67.1\n65.4 80.5\n45.4\n40.1\nVST-3B-SFT (ours)\n57.9\n69.3\n45.4\n71.8\n62.4\n59.0 46.0\n38.7\n70.2\nVST-3B-RL (ours)\n57.7\n66.6\n45.0\n72.8\n60.9\n59.9 47.6\n40.7\n68.3\nVST-7B-SFT (ours)\n60.6\n72.0\n44.4\n74.3\n68.3\n59.7 55.8\n44.9\n65.2\nVST-7B-RL (ours)\n61.2\n71.6\n43.8 75.5\n69.2\n60.0 55.6\n44.3\n69.2\nObject\nCount\nAppearance\nOrder\nAbsolute\nDistance\nObject\nSize\nRoute\nPlan\nRelative\nDistance\nRelative\nDirection\nRoom\nSize\n72.0\n70.2\n45.4\n74.3\n44.9\n59.7\n55.8\n68.3\nGemini-1.5 Pro\nLLaVA-Video-7B\nQwen2.5-VL-7B\nSpaceR-7B\nVILASR-7B\nVST-7B-SFT (ours)\nTable 3 Comparison with state-of-the-art VLMs on VSI-Bench [72]. Methods\nAP@15\nSeed1.5-VL [27]\n33.5\nGemini-2.0-Pro [17]\n32.5\nGemini Robotics-ER [56]\n48.3\nTotal3DU [46]\n14.3\nImplicit3D [81]\n24.1\nVST-3B-SFT (ours)\n37.3\nVST-3B-RL (ours)\n40.1\nVST-7B-SFT (ours)\n41.6\nVST-7B-RL (ours)\n44.2\nGemini Robotics-ER\nVST-7B-RL\nVST-7B-SFT\nVST-3B-RL\nVST-3B-SFT\nSeed1.5-VL\nGemini-2.0-Pro\nImplicit3D\nTotal3DU\n0\n10\n20\n30\n40\n50\nAP@15\n48.3\n44.2\n41.6\n40.1\n37.3\n33.5\n32.5\n24.1\n14.3\nExpert Models\nProprietary Models\nVST (ours)\nTable 4 Comparison AP@15 on SUN RGB-D 3D object detection benchmark [54]. Table 4 summarizes results on the SUN RGB-D [54] 3D object detection benchmark.", "start_char": 33273, "end_char": 35229, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_021", "doc_id": "2511.05491v1.pdf", "text": ".3\nExpert Models\nProprietary Models\nVST (ours)\nTable 4 Comparison AP@15 on SUN RGB-D 3D object detection benchmark [54]. Table 4 summarizes results on the SUN RGB-D [54] 3D object detection benchmark. VST-7B-SFT reaches\n41.6 AP@15, while VST-7B-RL improves to 44.2, ranking first among both general VLMs [17, 27] and expert\nmethods [46, 81]. The RL model outperforms Gemini-2.0-Pro [17] at 32.5 and Seed1.5-VL [27] at 33.5, as\nwell as specialized systems such as Implicit3D [81] at 24.1 and Total3DU [46] at 14.3. These findings show\nthat VST, even without auxiliary 3D encoders, can achieve strong 3D object detection purely through visual\nspatial tuning. 4.3\nAblation Study\nAblation for the single-image data. Our baseline model is the Qwen2.5-VL-3B fine-tuned on a general\ndataset of 800K samples. From this baseline, we incrementally introduce different types of data to enhance its\ncapabilities. The results are recorded in Table 5. Given that the baseline model already possesses the 2D\nperception ability, our initial goal is to endow it with 3D spatial awareness by incorporating the 3D object\ndetection task, using only monocular RGB inputs. Introducing these 3D object detection data improves the\nperformance on spatial understanding benchmarks by 1.0%. Notably, this data leads to a 5.7% improvement\non CVBench-3D [60], demonstrating that the model has successfully acquired foundational 3D perception\nskills from the detection task. However, the model fails to generalize these newly learned 3D representations\nto other tasks, i.e., the model exhibits only marginal improvements or a slight degradation in performance on\nother benchmarks. To address this, we design a series of auxiliary tasks. The first auxiliary task involves 3D\ngrounding data, designed to teach the model to interpret the spatial meaning of a given 3D bounding box.", "start_char": 35029, "end_char": 36877, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_022", "doc_id": "2511.05491v1.pdf", "text": "s. To address this, we design a series of auxiliary tasks. The first auxiliary task involves 3D\ngrounding data, designed to teach the model to interpret the spatial meaning of a given 3D bounding box. Although this data does not yield an overall performance boost, we retain it in our training set to maintain\n9\nthe diversity of the overall data and tasks. The second task utilizes scene captioning data, which primarily\ndescribes the spatial relationships between objects within an image. This addition resulted in a 2.3% overall\nimprovement on spatial understanding benchmarks, with specific gains of 11% on CVBench-3D [60], 1.7% on\n3DSRBench [41], and 2.1% on VSIBench [72]. Third, we introduce measurement data to inject priors for an\nobject‚Äôs length, width, and height, which obtains a 0.8% improvement on 3DSRBench [41]. Surprisingly, this\nmeasurement priors also generalized to video tasks, yielding a 2.4% improvement on VSIBench [72]. The final\nauxiliary task incorporated depth data. Explicitly training on depth data led to significant improvements,\nincluding a 10.4% gain on CVBench-3D [60], a 1.2% gain on BLINK [23], and a 3.2% gain on VSIBench [72]. However, the results show that incorporating single-image data for spatial understanding fails to bring\nsignificant improvements on the MMSI-Bench [75]. This may be because the MMSI-Bench requires models to\nperform more advanced spatial reasoning. Data\nS-AVG\nSingle-image\nMulti-image\nVideo\nMM-AVG\nCV-2D\nCV-3D\n3DSR\nMMSI\nBLINK\nVSI\nBaseline\n49.9\n71.2\n72.6\n50.5\n26.1\n49.2\n29.6\n68.3\n+ 3D Object Detection\n50.9\n71.5\n78.3\n51.0\n25.5\n48.2\n30.9\n70.0\n+ 3D Grounding\n50.5\n73.3\n72.3\n50.5\n27.7\n48.4\n31.0\n69.5\n+ Scene Caption (si)\n52.8\n72.4\n83.8\n52.2\n25.9\n49.4\n33.1\n69.8\n+ Measurement (si)\n53.3\n71.9\n83.0\n53.0\n26.7\n49.4\n35.5\n69.2\n+ Depth and Distance Data\n56.4\n73.5\n93.4\n53.2\n28.8\n50.6\n38.7\n69.7\nTable 5 Ablation for the single-image data. si denotes single-image data. Ablation for the multi-image data.", "start_char": 36677, "end_char": 38631, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_023", "doc_id": "2511.05491v1.pdf", "text": "53.0\n26.7\n49.4\n35.5\n69.2\n+ Depth and Distance Data\n56.4\n73.5\n93.4\n53.2\n28.8\n50.6\n38.7\n69.7\nTable 5 Ablation for the single-image data. si denotes single-image data. Ablation for the multi-image data. Building on the model‚Äôs ability to understand spatial relationships in\nsingle images, we aim to extend its capabilities to comprehend correspondences, object relationships, and\ncamera relationships in multi-image scenarios. Therefore, we use Qwen2.5-VL-3B, fine-tuned on single-image\ndata, as our baseline model and further enhance it by incorporating multi-image data. The results are shown\nin Table 6. First, we introduce multi-view correspondence data, which leads to a 1.8% improvement on the\nBLINK [23]. Second, we incorporate multi-image object detection data, which requires the model to identify\ncorresponding objects across multiple views and represent them using 3D bounding boxes. This addition\nyields a 2.1% improvement on MMSI-Bench [75], demonstrating that reconstructing layouts from limited\nviews enhances spatial reasoning. Third, we include rule-based data that captures relationships between\nobjects and cameras. This data yields gains of 1.0% on BLINK [23], 0.8% on MMSI-Bench [75], and 1.1% on\nVSI-Bench [72]. Similar to single-image scene captioning, we aim for the model to understand scenes depicted\nacross multiple images. Thus, we create multi-image scene caption data, which contributes an additional 0.3%\ngain on MMSI-Bench [75]. Next, we add data related to camera motion, resulting in a 2% improvement on\nBLINK. Finally, we incorporate general multi-image understanding data to maintain the overall capabilities\nof the model. Overall, incorporating multi-image data results in a 1.3% improvement in the average score for\nspatial understanding, but it does not lead to any progress on single-image benchmarks.", "start_char": 38431, "end_char": 40270, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_024", "doc_id": "2511.05491v1.pdf", "text": "ies\nof the model. Overall, incorporating multi-image data results in a 1.3% improvement in the average score for\nspatial understanding, but it does not lead to any progress on single-image benchmarks. Data\nS-AVG\nSingle-image\nMulti-image\nVideo\nMM-AVG\nCV-2D\nCV-3D\n3DSR\nMMSI\nBLINK\nVSI\nBaseline\n56.4\n73.5\n93.4\n53.2\n28.8\n50.6\n38.7\n69.7\n+ Corespondence\n56.7\n74.1\n92.5\n53.9\n27.9\n52.4\n39.1\n69.7\n+ 3D Object Detection (mi)\n56.5\n73.3\n92.3\n53.1\n30.0\n52.7\n37.7\n69.2\n+ Object-object Relation\n57.1\n74.3\n91.9\n53.2\n31.9\n53.2\n38.3\n69.4\n+ Camera-camera Relation\n57.3\n72.7\n93.1\n53.6\n31.8\n53.7\n38.8\n68.8\n+ Scene Caption (mi)\n57.4\n73.2\n92.8\n54.0\n32.1\n53.0\n39.3\n69.2\n+ Camera Motion\n57.7\n73.9\n92.5\n54.0\n32.4\n55.0\n38.2\n68.7\n+ General Data (mi)\n57.7\n73.8\n92.4\n53.7\n32.7\n55.4\n38.4\n69.2\nTable 6 Ablation for the multi-image data. mi denotes mingle-image data. Ablation for the video data. Since single-image and multi-image data provide minimal improvement on video\nbenchmarks, we further constructed video data to enhance spatial understanding for video inputs. We use\n10\nQwen2.5-VL-3B, a model fine-tuned on both single-image and multi-image data, as our baseline model. As\ndemonstrated in Table 7, when incorporating a portion of general video data from LLaVA-OneVision [33],\nthe VSI-Bench score decreases by 0.3%, indicating the limitations of general video data in video spatial\nunderstanding tasks. When our VST-video data is introduced, the model achieves a 16.6% improvement on\nVSI-Bench [72]. Moreover, it is able to maintain performance on other benchmarks. Data\nS-AVG\nSingle-image\nMulti-image\nVideo\nMM-AVG\nCV-2D\nCV-3D\n3DSR\nMMSI\nBLINK\nVSI\nBaseline\n54.8\n73.8\n92.4\n53.7\n32.7\n55.4\n38.4\n69.2\n+ General Video\n57.9\n74.3\n92.3\n53.5\n31.9\n57.6\n38.1\n69.4\n+ VST-video\n60.6\n75.1\n93.1\n54.0\n31.3\n55.6\n54.7\n69.4\nTable 7 Ablation for the video data. Scaling up model size yields consistent gains in spatial benchmarks, whereas for 3D object detection,\nperformance improvements saturate at the 7B model.", "start_char": 40070, "end_char": 42039, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_025", "doc_id": "2511.05491v1.pdf", "text": "54.7\n69.4\nTable 7 Ablation for the video data. Scaling up model size yields consistent gains in spatial benchmarks, whereas for 3D object detection,\nperformance improvements saturate at the 7B model. We further investigate the relationship between model\nsize and spatial understanding performance, as shown in Table 8. Increasing the model size from 3B to 7B\nresults in a 1.3% improvement in average scores on spatial understanding benchmarks. Further increasing the\nmodel size to 32B yields a 1.7% improvement in average scores on these benchmarks. These results suggest\nthat larger models achieve greater improvements on spatial understanding tasks. The relationship between model size and 3D object detection is presented in Table 9. When increasing the\nmodel size from 3B to 7B, we observe a 4.2 AP improvement on SUN RGB-D [54] and a 3.5 AP improvement\non ARKitScenes [4]. However, when increasing the model size from 7B to 32B, the performance does not\nexhibit a positive correlation as seen in the spatial understanding benchmarks. This may be because a model\nwith 7B parameters is already sufficient to handle this fundamental perception task. Data scaling boosts performance for all models, yet larger models gain more on spatial benchmarks while\nsmaller models benefit most on 3D object detection. When the dataset is increased three times, all models\nshow improvement. As shown in Table 8, the tripling of the data scale resulted in improvements of 1.1%,\n1.5%, and 1.7% for the 3B, 7B, and 32B models, respectively, on spatial benchmarks. As recorded in Table 9,\ndata scaling also enhances the performance of 3D object detection tasks.", "start_char": 41839, "end_char": 43486, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_026", "doc_id": "2511.05491v1.pdf", "text": "provements of 1.1%,\n1.5%, and 1.7% for the 3B, 7B, and 32B models, respectively, on spatial benchmarks. As recorded in Table 9,\ndata scaling also enhances the performance of 3D object detection tasks. Model\nSize\nData\nScale S-AVG\nSingle-image\nMulti-image\nVideo MM-AVG\nCV-2D CV-3D 3DSR MMSI BLINK\nVSI\n3B\n1√ó\n60.6\n75.1\n93.1\n54.0\n31.3\n55.6\n54.7\n69.4\n3√ó\n61.7\n75.0\n93.8\n54.1\n30.2\n59.1\n57.9\n69.9\n7B\n1√ó\n61.9\n76.5\n94.4\n53.5\n31.9\n58.3\n57.0\n73.1\n3√ó\n63.4\n75.2\n95.7\n54.6\n32.0\n62.1\n60.6\n73.3\n32B\n1√ó\n63.6\n77.5\n93.1\n55.7\n36.9\n61.4\n57.1\n75.6\n3√ó\n65.3\n80.7\n94.8\n56.1\n36.0\n65.4\n58.7\n75.9\n3B\n7B\n32B\n55\n60\n65\n70\nS-AVG (%)\n+1.1\n+1.5\n+1.7\n1√ó\n3√ó\nTable 8 Results of scaling model and data size\nA clearer reasoning trace from ‚Äôprompting with BEV‚Äô can improve spatial reasoning performance. We conduct\nan ablation study on CoT data for spatial reasoning, as recorded in Table 10. Our baseline model is Qwen2.5-\nVL-7B [71], fine-tuned on one-third of VST-P single-image data, VST-P multi-view correspondence data,\nVST-P multi-image 3D object-detection data, and 800K general samples. Our first attempt is to represent\nthe layout of multi-image scenes with 3D bounding boxes and infer spatial relations through numerical\ncomputation, denoted as numerical CoT (‚ÄòNum‚Äô). This method achieves a score of 29.2 on MMSI-Bench,\noutperforming the baseline by 2.8%. It performs well on the attribute subset because 3D object detection\naligns objects in a shared 3D coordinate, avoiding parallax effects in the pixel space (e.g., distant objects\noccupy fewer pixels while nearby objects occupy more).", "start_char": 43286, "end_char": 44844, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_027", "doc_id": "2511.05491v1.pdf", "text": "subset because 3D object detection\naligns objects in a shared 3D coordinate, avoiding parallax effects in the pixel space (e.g., distant objects\noccupy fewer pixels while nearby objects occupy more). However, estimating camera poses across multiple\n11\nModel\nSize\nData\nScale\nSUN RGB-D\nARKitScenes\nAP AP@15 AP@25 AP@50 AR@100 AP AP@15 AP@25 AP@50 AR@100\n3B\n1√ó\n20.2\n30.3\n20.6\n4.5\n33.5\n31.5\n45.1\n34.8\n8.3\n46.6\n3√ó\n26.5\n37.3\n28.9\n7.1\n39.7\n38.0\n51.7\n41.5\n14.3\n53.4\n7B\n1√ó\n24.2\n37.4\n24.6\n5.4\n37.7\n35.0\n48.8\n38.0\n11.3\n50.1\n3√ó\n28.0\n41.6\n29.5\n7.7\n42.1\n39.1\n52.8\n42.3\n15.3\n54.3\n32B\n1√ó\n19.6\n29.5\n19.5\n4.3\n32.7\n31.1\n44.3\n33.7\n8.9\n46.3\n3√ó\n22.5\n33.2\n23.3\n5.1\n36.1\n33.6\n47.6\n35.8\n11.3\n49.1\n3B\n7B\n32B\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\nAP\n+6.3\n+3.8\n+2.9\n+6.5\n+4.1\n+2.5\nSUN 1√ó\nSUN 3√ó\nARK 1√ó\nARK 3√ó\nTable 9 3D object detection results of scaling model and data size\nimages can be difficult, especially when viewpoints vary greatly, which can reduce the accuracy of 3D object\ndetection. Moreover, human beings do not reason about space through direct numerical calculation. Inspired\nby this, we propose reconstructing the scene using text within the CoT (RT-CoT). Compared with Num-CoT,\nRT-CoT handles object, camera, and scene relations much better; for example, on the camera‚Äìobject (CO)\nsubset of MMSI-Bench, it delivers a 3.5% improvement over Num-CoT. Moreover, because current large\nmodels are weak at spatial reasoning, the CoT may misidentify object relations when constructing training\ndata. To address this, we condition CoT generation on a BEV image to improve its accuracy. This yields a\nfurther 1.1% gain. We further add data about camera‚Äìcamera relationships and camera motion to increase\ndata diversity, which pushes the score to 31.7%.", "start_char": 44644, "end_char": 46369, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_028", "doc_id": "2511.05491v1.pdf", "text": "BEV image to improve its accuracy. This yields a\nfurther 1.1% gain. We further add data about camera‚Äìcamera relationships and camera motion to increase\ndata diversity, which pushes the score to 31.7%. CoT Type\nData\nOverall\nPositional Relationship\nAttribute\nMotion\nMSR\nCC\nOO\nRR\nCO\nOR\nCR\nM\nA\nC\nO\n-\n-\n-\n26.4\n22.6\n28.7\n17.3\n39.5\n38.8\n28.9\n20.3\n18.2\n21.6\n28.9\n25.3\nNum-CoT\nOO\n29.2\n28.0\n33.0\n28.4\n40.7\n25.9\n36.1\n39.1\n36.4\n20.3\n31.6\n18.7\nRT-CoT\nOO\n30.0\n35.5\n31.9\n25.9\n44.2\n36.5\n32.5\n32.8\n30.3\n21.6\n27.6\n21.2\nRT-CoTBEV\nOO\n31.1\n31.2\n36.2\n27.2\n46.5\n40.0\n36.1\n26.6\n30.3\n17.6\n30.3\n24.8\nRT-CoTBEV\nMix\n31.7\n35.5\n39.4\n37.0\n44.2\n36.5\n43.4\n28.1\n22.7\n17.6\n30.3\n21.7\nTable 10 Cold-start results for spatial reasoning on the MMSI-Bench [75]. CoT Types: ‚ÄòNum.‚Äô denotes numerical\ncalculation; ‚ÄòRT‚Äô involves layout reconstruction using text from visual input; ‚ÄòRTBEV‚Äô denotes the generated CoT is\ngrounded on the BEV image. Data: OO refers to the object-object subset, while Mix includes all data types. RL facilitates the development of spatial reasoning ability. However, we found that after CoT cold start, the\nmodel performs better without CoT inference than with it. As shown in Table 11, the model achieves 33.6%\naccuracy on MMSI-Bench [75] without CoT inference, surpassing the CoT setting by 1.9%. This suggests\nthe model learns spatial knowledge from the CoT process, but its CoT reasoning ability is weak, leading to\npoorer results. Therefore, to reinforce the spatial reasoning ability, we apply online RL [51] as detailed in\nSection 3.1. After RL training, the model reaches 35.3% on MMSI-Bench [75] when using CoT inference.", "start_char": 46169, "end_char": 47783, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_029", "doc_id": "2511.05491v1.pdf", "text": "ults. Therefore, to reinforce the spatial reasoning ability, we apply online RL [51] as detailed in\nSection 3.1. After RL training, the model reaches 35.3% on MMSI-Bench [75] when using CoT inference. Training\nStage\nCoT\nInference\nOverall\nPositional relationship\nAttribute\nMotion\nMSR\nCC\nOO\nRR\nCO\nOR\nCR\nM\nA\nC\nO\n-\nCold start\n‚úó\n33.6\n43.0\n35.1\n33.3\n43.0\n36.5\n44.6\n32.8\n27.3\n28.4\n32.9\n23.2\nCold start\n‚úì\n31.7\n35.5\n39.4\n37.0\n44.2\n36.5\n43.4\n28.1\n22.7\n17.6\n30.3\n21.7\nRL\n‚úó\n34.7\n34.4\n38.3\n29.6\n53.5\n37.6\n43.4\n25.0\n36.4\n27.0\n40.8\n25.3\nRL\n‚úì\n35.3\n35.5\n38.3\n30.9\n54.6\n35.3\n47.0\n26.6\n48.5\n33.8\n31.6\n22.7\nTable 11 RL results for spatial reasoning on the MMSI-Bench [75]. Ablation for the 3D object detection. We conduct ablation studies on 3D object detection settings in Table 12. The baseline model is trained on a mix of 600K 3D detection samples and 800K general samples. First, lifting\n2D to 3D needs camera intrinsics or FoV, and different devices have different intrinsics, which forces the VLM\n12\nto fit many camera models. To address this, we unify the FoV across datasets with different FoVs. With a\nunified FoV, the baseline drops by 0.1 AP on SUN RGB-D but improves by 2.5 AP on ARKitScenes. Second,\nwe study angle representations for 3D bounding boxes. When we replace Euler angles with quaternions, the\nbaseline performs worse on SUN RGB-D and ARKitScenes. We propose mixing single-turn and multi-turn\ndata in the 3D detection corpus, as we believe multi-turn data helps the VLM build layouts from context and\nlearn spatial information better. When we replace all multi-turn data with the same amount of single-turn\ndata, the model drops by 1.7 AP and 1.8 AP on SUN RGB-D and ARKitScenes, respectively.", "start_char": 47583, "end_char": 49281, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_030", "doc_id": "2511.05491v1.pdf", "text": "ext and\nlearn spatial information better. When we replace all multi-turn data with the same amount of single-turn\ndata, the model drops by 1.7 AP and 1.8 AP on SUN RGB-D and ARKitScenes, respectively. Settings\nSUN RGB-D\nARKitScenes\nAP\nAP@15\nAP@25\nAP@50\nAR@100\nAP\nAP@15\nAP@25\nAP@50\nAR@100\nBaseline\n18.5\n28.5\n19.1\n3.2\n30.7\n29.4\n42.7\n31.8\n7.2\n44.6\nw/o FoV Unification\n18.6\n29.1\n19.2\n2.2\n30.4\n26.9\n40.9\n28.2\n5.4\n42.0\nEuler Angle ‚Äì> Quaternions\n18.3\n29.4\n17.8\n2.8\n30.6\n28.6\n42.4\n30.8\n6.3\n43.9\nSinge-turn Data\n16.8\n26.0\n17.0\n2.7\n28.4\n27.6\n41.9\n29.0\n6.0\n42.5\nTable 12 Ablation study of different settings for 3D object detection. All models are trained on a mixture of 600K 3D\nobject detection samples and 800K general data samples. 3D IoU combined with F1 score serves as an effective accuracy reward for the 3D object detection task. We\nconduct ablation studies on the accuracy reward designed for 3D object detection, as recorded in Table 13. First, we use 3D IoU and recall as the accuracy reward. After RL training, performance drops markedly\nbecause each ground-truth box is matched with too many false-positive predictions. Therefore, we switch to\nIoU and F1 score as the accuracy reward, which yields a 4.2 AP improvement. Accuracy Reward\nAP\nAP@15\nAP@25\nAP@50\nAR@100\nBaseline\n20.2\n30.3\n20.6\n4.5\n33.5\n3D IoU + Recall\n13.8\n20.9\n14.7\n2.6\n33.6\n3D IoU + F1 Score\n24.4\n36.0\n26.3\n5.8\n37.2\nTable 13 Ablation for the accuracy reward on 3D object detection task on SUN RGB-D [54]\n4.4\nExpanding to VLA Model\nVLA Backbone\nSteps\nSpatial\nObject\nGoal\n10\nAVG\nQwen2.5-VL-3B [3]\n2.5K\n56.6\n86.6\n53.8\n15.2\n53.1\nVST-tuned Qwen2.5-VL-3B (ours)\n2.5K\n65.0 (+8.4)\n88.4 (+1.8)\n67.8 (+4.0)\n25.6 (+10.4)\n61.7 (+8.6)\nQwen2.5-VL-3B [3]\n10K\n76.0\n83.8\n72.0\n39.6\n67.9\nVST-tuned Qwen2.5-VL-3B (ours)\n10K\n78.4 (+2.4)\n87.8 (+4.0)\n76.0 (+4.0)\n41.0 (+1.4)\n70.8 (+2.9)\nTable 14 Success rate comparison on the LIBERO benchmark [35]. As detailed in Section 3.2, we adapt our VST-tuned VLM into a VLA model.", "start_char": 49081, "end_char": 51047, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_031", "doc_id": "2511.05491v1.pdf", "text": "10K\n78.4 (+2.4)\n87.8 (+4.0)\n76.0 (+4.0)\n41.0 (+1.4)\n70.8 (+2.9)\nTable 14 Success rate comparison on the LIBERO benchmark [35]. As detailed in Section 3.2, we adapt our VST-tuned VLM into a VLA model. In contrast to the approach used\nby OpenVLA [31], we do not utilize any pre-trained data on robotic learning. Instead, we directly fine-tune\nthe VLM and its action embeddings on the small-scale LIBERO benchmark [35] from scratch for action\nprediction. The results are presented in Table 14. Notably, the VLA model based on the VST-tuned model,\nwhich incorporates spatial knowledge, surpasses the one based on Qwen2.5-VL-3B [3] by an average of 8.6%\nin success rate. Even with a fourfold increase in training steps, the VST-tuned model still demonstrated an\nimprovement of 2.9%. This improvement clearly demonstrates that the integration of spatial knowledge helps\nrobotic learning. 5\nRelated Work\nLarge Vision-Language Models. Recently, Large Vision‚ÄìLanguage Models (LVLMs) have become a pivotal\ntechnology in artificial intelligence, able to understand and integrate information across multiple modalities such\n13\nas text, images, and video. Most LVLMs use the same efficient architecture [3, 13, 20, 33, 37, 62]. Specifically,\na pre-trained vision encoder is employed to extract visual embeddings, which are subsequently projected into\nthe language space via a projector composed of multilayer perceptron (MLP) layers. Various strategies have\nbeen implemented to enhance the performance of VLMs, including the utilization of more advanced vision\nencoders [14, 80], increasing input resolution [13, 36], adopting dynamic resolution techniques [62], refining\nmultimodal positional embeddings [3, 71], and synthesizing high-quality training datasets [16, 33]. Additionally,\nsome works [5, 74] aim to develop LVLMs as unified models that consolidate vision and language capabilities.", "start_char": 50847, "end_char": 52729, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_032", "doc_id": "2511.05491v1.pdf", "text": "embeddings [3, 71], and synthesizing high-quality training datasets [16, 33]. Additionally,\nsome works [5, 74] aim to develop LVLMs as unified models that consolidate vision and language capabilities. Moreover, some studies [29, 45, 70] explore the use of reinforcement learning in the post-training stage to\nenhance the visual reasoning capabilities of LVLMs, an approach inspired mainly by DeepSeek-R1 [26]. The\nmajority of these works apply Generalized Reinforcement Learning from Preference Optimization (GRPO) [51]\nto train LVLMs, achieving significant improvements in many tasks. Large Vision-Language Models for Spatial Understanding and Reasoning. Despite the remarkable progress\nof current LVLMs in visual tasks [38, 39, 44, 67, 79], numerous benchmarks [41, 72, 75] have highlighted\npersistent challenges in spatial understanding and reasoning. To address these issues, SpatialVLM [11]\npioneered the application of VLMs to spatial understanding by constructing VQA datasets using expert\nmodels. Similarly, SpatialRGPT [15] expanded RGB-based spatial understanding to the RGB-D domain by\ngenerating spatial datasets from 3D scene graphs. Recognizing the prohibitive cost of collecting and annotating\nreal-world data, SAT [49] employed simulators to generate training data, thereby extending its focus from\nstatic to dynamic tasks. SpatialBot [9] enables VLMs to invoke external tools for depth estimation, thereby\nimproving their ability to interpret spatial information in input images. Subsequent studies [19, 21, 68, 82] have\nfurther advanced the field by constructing more comprehensive datasets to enhance the spatial understanding\ncapabilities of VLMs. In parallel, another line of research focuses on enhancing the spatial reasoning abilities of\nVLMs. For example, MVoT [65] leverages multimodal representations within its reasoning traces to strengthen\nspatial reasoning.", "start_char": 52529, "end_char": 54417, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_033", "doc_id": "2511.05491v1.pdf", "text": "line of research focuses on enhancing the spatial reasoning abilities of\nVLMs. For example, MVoT [65] leverages multimodal representations within its reasoning traces to strengthen\nspatial reasoning. SpaceR [47] and MindCube [77] incorporate textual cognition maps into their reasoning\ntraces to enhance spatial reasoning, further improving performance through reinforcement learning. Similarly,\nSpatialreasoner [42] performs spatial reasoning by predicting 3D locations and poses as intermediate results. VILASR [64] enhances spatial reasoning by incorporating visual tools and introducing visual prompting into\nthe reasoning process. In contrast to these prior studies, which typically focus exclusively on either spatial\nunderstanding or spatial reasoning, our approach begins with foundational capabilities and builds upon them\nto enhance the model‚Äôs overall reasoning skills. 6\nConclusion\nWe present Visual Spatial Tuning (VST), a general and scalable framework that endows vision-language\nmodels with human-like spatial perception and reasoning abilities. With the large-scale perception data\n(VST-P) and curated reasoning instructions (VST-R), VST effectively acquires spatial awareness without\ndegrading general capabilities. The proposed approach achieves state-of-the-art performance on multiple\nspatial benchmarks, demonstrating that spatial abilities in foundation models can be systematically scaled. Moreover, the Vision-Language-Action (VLA) models have been proven to enhance visuospatial skills, enabling\nmore grounded interaction with the physical world. The generality, scalability, and effectiveness of VST\nhighlight a promising direction toward building physical AI systems that reason and act in space with\nhuman-like intelligence. 14\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774,\n2023.", "start_char": 54217, "end_char": 56202, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_034", "doc_id": "2511.05491v1.pdf", "text": "Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774,\n2023. [2] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron: A\nlarge scale dataset of object-centric videos in the wild with pose annotations. In CVPR, 2021. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang,\nJun Tang, et al. Qwen2. 5-vl technical report. arXiv:2502.13923, 2025. [4] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe,\nDaniel Kurz, Arik Schwartz, et al. Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding\nusing mobile rgb-d data. arXiv:2111.08897, 2021. [5] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Saƒünak\nTa≈üƒ±rlar. Introducing our multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b. [6] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G Shapiro, and Ranjay\nKrishna. Perception tokens enhance visual reasoning in multimodal language models. In CVPR, 2025. [7] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: A\nlarge benchmark and model for 3d object detection in the wild. In CVPR, 2023. [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world\ncontrol at scale. arXiv:2212.06817, 2022. [9] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot:\nPrecise spatial understanding with vision language models. arXiv:2406.13642, 2024.", "start_char": 56002, "end_char": 57890, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_035", "doc_id": "2511.05491v1.pdf", "text": "2022. [9] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot:\nPrecise spatial understanding with vision language models. arXiv:2406.13642, 2024. [10] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv:1709.06158,\n2017. [11] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm:\nEndowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. [12] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. [13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao\nTian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model,\ndata, and test-time scaling. arXiv:2412.05271, 2024. [14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\nZhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. [15] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. NeurIPS, 2024. [16] Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad\nMaaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for\ndetailed visual understanding. arXiv:2504.13180, 2025. [17] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel\nBlistein, Ori Ram, Dan Zhang, Evan Rosen, et al.", "start_char": 57690, "end_char": 59654, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_036", "doc_id": "2511.05491v1.pdf", "text": "l understanding. arXiv:2504.13180, 2025. [17] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel\nBlistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning,\nmultimodality, long context, and next generation agentic capabilities. arXiv:2507.06261, 2025. [18] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie√üner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. [19] Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin\nEichner, Yinfei Yang, Afshin Dehghan, et al. Mm-spatial: Exploring 3d spatial understanding in multimodal llms. arXiv:2503.13111, 2025. 15\n[20] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza\nSalehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for\nstate-of-the-art vision-language models. In CVPR, 2025. [21] Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei,\nTianshuo Yang, Min Dou, et al. Internspatial: A comprehensive dataset for spatial reasoning in vision-language\nmodels. arXiv:2506.18385, 2025. [22] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin\nWang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv:2505.20279, 2025. [23] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu\nMa, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, 2024. [24] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:\nElevating the role of image understanding in visual question answering. In CVPR, 2017.", "start_char": 59454, "end_char": 61401, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_037", "doc_id": "2511.05491v1.pdf", "text": "024. [24] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:\nElevating the role of image understanding in visual question answering. In CVPR, 2017. [25] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. [26] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948, 2025. [27] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang,\nJiawei Wang, et al. Seed1. 5-vl technical report. arXiv:2505.07062, 2025. [28] Mary Hegarty, Daniel R Montello, Anthony E Richardson, Toru Ishikawa, and Kristin Lovelace. Spatial abilities\nat different scales: Individual differences in aptitude-test performance and spatial-layout learning. Intelligence, 34\n(2):151‚Äì176, 2006. [29] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv:2503.06749, 2025. [30] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram\nis worth a dozen images. In ECCV, 2016. [31] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov,\nEthan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv:2406.09246, 2024. [32] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):\n83‚Äì97, 1955. [33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu,\nand Chunyuan Li.", "start_char": 61201, "end_char": 63173, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_038", "doc_id": "2511.05491v1.pdf", "text": "ssignment problem. Naval research logistics quarterly, 2(1-2):\n83‚Äì97, 1955. [33] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu,\nand Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv:2408.03326, 2024. [34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. [35] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking\nknowledge transfer for lifelong robot learning. NeurIPS, 2023. [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In\nCVPR, 2024. [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024. [38] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. [39] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu,\nLianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China\nInformation Sciences, 67(12):220102, 2024. 16\n[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [41] Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso M de Melo, and Alan Yuille. 3dsrbench: A\ncomprehensive 3d spatial reasoning benchmark. arXiv:2412.07825, 2024. [42] Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, and Alan Yuille. Spatialreasoner:\nTowards explicit and generalizable 3d spatial reasoning. arXiv:2504.20024, 2025. [43] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. arXiv:2203.10244, 2022.", "start_char": 62973, "end_char": 64959, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_039", "doc_id": "2511.05491v1.pdf", "text": ", 2025. [43] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. arXiv:2203.10244, 2022. [44] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In\nWACV, 2021. [45] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai\nWang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale\nreinforcement learning. CoRR, 2025. [46] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Jun Zhang. Total3dunderstanding:\nJoint layout, object pose and mesh reconstruction for indoor scenes from a single image. In CVPR, 2020. [47] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer:\nReinforcing mllms in video spatial reasoning. arXiv:2504.01805, 2025. [48] Jean Piaget. Child‚Äôs Conception of Space: Selected Works vol 4. Routledge, 2013. [49] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha\nKembhavi, Bryan A Plummer, Ranjay Krishna, et al. Sat: Dynamic spatial aptitude training for multimodal\nlanguage models. arXiv:2412.07755, 2024. [50] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan,\nRuss Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene\nunderstanding. In ICCV, 2021. [51] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv:2402.03300, 2024. [52] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and\nChuan Wu. Hybridflow: A flexible and efficient rlhf framework. In EuroSys, 2025.", "start_char": 64759, "end_char": 66722, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_040", "doc_id": "2511.05491v1.pdf", "text": "00, 2024. [52] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and\nChuan Wu. Hybridflow: A flexible and efficient rlhf framework. In EuroSys, 2025. [53] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In CVPR, 2019. [54] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark\nsuite. In CVPR, 2015. [55] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv:2405.09818, 2024. [56] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv:2312.11805, 2023. [57] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien\nVincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of\ntokens of context. arXiv:2403.05530, 2024. [58] MiMo-VL Team. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. [59] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng\nLang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv:2402.12289, 2024. [60] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula,\nShusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: A fully open, vision-centric\nexploration of multimodal llms. NeurIPS, 2024. 17\n[61] Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun\nChen, Sizhe Yang, et al. Grutopia: Dream general robots in a city at scale. arXiv:2407.10943, 2024.", "start_char": 66522, "end_char": 68413, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_041", "doc_id": "2511.05491v1.pdf", "text": "Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun\nChen, Sizhe Yang, et al. Grutopia: Dream general robots in a city at scale. arXiv:2407.10943, 2024. [62] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang,\nWenbin Ge, et al. Qwen2-vl: Enhancing vision-language model‚Äôs perception of the world at any resolution. arXiv:2409.12191, 2024. [63] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai\nChen, Tianfan Xue, et al. Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai. In\nCVPR, 2024. [64] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing\nspatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv:2506.09965, 2025. [65] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Mind‚Äôs eye of llms:\nvisualization-of-thought elicits spatial reasoning in large language models. NeurIPS, 2024. [66] x.ai. Grok-1.5 vision preview, 2024. URL https://x.ai/blog/grok-1.5v. [67] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun\nCheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in\nreal computer environments. NeurIPS, 2024. [68] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and\nKevin J Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv:2505.17015, 2025. [69] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and\nKevin J Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv:2505.17015, 2025.", "start_char": 68213, "end_char": 70107, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_042", "doc_id": "2511.05491v1.pdf", "text": "Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and\nKevin J Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv:2505.17015, 2025. [70] Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai\nTong, et al. Mixed-r1: Unified reward perspective for reasoning capability in multimodal large language models. arXiv:2505.24164, 2025. [71] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. [72] Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How\nmultimodal large language models see, remember, and recall spaces. In CVPR, 2025. [73] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth\nanything v2. NeurIPS, 2024. [74] Rui Yang, Lin Song, Yicheng Xiao, Runhui Huang, Yixiao Ge, Ying Shan, and Hengshuang Zhao. Haplovl: A\nsingle-transformer baseline for multi-modal understanding. arXiv:2503.14694, 2025. [75] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong\nDuan, Xiangyu Yue, et al. Mmsi-bench: A benchmark for multi-image spatial intelligence. arXiv:2505.23764,\n2025. [76] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie√üner, and Angela Dai. Scannet++: A high-fidelity dataset of\n3d indoor scenes. In ICCV, 2023. [77] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan\nChandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. arXiv:2506.21458,\n2025. [78] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong\nLiu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv:2503.14476, 2025.", "start_char": 69907, "end_char": 71863, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_043", "doc_id": "2511.05491v1.pdf", "text": "hang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong\nLiu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv:2503.14476, 2025. [79] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning\nbenchmark for expert agi. In CVPR, 2024. [80] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 18\n[81] Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys, and Shuaicheng Liu. Holistic 3d scene\nunderstanding from a single image with implicit representation. In CVPR, 2021. [82] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue\nCai, Guowei Huang, et al. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv:2503.22976, 2025. [83] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Llava-video: Video\ninstruction tuning with synthetic data. Transactions on Machine Learning Research, 2025. [84] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie\nSu, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal\nmodels. arXiv:2504.10479, 2025. [85] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer\nfor 3d vision and text alignment. In ICCV, 2023. [86] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker,\nAyzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL,\n2023. 19\nAppendix\nA\nMore Implementation Details\nA.1\nCoordinate\nCamera coordinate.", "start_char": 71663, "end_char": 73599, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_044", "doc_id": "2511.05491v1.pdf", "text": "n Welker,\nAyzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL,\n2023. 19\nAppendix\nA\nMore Implementation Details\nA.1\nCoordinate\nCamera coordinate. We define the camera coordinate system based on the right-hand rule. The camera\ncenter is taken as the origin, with the X-axis pointing to the right (parallel to the image plane), the Y-axis\npointing downward (also parallel to the image plane), and the Z-axis pointing forward along the optical axis. Bounding box coordinate. Within the defined camera system, a 3D bounding box is specified by its center\ncoordinates (x, y, z), its size (xl, yl, zl), and its orientation (pitch, yaw, roll). In our convention, the X-\ndimension corresponds to the front‚Äìback size of the box, Y represents the vertical extent, and Z denotes the\nlateral (side) extent. In our definition of a 3D bounding box, the X-axis points to the front, the Y-axis points\ndownward, and the Z-axis points sideways. The rotation angles are defined as the transformations from the\ncamera axes to the box axes, measured in degrees and normalized by œÄ (i.e., divided by 180‚ó¶). The center\nand dimension are given in meters. All values are rounded to two decimal places. A.2\nInstruction Format\nThe CoT Format\n<|im_start|>system\nYou are a helpful assistant. You should first think about the reasoning process in the mind and then\nprovide the user with the answer. The reasoning process is enclosed within <think> </think> tags,\ni.e. <think> reasoning process here </think> answer here.<|im_end|>\n<|im_start|>user\n<|vision_start|>image.jpg<|vision_end|>{question}<|im_end|>\n<|im_start|>assistant\n<think>{thinking content}</think> {answer}<|im_end|>\nA.3\nFoV Unification\nWe show the process of FoV unification in Algorithm 1. We resize the input image according to a pre-defined\nfocal length fnew.", "start_char": 73399, "end_char": 75253, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_045", "doc_id": "2511.05491v1.pdf", "text": "<think>{thinking content}</think> {answer}<|im_end|>\nA.3\nFoV Unification\nWe show the process of FoV unification in Algorithm 1. We resize the input image according to a pre-defined\nfocal length fnew. This approach ensures that all images fed into the VLM for the 3D object detection\ntask share a uniform focal length, thereby mitigating the potential challenges arising from disparate camera\nparameters when training on mixed datasets. Algorithm 1 FoV Unification\n1 # I:\nInput\nimage\nwith\nsize (W, H)\n2 # f:\nFocal\nlength of the\noriginal\ncamera\n3 # f_new: Unified\nfocal\nlength\n4 # hfov\nand\nwfov\nare the\nhorizontal\nand\nvertical\nfields of view. 5 hfov = 2 * atan(W / (2 * f))\n6 wfov = 2 * atan(H / (2 * f))\n7 W_new = 2 * f_new * tan(hfov / 2)\n8 H_new = 2 * f_new * tan(wfov / 2)\n9 # Resize\nthe\noriginal\nimage\n10 I_new = resize(I, (W_new , H_new))\nA.4\nSettings\nVLM. We summarize the training details in Table 15, 16, and 17. VLA. We select four task suites on the LIBERO benchmark [35]: LIBERO-Spatial, LIBERO-Object, LIBERO-\nGoal, and LIBERO-Long (LIBERO-10). In line with the methodology of OpenVLA [31], we preprocess\n20\nModel Size Optimizer\nLR\nViT LR Packing length Global batch size Warmup ratio Epoch\n3B\nAdamW\n5.00e-5\n5.00e-6\n16384\n128\n0.03\n1\n7B\nAdamW\n5.00e-5\n5.00e-6\n16384\n128\n0.03\n1\n32B\nAdamW\n5.00e-5\n5.00e-6\n16384\n128\n0.03\n1\nTable 15 Summary of training parameters used in the SFT stage. Model Size Optimizer\nLR\nViT LR Packing length Global batch size Warmup ratio Epoch\n3B\nAdamW\n1.00e-5\n1.00e-6\n16384\n128\n0.03\n2\n7B\nAdamW\n1.00e-5\n1.00e-6\n16384\n128\n0.03\n2\n32B\nAdamW\n1.00e-5\n1.00e-6\n16384\n128\n0.03\n2\nTable 16 Summary of training parameters used in the CoT cold start stage. Model Size Optimizer\nLR\nViT LR Max length Global batch size Rollout number\nKL\nClip_low Clip_high\n3B\nAdamW\n1.00e-6\n1.00e-6\n8192\n128\n5\ndisable\n0.2\n0.28\n7B\nAdamW\n1.00e-6\n1.00e-6\n8192\n128\n5\ndisable\n0.2\n0.28\n32B\nAdamW\n1.00e-6\n1.00e-6\n8192\n128\n5\ndisable\n0.2\n0.28\nTable 17 Summary of training parameters used in the RL stage.", "start_char": 75053, "end_char": 77048, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_046", "doc_id": "2511.05491v1.pdf", "text": "e-6\n8192\n128\n5\ndisable\n0.2\n0.28\n7B\nAdamW\n1.00e-6\n1.00e-6\n8192\n128\n5\ndisable\n0.2\n0.28\n32B\nAdamW\n1.00e-6\n1.00e-6\n8192\n128\n5\ndisable\n0.2\n0.28\nTable 17 Summary of training parameters used in the RL stage. the dataset by filtering out \"no-operation\" actions and unsuccessful demonstrations to accommodate the\nsingle-step policy. The visual input is a third-person camera image, resized to a resolution of 256 √ó 256 pixels. To evaluate performance, we execute 50 independent rollouts for each task and compute the average success\nrate. Simulator. We use Isaac Sim-4.2 to generate the 3D bounding box of a scene. B\nMore Results\nData scaling of spatial foundational tasks. To enable the VLM to perceive the positions of objects in\n3D space, we selected monocular 3D object detection and depth estimation as our foundational tasks. We\nthen incrementally scaled the volume of training data to validate the VLM‚Äôs emerging spatial perception\ncapabilities. We use Qwen2.5-VL-3B [3] as a baseline model. As shown in Figure 5a, the AP@15 on the\nSUN RGB-D [54] and ARKitScenes [4] datasets progressively improved as the amount of 3D detection data\nincreased, demonstrating its ability to learn how to perceive the 3D spatial positions of objects from visual\ninput. Furthermore, by gradually introducing depth-related data, we discovered that the VLM could learn to\njudge the relative distances between objects and the camera, even with a comparatively small amount of data,\nas illustrated in Figure 5b. C\nMore Data Engine Details\nC.1\nPrompting with BEV Annotation\nIn the process of generating spatial reasoning data for object relationships, we augment the object information\nextracted from multi-view images with a visualized BEV image as additional contextual input, as demonstrated\nin Table 18. The corresponding prompt is detailed in Table 19.", "start_char": 76848, "end_char": 78679, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_047", "doc_id": "2511.05491v1.pdf", "text": "gment the object information\nextracted from multi-view images with a visualized BEV image as additional contextual input, as demonstrated\nin Table 18. The corresponding prompt is detailed in Table 19. Given the current limitations of large VLMs\nin spatial perception, incorporating a BEV image as an auxiliary input enhances the model to comprehend\nspatial relationships among objects within RGB images. This integration facilitates more accurate caption\ngeneration and enables more effective text-based reasoning. 21\n0K\n200K\n400K\n600K\n800K\n1600K\nNumber of 3D Detection Data\n0\n10\n20\n30\n40\n50\n60\n70\nPerformance\nAP@15-SUN\nAP@15-ARK\nS-AVG\nMM-AVG\n(a) Data scaling of 3D detection data. 0K\n200K\n400K\n600K\n800K\n1600K\nNumber of Depth Data\n55\n60\n65\n70\n75\n80\n85\n90\n95\nPerformance\nCV-3D\nBLINK-D\nS-AVG\nMM-AVG\n(b) Data scaling of depth-related data. Figure 5 Data scaling of spatial foundational tasks. C.2\nDataset Visualization\nDepth comparison. The depth-related data are visualized in Table 20 and Table 21. We have devised multiple\nrepresentation formats to refer to objects, including visual box, visual point, textual box, textual point, and\ntext. For data involving points, we utilize the mask center to ensure that the point is located on the object. Additionally, we incorporate samples to estimate the relative depth at randomly selected points. Distance Comparison. The distance-related data are presented in Table 22, where objects are referenced using\nvisual boxes or points. 3D Object Detection. The 3D object detection data are presented in Table 23. We employ 9-DoF (Degrees of\nFreedom) 3D bounding boxes, with further details provided in Section A.1. All images are resized using the\nmethod described in Algorithm 1 to ensure a unified field of view (FoV). Measurement. Measurement data includes information related to object height and size, as presented in\nTable 24. Due to ambiguities in specifying the width and length of objects, we only annotate the height.", "start_char": 78479, "end_char": 80447, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_048", "doc_id": "2511.05491v1.pdf", "text": "ement. Measurement data includes information related to object height and size, as presented in\nTable 24. Due to ambiguities in specifying the width and length of objects, we only annotate the height. Otherwise, we assign the maximum value among width, height, and length. Additionally, the data is organized\nin a multi-turn format, as we aim for the model to learn the relative relationships between objects from\nmulti-turn data. Scene caption. Scene caption data for single-view and multi-view scenarios are presented in Table 25. The\ncaptions are generated based on the prompt outlined in Table 26. In the generation process, we incorporate\ndetailed object information‚Äîincluding category, spatial center, and size‚Äîas well as object relationships\nderived from the scene graph, to enrich the contextual understanding. Correspondence. The correspondence data are presented in Table 27. Given a reference visual point, the model\nis tasked with identifying the corresponding visual point in the second image. To obtain the correspondence\npoints, we begin with the ground truth instance masks of a point cloud. For each object instance, we randomly\nsample a set of N points from its surface. Each sampled 3D point is then projected into the 2D image plane\nto determine its visibility. A point is considered a valid correspondence and thus ‚Äúvisible‚Äù only if it satisfies all\nof the following criteria:\n1. Image Boundary Check: The projected 2D coordinates (u, v) must lie within the image boundaries, i.e.,\n0 ‚â§u < W and 0 ‚â§v < H, where W and H are the width and height of the image, respectively. Points\nprojecting outside these bounds are discarded. 2. Depth Validity Check: The point‚Äôs depth value in the camera‚Äôs coordinate system must be positive\n(z > 0), ensuring it is in front of the camera plane. 3. Occlusion Check: The point must not be occluded by other geometry.", "start_char": 80247, "end_char": 82117, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_049", "doc_id": "2511.05491v1.pdf", "text": ": The point‚Äôs depth value in the camera‚Äôs coordinate system must be positive\n(z > 0), ensuring it is in front of the camera plane. 3. Occlusion Check: The point must not be occluded by other geometry. We verify this by comparing the\npoint‚Äôs ground true depth, zgt, with the depth value zdepth recorded in the depth map at its projected\n22\nPrompting with BEV annotation\nPrompt\n<system prompt> <ground-truth> <question>\nIf the small metal trash can on the floor near the white storage cabinets in the first image is north of the small\nprinter on the white cabinet near the door in the second image, what direction is the clear plastic bottle on\nthe white desk with two monitors in the second image from the printer? Options: A. southeast, B. northeast,\nC. south, D. northwest\nTeacher\nmodel\nFirst, I look at the first image. The scene has a long row of white storage cabinets along the left wall, a white\ndesk with shelves holding folders on the right, and a small metal trash can sitting on the floor between the\ncabinets and the desk. There‚Äôs also a coat hanger in the corner and some boxes stacked nearby. The trash\ncan is clearly positioned on the left side of the room, close to the storage cabinets. Next, I examine the second image, which was taken from a closer, more forward angle-like the camera moved\ntoward the desk and tilted up slightly. The desk is now in the foreground, holding two black monitors, a\nkeyboard, a mouse, a telephone, and a clear plastic bottle. Against the back wall near the door, there‚Äôs a small\nprinter on a white cabinet. The door and coat hanger from the first image are still visible in the background,\nbut the trash can isn‚Äôt present here. Putting both images together, the room‚Äôs layout makes sense: the left wall has storage cabinets, the center/right\nhas a desk, and the back wall has a door.", "start_char": 81917, "end_char": 83748, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_050", "doc_id": "2511.05491v1.pdf", "text": "round,\nbut the trash can isn‚Äôt present here. Putting both images together, the room‚Äôs layout makes sense: the left wall has storage cabinets, the center/right\nhas a desk, and the back wall has a door. The trash can (from image 1) is on the left side near the cabinets, the\nprinter (from image 2) is on the back right near the door, and the bottle (from image 2) is on the foreground\ndesk. The question says the trash can is north of the printer. Since the trash can is on the left side of the room and\nthe printer is on the back right, \"north\" from the printer‚Äôs perspective must be toward the left-toward the\ntrash can. Now, I need to figure out where the bottle is relative to the printer. The bottle is on the desk,\nwhich is in the foreground of the second image. From the printer‚Äôs position (back right near the door), the\ndesk is in front of it-closer to the camera. The bottle is on the part of the desk that‚Äôs to the left of the printer\n(since the desk extends from the foreground to the back wall, and the printer is on a cabinet against the back\nwall to the right of the desk). So, from the printer‚Äôs view: the bottle is to the left (the same direction as north, since north is toward the\ntrash can on the left) and forward (since the desk is closer than the trash can). Combining these, the bottle is\nin the northwest direction from the printer. That matches option D. Table 18 An example to illustrate the prompting with BEV annotation. 2D location. The point is deemed occluded (and thus invisible) if the relative depth difference exceeds a\nthreshold of 5%:\n|zgt‚àízdepth|\nzgt\n> 0.05. Points that pass this check are considered visible. Only the points that successfully pass all three checks are retained as the final set of valid correspondence\npoints. 3D Object Detection-MI. We adopt the same 3D bounding box representation as used in single-image 3D\nobject detection tasks. Note that we designate the camera coordinate system of the first frame as the world\ncoordinate system.", "start_char": 83548, "end_char": 85540, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_051", "doc_id": "2511.05491v1.pdf", "text": "adopt the same 3D bounding box representation as used in single-image 3D\nobject detection tasks. Note that we designate the camera coordinate system of the first frame as the world\ncoordinate system. Object-Object Relationship. Object-related data is visualized in Table 28. We select pairs of images that\ncontain at least one common object (object-level correspondence), as well as objects that are unique to a\nsingle image. Anchor objects are defined as those present exclusively in one image and whose center point\nheight difference is less than one meter. For these anchor objects, we formulate questions pertaining to spatial\ndirection and distance. Direction-related queries necessitate a reference direction: given objects A, B, and C,\nthe vector from object A to object B serves as the reference direction, and we examine the spatial relationship\nbetween object A and object C accordingly. Alternatively, the camera position may substitute for object A as\n23\nYou are an expert in 3D annotation. Given a set of images, ground-truth camera poses, ground-truth objects, and a specific\nquestion-answer pair, your task is to describe the given objects, rewrite the question and generate a chain-of-thought for the answer. # Basic Information\nCoordinate System:\n- Origin: The first image‚Äôs camera viewpoint at (0,0,0)\n- Axes:\n‚ÜíX: Rightward (horizontal)\n‚ÜìY: Downward (vertical, perpendicular to the ground)\n‚ÜóZ: Forward\n# Objects:\nEach object is annotated with a class name, 3D position (x_center, y_center, z_center), size (x_size, y_size, z_size), and in_-\nframes flag. * x_center, y_center, z_center: Center position of the object in the frame1 coordinate system, in meters. If x_center is positive, it\nmeans the object is to the right of the camera in frame1. If y_center is positive, it means the object is below the camera in frame1. If\nz_center is positive, it means the object is in front of the camera in frame1.", "start_char": 85340, "end_char": 87262, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_052", "doc_id": "2511.05491v1.pdf", "text": "t is to the right of the camera in frame1. If y_center is positive, it means the object is below the camera in frame1. If\nz_center is positive, it means the object is in front of the camera in frame1. * x_size, y_size, z_size: The dimensions of the object along the (XY Z) axes, in meters, when the rotation angles are zero. y_size\nis the height. * in_frames: A list indicating whether the object is present in each frame. The length of this list should match the number of frames. For instance, if an object is present in frame1 and frame2, the in_frames list will be [True, True]. If it is only present in frame1, the\nlist will be [True, False]. # Input and Output format\nThe input is a set of images, ground-truth camera pose and 3D bounding boxes, question type, question and answer. The output should be a triplet in json format, containing a revised question, a chain-of-thought process, and an answer:\n{\n‚Äôquestion‚Äô: revised_question,\n‚Äôthought‚Äô: generated_thought,\n‚Äôanswer‚Äô: given_answer\n}\n# Requirement\nWhen revising the question, you must describe each object using a concise caption that reflects its spatial position, material, shape,\nrelationship with the environment, and other relevant features. Use these short captions to refer to objects in the question instead of\ntheir ‚Äôlabel‚Äô directly. Your reasoning should be presented in the first person perspective, a detailed, step-by-step manner, forming a logical chain of thought\nthat leads to the answer. This process must incorporate visual content from both image-1 and image-2, describing the entire scene. Structure your response in four parts:\n1. Describe the visual content of the first image, providing your initial observations of the scene. 2. Describe the visual content of the second image, emphasizing correspondences with the first image and noting any new elements or\nchanges. If feasible, also describe the second image‚Äôs camera rotation and translation relative to the first image‚Äôs camera. 3.", "start_char": 87062, "end_char": 89033, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_053", "doc_id": "2511.05491v1.pdf", "text": "zing correspondences with the first image and noting any new elements or\nchanges. If feasible, also describe the second image‚Äôs camera rotation and translation relative to the first image‚Äôs camera. 3. Summarize the overall layout of the scene as inferred from the two images and the TOP VIEW image. Then, provide a detailed,\nlogical reasoning process based on your observations. 4. Summarize your answer. Always use the TOP VIEW image to ensure your spatial descriptions are consistent with the overall scene layout, but do not explicitly\nreference the TOP VIEW image in your wording. Ensure all spatial information in your descriptions aligns with what is visualized in the TOP VIEW image. Do NOT refer to objects using ‚Äôcolor point‚Äô or similar terms, as these are not used in real-world visualizations. Do NOT include any numerical calculations. Do NOT include any coordinate-related information and their values, such as `x-axis`, `z-axis`. Do NOT use phrases like √†ccording to ground-truth√≤r similar expressions. The question must be clear and free of any thought guidance. # Examples and question type description\nThese are examples for reference. You can freely play with them. {few_shot_examples}\n# Input\nThese two images were taken consecutively, recording information about the same scene:\nimage-1:<|image_pad|>\nimage-2:<|image_pad|>\nGround-truth camera poses are: {camera_info}\nGround-truth objects are: {object_info}\nThe TOP VIEW of the scene described by the two images:<|image_pad|>\nQuestion type is camera-object orientation: {question}\nThe calculation process is for your reference (You don‚Äôt have to use it): {text_orientation_process}\nNow, generate response following task description, input and output format, and examples. Please strictly follow the json format. Use\ndifferent phrasings or styles to ask the questions while maintaining consistency with the intent and structure of the examples. Table 19 Prompt used in the prompting with bev annotation.", "start_char": 88833, "end_char": 90805, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_054", "doc_id": "2511.05491v1.pdf", "text": "n format. Use\ndifferent phrasings or styles to ask the questions while maintaining consistency with the intent and structure of the examples. Table 19 Prompt used in the prompting with bev annotation. {camera_info} is the camera motion information,\n{object_info} will be replaced by object information (category, center, and size), and {text_orientation_process}\nis the textual calculation process. 24\nthe reference point. Camera-Camera Relationship. As shown in Table 29, similar to the object-object data, we select pairs of\nimages that share at least one common object (i.e., object-level correspondence), as well as images containing\nobjects unique to a single frame. The objective is to enable the model to infer the spatial relationship between\nthe camera positions in the two images, based on their respective layouts. Camera Motion. As shown in Table 30, we divide the camera motion into rotation and translation movements. The rotation category encompasses the following motions: panning to the right, panning to the left, tilting\nupward, tilting downward, rolling clockwise, and rolling counterclockwise. The translation category includes:\nrightward, leftward, upward, downward, forward, and backward movements. It is important to note that the\nprecise definitions of rotation and translation will be provided within the specific context. Video Object-object Relationship. We present video data related to object-object relationships in Table 31,\nwhich includes information on object distances and object direction types. Furthermore, we organize the data\ninto a multi-turn format, enabling the model to learn object-relative relationships within the context. This\napproach also enhances training efficiency, as it mitigates the computational overhead associated with loading\nvideo data. Video Counting Data. We present video counting data in Table 32. Video Spatiotemporal Data. We present video spatiotemporal data in Table 33. General Reasoning Data.", "start_char": 90605, "end_char": 92568, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_055", "doc_id": "2511.05491v1.pdf", "text": "ssociated with loading\nvideo data. Video Counting Data. We present video counting data in Table 32. Video Spatiotemporal Data. We present video spatiotemporal data in Table 33. General Reasoning Data. We select data from LLaVA-Onevision [33] that can be verified through predefined\nrules and categorize it into three types: math, OCR, and knowledge. Each sample is inferred four times using\nthe Qwen2.5-VL [3], and we filter out samples that are either entirely correct or entirely incorrect, retaining\nonly those with partial correctness. Subsequently, the selected samples are inferred using the Seed1.5-VL [27],\nduring which we record the reasoning process. Since this reasoning often contains repetitive patterns (e.g.,\nrepeated use of the term \"wait\"), we further utilize Seed1.5-VL [27] to rewrite and refine the recorded reasoning\nsteps, enhancing their clarity. Spatial Reasoning data. For spatial reasoning data concerning the relationships between objects and cameras,\nwe employ prompting with BEV annotations (refer to Section C.1) to facilitate the generation of the reasoning\nprocess. For data related to camera motion, we utilize the prompts outlined in Table 34 and Table 35, which\ninstruct the model to leverage the parallax effect when reasoning about the types of camera motion. The\ngenerated samples are shown in Table 36 and Table 37. 25\nDepth-related Data\nPrompt\nTell me the depth relationship of the objects of A.sink, B.shelves, C.blinds, D.lamp. Answer\nThe objects from near to far is A, D, B, C\nPrompt\nThere are several boxes in the image: box-A, box-B. Each box represents an object. Present the object\nrepresented by these boxes in an order that goes from close to far and give their names. Answer\nbox-A (chair), box-B (picture)\nPrompt\nYou are given several 2D bounding boxes in the image: [3, 132, 98, 468] [322, 83, 475, 481] Arrange the object\nrepresented by these boxes from the farthest to the nearest based on their depth relationship and give their\nnames.", "start_char": 92368, "end_char": 94357, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_056", "doc_id": "2511.05491v1.pdf", "text": "ounding boxes in the image: [3, 132, 98, 468] [322, 83, 475, 481] Arrange the object\nrepresented by these boxes from the farthest to the nearest based on their depth relationship and give their\nnames. Output the sorted bboxes and labels using JSON format. Answer\n[\n{\"bbox_2d\":[324,81,477,475],\"label\":\"person\"},\n{\"bbox_2d\":[3,130,98,462],\"label\":\"person\"}\n]\nPrompt\nAfter reviewing the image, which object‚Äîtable (cyan box), window-A (orange box), window-B (pink box)\nand door (purple box)‚Äîwould you say is nearer to the camera? Answer\ntable\nTable 20 Examples of the depth-related data. 26\nDepth-related Data\nPrompt\nBased on the image, arrange the object represented by these sentences in a list from the closest to the farthest. 1. the chair with the cat laying in it 2. hanging plant sitting on ground\nAnswer\n1. the chair with the cat laying in it, 2. hanging plant sitting on ground\nPrompt\nThe image contains red point-A and point-B. Please decide which point is closer to the camera. Answer\npoint-A\nTable 21 Examples of the depth-related data. Distance-related Data\nPrompt\nConsidering the positions, which object‚Äîthe toilet-A (orange box) or the toilet-B (green box)‚Äîdo you think\nis farthest from the door (purple box)? Answer\ntoilet-A\nPrompt\nEstimate the real distances and identify which object‚Äîthe pillow-A (orange point) or the pillow-B (purple\npoint)‚Äîis closest to the chair (yellow point). Answer\nThe distance relationships are: Distance[chair, pillow-A]=3.51m Distance[chair, pillow-B]=3.67m So, the\nanswer is pillow-A. Table 22 Examples of the distance-related data. 27\n3D Object Detection Data\nPrompt\nHere are the detailed camera parameters for the image. Camera intrinsic parameters: Horizontal fov, hfov=69.16, and vertical fov, vfov=53.17. Image width=959 and\nheight=696. We do not consider distortion parameters here. Camera coordinate: X-axis points rightward, Y-axis points downward, and Z-axis points forward. The origin\npoint is the camera location.", "start_char": 94157, "end_char": 96125, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_057", "doc_id": "2511.05491v1.pdf", "text": "9 and\nheight=696. We do not consider distortion parameters here. Camera coordinate: X-axis points rightward, Y-axis points downward, and Z-axis points forward. The origin\npoint is the camera location. We take the camera coordinate system as the world coordinate system. 3D bounding box format: [x_center, y_center, z_center, x_size, y_size, z_size, pitch, yaw, roll]\n* x_center, y_center, z_center: the center of the object in the camera coordinate, in meters. z_center is the\ndepth of the object in space. * x_size, y_size, z_size: The dimensions of the object along the ( XYZ ) axes, in meters, when the rotation\nangles are zero. * pitch, yaw, roll: Euler angles representing rotations around the X, Y, and Z axes, respectively. Each angle is\nnormalized to the range of (-1, 1) and is multiplied by 180 to convert it into degrees. Output a json list where each entry contains the object name in \"label\" and its 3D bounding box in \"bbox_3d\". Detect the 3D bounding boxes of printer. Answer\n[\n{\"bbox_3d\":[-0.16,0.12,1.56,0.44,0.51,0.41,0.11,0.28,0.05],\"label\":\"printer\"},\n{\"bbox_3d\":[0.40,-0.02,1.96,0.45,0.51,0.36,0.11,0.27,0.05],\"label\":\"printer\"}\n]\nPrompt\nDetect the 3D bounding boxes of table. Answer\n[\n{\"bbox_3d\":[0.19,0.56,1.78,0.53,0.52,1.36,0.1,0.26,0.04],\"label\":\"table\"}\n]\nTable 23 Examples of 3D object detection data. 28\nMeasurement-related Data\nPrompt\nCould you provide the height of the table (specified at the pink point-1)? Specify the measurement in\ncentimeters. Answer the question using a single word or phrase. Answer\n71 cm\nPrompt\nWhat is the length of the dimension with the maximum value (length, width, or height) of the chair (signified\nby the brown point-A) in meters? Please try to answer the question with short words or phrases if possible. Answer\n0.97 m\nPrompt\nWhat is the length of the largest dimension (length, width, or height) of the window (signified by the orange\npoint-B) in centimeters? Answer\n2031 cm\nTable 24 Examples of the measurement-related data.", "start_char": 95925, "end_char": 97915, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_058", "doc_id": "2511.05491v1.pdf", "text": "What is the length of the largest dimension (length, width, or height) of the window (signified by the orange\npoint-B) in centimeters? Answer\n2031 cm\nTable 24 Examples of the measurement-related data. 29\nScene Caption\nPrompt\nAnalyze the spatial organization and positional relationship in this image. Answer\nStarting from the camera‚Äôs viewpoint, the nearest object is a chair (orange point-2) with a depth of 1.72m. Adjacent to it, behind, is another chair (orange point-3) at 2.72m depth. Moving forward, a table (blue\npoint-1) spans 3.00m depth. To the table‚Äôs left, chairs (orange points-4,5,6) are aligned at depths 3.20m,\n3.63m, and 4.11m respectively. On the table‚Äôs right, chairs (orange points-7,8) are aligned, with chair (orange\npoint-7) beside chair (orange point-8). Chair (orange point-6) is in front of chair (orange point-8). Distances:\nchair (orange point-2) is 1.79m from the table; chair (orange point-3) is 1.58m from the table. Chairs (orange\npoints-3,4,5,6) form a left-aligned row. Chairs (orange points-6,7,8) form a right-aligned row. Chair (orange\npoint-8) is 0.52m from chair (orange point-7). Prompt\nThe observer‚Äôs line of sight is preserved as images are captured one by one. Describe these two frames in\ndetail. Answer\nThe first image shows a room scene with a whiteboard on the left wall, a chair in the lower central area, a\nwindow on the right side with a view of a building outside, a heater below the window, a box on the windowsill,\nsockets on the wall near the heater, and a light switch on the wall. Key holders with colorful keys are attached\nto the whiteboard. The second image, taken with the camera rotated right from the first perspective, shows\npart of the window and the heater still visible. New elements include a trash can on the floor, a table partially\nin view, and additional sockets. Objects present in the first image such as the whiteboard, light switch, and\nthe box on the windowsill are no longer in view, while the chair remains present.", "start_char": 97715, "end_char": 99708, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_059", "doc_id": "2511.05491v1.pdf", "text": "artially\nin view, and additional sockets. Objects present in the first image such as the whiteboard, light switch, and\nthe box on the windowsill are no longer in view, while the chair remains present. Table 25 Examples of the scene caption data. 30\nYou are a 3D scene understanding expert tasked with generating ego-centric spatial descriptions for the given image. Coordinate System\n- Origin: Camera viewpoint at (0,0,0)\n- Axes:\nX (Rightward, horizontal)\nY (Downward, vertical)\nZ (Depth, forward)\nInput Data Specification\nFor each object:\n- name: \"object (color point-x)\"\n- size: [X-width, Y-height, Z-length] in meters\n- centroid: (x, y, z) where z is depth from image plane\n- distance_to_camera: distance from object center to camera, in meters\nRelationships include:\n1. Binary relationships: [Object-A, Object-B, spatial_relation] (direct positional relationships)\n2. Multi-object relationships: [[Object-A, Object-B, Object-C...], spatial_relation] (group spatial relationships)\n3. Distance between two objects in 3D space: [Object-A, Object-B, \"distance = X.XX m\"] (quantitative supplements)\nScene information\nCamera: center_x = 0, center_y = 0, center_z (depth) = 0\n{object_info}\nOutput Guidelines\n1. Depth Handling:\n- Explicitly state depth comparisons using meter values (e.g., \"1.2 m closer\")\n- Use \"closer/farther\" for relative depth comparisons\n2. Spatial Relations:\n- You can refer to \"Binary relationships\" and \"Multi-object relationships\"\n- You can also summarize from the original data:\nX-axis: absolute delta x > 0.5 m implies clear left/right\nY-axis: absolute delta y > 0.3 m implies taller/shorter (Y-axis)\nZ-axis: absolute delta z > 1 m implies clear in front of/behind\n3. Relationship Coverage:\n- 100 percent coverage of provided binary/multi-object relationships\n- Include more than 70 percent of distance metrics (prioritize distances less than 3 m)\n4.", "start_char": 99508, "end_char": 101383, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_060", "doc_id": "2511.05491v1.pdf", "text": "ront of/behind\n3. Relationship Coverage:\n- 100 percent coverage of provided binary/multi-object relationships\n- Include more than 70 percent of distance metrics (prioritize distances less than 3 m)\n4. Conflict Resolution:\n- Prioritize explicit relationship declarations over coordinate calculations\n- Use distance metrics only for quantitative enhancement, not directional correction\n5. Viewpoint Consistency:\n- All directional descriptions must be strictly from camera perspective\n- Never use absolute coordinates except for Z-axis depth values\n6. Structural Flow:\n- Camera to nearest object to depth progression (describe the depth value if necessary)\n- Anchor object (table) to surrounding objects\n- Group formations to individual outliers\n7. Format Requirements (MUST follow):\n- Output 4 to 20 sentences\n- Plain text only (no markdown)\n- Strictly preserve object names (exact \"object (color point-x)\" format)\n- When making numerical references, specify the name of the numerical relationship, for example, (depth = x m). Unit is required. - Prohibited phrases: \"according to the coordinates\", \"as shown in the data\", axis terminology, \"The multi - object relationships\nshow\", \"Binary relationships:\"\n- Prohibited raw data regurgitation, relationship lists\n- When referring to object quantity, count the objects in the image and text instead of referring to the point index. Now, generate a caption for this scene based on the reference image and the above information, enclosed in <caption> </caption>\nTable 26\nPrompt used in the scene caption generation. {object_info} will be replaced by object information\n(category, center, and size) and relationships extracted from the scene graph. 31\nCorrespondence Data\nPrompt\nThe first image shows a point circled in gold. After adjusting the camera or lighting, the second image presents\nseveral gold-circled points labeled ‚ÄôA, B, C, D‚Äô. Which matches the original?", "start_char": 101183, "end_char": 103095, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_061", "doc_id": "2511.05491v1.pdf", "text": "ta\nPrompt\nThe first image shows a point circled in gold. After adjusting the camera or lighting, the second image presents\nseveral gold-circled points labeled ‚ÄôA, B, C, D‚Äô. Which matches the original? Options: A: point-A, B: point-B,\nC: point-C, D: point-D\nAnswer\nB: point-B\nTable 27 Examples of the correspondence data. Object-related Data\nPrompt\nIf the small white cabinet under the white desk is north of the black monitor on the left side of the desk,\nwhat direction is the chair on the right side of the room from the black monitor? Options: A. southeast B. north C. south D. southwest\nAnswer\nA. southeast\nPrompt\nIf, from the camera position of the first image, the direction toward the hanging jacket (visible in the first\nimage) is north, then in which direction does the window (visible in the second image) lie relative to the first\nimage‚Äôs camera? Options: A. northeast, B. southeast, C. east, D. south\nAnswer\nA. northeast\nTable 28 Examples of the object-related data. 32\nCamera-related Data\nPrompt\nThe frames are acquired in a continuous sequence from a first-person perspective. If the first picture was\ntaken with the camera facing west, what is the direction for the second picture? Options: A. southeast, B. north, C. south, D. northwest\nAnswer\nD. northwest\nPrompt\nImages are shot one after another from a first-person perspective. When positioned at the second photo spot,\nhow is the first camera placed relative to me? Options: A. right, B. back, C. front, D. front right\nAnswer\nB. back\nTable 29 Examples of the camera-related data. Camera Motion Data\nPrompt\nThe frames are captured in a continuous manner from a first-person perspective. You are to determine the\nmain direction in which the camera is translated, disregarding small shakes or jitters and concentrating on the\noverall intentional movement. Which way is the camera‚Äôs perspective moving? Options: A. moving backward,\nB. moving rightward and forward, C. moving backward and upward, D. moving leftward\nAnswer\nB.", "start_char": 102895, "end_char": 104885, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_062", "doc_id": "2511.05491v1.pdf", "text": "he\noverall intentional movement. Which way is the camera‚Äôs perspective moving? Options: A. moving backward,\nB. moving rightward and forward, C. moving backward and upward, D. moving leftward\nAnswer\nB. moving rightward and forward\nPrompt\nThe visual narrative unfolds through a series of images, all from a first-person angle. If we‚Äôre only considering\nhorizontal rotation, does the camera pan left or right from image one to image two? Options: A. panning to\nthe left, B. panning to the right\nAnswer\nA. panning to the left\nTable 30 Examples of the camera motion data. 33\nObject-Object Video Data\nPrompt\nDuring the course of this video, which of the objects flowerpot, vase, chandelier or stool is closest to the light? Options: A. flowerpot, B. stool, C. chandelier, D. vase\nAnswer\nC. chandelier\nPrompt\nConsider the scene in the video. You are positioned at shoe, with your gaze fixed on basket. In which direction\nis bucket? Options: A. back, B. back-left, C. left, D. front-right\nAnswer\nD. front-right\nPrompt\nAssuming you are at bucket and looking at basket, determine the location of shoe relative to you. Options: A. back-left, B. front-right, C. right, D. front\nAnswer\nA. back-left\nTable 31 Examples of the object-object video data. Video Counting Data\nPrompt\nHow many chairs can you spot in this part of the video? Answer the question using a single word or phrase. Answer\n8\nPrompt\nHow many stools show up in the video? Answer\n1\nTable 32 Examples of the video counting data. Video Spatiotemporal Data\nPrompt\nProvide the appearance order for the initial sighting of these objects within the video: trash bin, pillar, table\ndivider. Options: A. pillar, trash bin, table divider B. table divider, pillar, trash bin C. trash bin, table\ndivider, pillar\nAnswer\nC. trash bin, table divider, pillar\nPrompt\nArrange the given objects based on the timestamp of their first appearance in the video: cabinet, trash bin,\ntable divider. Options: A. table divider, trash bin, cabinet B.", "start_char": 104685, "end_char": 106660, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_063", "doc_id": "2511.05491v1.pdf", "text": "able divider, pillar\nPrompt\nArrange the given objects based on the timestamp of their first appearance in the video: cabinet, trash bin,\ntable divider. Options: A. table divider, trash bin, cabinet B. cabinet, trash bin, table divider C. trash bin,\ncabinet, table divider\nAnswer\nB. cabinet, trash bin, table divider\nTable 33 Examples of the video spatiotemporal data. 34\nYou are an expert in 3D spatial reasoning and camera motion analysis. Your task is to analyze a pair of images that represent a\nsequence of camera movements. Given a specific question-answer pair about the camera‚Äôs motion, you will first rewrite the question to\nbe more precise and then generate a detailed, step-by-step chain-of-thought reasoning to justify the provided answer. # Coordinate System:\n- Origin: The first image‚Äôs camera viewpoint at (0,0,0)\n- Axes:\n‚ÜíX: Rightward (horizontal)\n‚ÜìY: Downward (vertical, perpendicular to the ground)\n‚ÜóZ: Forward (along the camera‚Äôs viewing direction)\n# Camera Motion Types (Rotations)\n1. Pan (Yaw): Rotation around the vertical axis. This is like turning your head from side to side. * Pan right: The camera rotates to its right. From the camera‚Äôs perspective, the visual content appears to shift to the left. Objects on\nthe left side of the frame may move out of view, while new objects on the right side may enter the frame. * Pan left: The camera rotates to its left. The visual content appears to shift to the right. 2. Tilt (Pitch): Rotation around the horizontal axis. This is like nodding your head up or down. * Tilt Up: The camera points upwards. From the camera‚Äôs perspective, the scene appears to shift downwards. Objects originally in\nthe center may move towards the bottom of the frame, and new content may appear at the top. * Tilt Down: The camera points downwards. The scene appears to shift upwards. 3. Roll: Rotation around the forward viewing direction. This is like tilting your head to touch your ear to your shoulder.", "start_char": 106460, "end_char": 108415, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_064", "doc_id": "2511.05491v1.pdf", "text": "* Tilt Down: The camera points downwards. The scene appears to shift upwards. 3. Roll: Rotation around the forward viewing direction. This is like tilting your head to touch your ear to your shoulder. * Roll Clockwise: The camera rotates clockwise. From the camera‚Äôs perspective, the entire scene and all objects within it will appear\nto rotate counter-clockwise. You should pay more\n* Roll Counter-clockwise: The camera rotates counter-clockwise. The scene will appear to rotate clockwise. # Input and Output format\nThe input is a set of images, question type, question, and answer. The output should be a triplet in JSON format, containing a revised question, a chain-of-thought process, and an answer:\n{{\n‚Äôquestion‚Äô: revised_question,\n‚Äôthought‚Äô: generated_thought,\n‚Äôanswer‚Äô: given_answer\n}}\n# Requirement\nTo generate high-quality Question, Answer, and Reasoning triplets. The generated content must be coherent, logical, and adhere to\nthe specified format and persona. 1. Rewrite the Provided Question\n* Your primary task is to rephrase the original, user-provided question. * The rewritten question must be clear, specific, and well-formulated. It should eliminate any ambiguity or vagueness present in the\noriginal. * The core intent and subject matter of the original question must be preserved. Do not change the topic. 2. For the answer, provide an accurate answer that directly corresponds to the **rewritten question**. The question and answer\nshould be match. 3. Construct a Detailed Reasoning Process\n* This is a critical component. You must articulate the step-by-step thought process that leads from the question to the answer. * The reasoning should be logical, transparent, and demonstrate how you arrived at the final answer. It should explain *why* you\nchose certain information, how you connected different concepts, and the structure of your final response.", "start_char": 108215, "end_char": 110092, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_065", "doc_id": "2511.05491v1.pdf", "text": "transparent, and demonstrate how you arrived at the final answer. It should explain *why* you\nchose certain information, how you connected different concepts, and the structure of your final response. * Your reasoning should be presented from a first-person perspective (e.g., \"First, I will examine...\", \"I can see that...\")\n4. Embrace Creativity and Expansion\n* You are encouraged to be creative and expand on the given basic knowledge. Do NOT include markdown format. Do NOT include any numerical calculations. Do NOT include any coordinate-related information and their values, such as ‚Äòx-axis‚Äò, ‚Äòy-axis‚Äò , ‚Äòz-axis‚Äò, ‚ÄòX-axis‚Äò, ‚ÄòY-axis‚Äò , ‚ÄòZ-axis‚Äò. Do NOT use phrases like ‚Äòaccording to ground-truth‚Äò or similar expressions. # Example\nThese are some examples but don‚Äôt feel constrained by the few-shot examples; you have creative freedom. {few_shot_examples}\n# Input\n{input}\nNow, generate response following the task description, input and output format, and examples. Please strictly follow the JSON format. Use different phrasings or styles to ask the questions while maintaining consistency with the intent and structure of the examples. Table 34 Prompt used in the generation of camera rotation reasoning data. {few_shot_examples} will be replaced\nby manually verified samples. {input} is the question and answer. 35\nYou are an expert in 3D spatial reasoning and camera motion analysis. Your task is to analyze a pair of images that represent a\nsequence of camera movements. Given a specific question-answer pair about the camera‚Äôs motion, you will first rewrite the question to\nbe more precise and then generate a detailed, step-by-step chain-of-thought reasoning to justify the provided answer. # Coordinate System:\n- Origin: The first image‚Äôs camera viewpoint at (0,0,0)\n- Axes:\n‚ÜíX: Rightward (horizontal)\n‚ÜìY: Downward (vertical, perpendicular to the ground)\n‚ÜóZ: Forward (along the camera‚Äôs viewing direction)\n# Parallax Effect:\nThe parallax effect is the cornerstone of your analysis.", "start_char": 109892, "end_char": 111887, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_066", "doc_id": "2511.05491v1.pdf", "text": "ghtward (horizontal)\n‚ÜìY: Downward (vertical, perpendicular to the ground)\n‚ÜóZ: Forward (along the camera‚Äôs viewing direction)\n# Parallax Effect:\nThe parallax effect is the cornerstone of your analysis. It is the apparent displacement of an object when viewed from different lines of\nsight. - Key Insight: Objects closer to the camera (foreground) will appear to move more significantly against the background than objects\nfarther away. # Camera Motion Types (Translation)\n1. Translation along Z-axis (Forward/Backward):\n- Moving Forward: All objects appear larger in appearent size. Foreground objects expand significantly more than background objects. Objects appear to move radially outward from the center of the view. - Moving Backward: All objects appear smaller in appearent object size. Foreground objects shrink significantly more than background\nobjects. Objects appear to move radially inward toward the center of the view. When referring to ‚Äôobject size,‚Äô it is important to distinguish between ‚Äôapparent object size‚Äô and ‚Äôreal object size in 3D space.‚Äô This is\nbecause the apparent size of an object changes as the camera moves, while its real size in 3D space remains constant. 2. Translation along X-axis (Left/Right):\n- Moving Rightward: Foreground objects appear to shift significantly to the LEFT relative to the background. - Moving Leftward: Foreground objects appear to shift significantly to the RIGHT relative to the background. 3. Translation along Y-axis (Up/Down):\n- Moving Upward: Foreground objects appear to shift significantly DOWNWARD relative to the background. - Moving Downward (+Y): Foreground objects appear to shift significantly UPWARD relative to the background. # Input and Output format\nThe input is a set of images, question type, question, and answer.", "start_char": 111687, "end_char": 113479, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_067", "doc_id": "2511.05491v1.pdf", "text": "- Moving Downward (+Y): Foreground objects appear to shift significantly UPWARD relative to the background. # Input and Output format\nThe input is a set of images, question type, question, and answer. The output should be a triplet in JSON format, containing a revised question, a chain-of-thought process, and an answer:\n{{\n‚Äôquestion‚Äô: revised_question,\n‚Äôthought‚Äô: generated_thought,\n‚Äôanswer‚Äô: given_answer\n}}\n# Requirement\nTo generate high-quality Question, Answer, and Reasoning triplets. The generated content must be coherent, logical, and adhere to\nthe specified format and persona. 1. Rewrite the Provided Question\n* Your primary task is to rephrase the original, user-provided question. * The rewritten question must be clear, specific, and well-formulated. It should eliminate any ambiguity or vagueness present in the\noriginal. * The core intent and subject matter of the original question must be preserved. Do not change the topic. 2. For the answer, provide an accurate answer that directly corresponds to the rewritten question. The question and answer should be\nmatch. 3. Construct a Detailed Reasoning Process\n* This is a critical component. You must articulate the step-by-step thought process that leads from the question to the answer. * The reasoning should be logical, transparent, and demonstrate how you arrived at the final answer. It should explain *why* you\nchose certain information, how you connected different concepts, and the structure of your final response. * Your reasoning should be presented from a first-person perspective (e.g., \"First, I will examine...\", \"I can see that...\")\n* When referring to ‚Äôobject size,‚Äô it is important to distinguish between ‚Äôapparent object size‚Äô and ‚Äôreal object size in 3D space.‚Äô This\nis because the apparent size of an object changes as the camera moves, while its real size in 3D space remains constant. 4. Embrace Creativity and Expansion\n* You are encouraged to be creative and expand on the given basic knowledge.", "start_char": 113279, "end_char": 115266, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_068", "doc_id": "2511.05491v1.pdf", "text": "n object changes as the camera moves, while its real size in 3D space remains constant. 4. Embrace Creativity and Expansion\n* You are encouraged to be creative and expand on the given basic knowledge. Do NOT include markdown format. Do NOT include any numerical calculations. Do NOT include any coordinate-related information and their values, such as ‚Äòx-axis‚Äò, ‚Äòy-axis‚Äò , ‚Äòz-axis‚Äò, ‚ÄòX-axis‚Äò, ‚ÄòY-axis‚Äò , ‚ÄòZ-axis‚Äò. Do NOT use phrases like ‚Äòaccording to ground-truth‚Äò or similar expressions. # Example\nThese are some examples but don‚Äôt feel constrained by the few-shot examples; you have creative freedom. {few_shot_examples}\n# Input\n{input}\nNow, generate response following the task description, input and output format, and examples. Please strictly follow the JSON format. Use different phrasings or styles to ask the questions while maintaining consistency with the intent and structure of the examples. Table 35 Prompt used in the generation of camera translation reasoning data. {few_shot_examples} will be replaced\nby manually verified samples. {input} is the question and answer. 36\nSpatial Reasoning for Camera Motion\nPrompt\nYou take two consecutive photos (Image 1 and Image 2) from a first-person perspective. How does the camera\nmove in space? Options: A. forward and left, B. upward, C. backward, leftward and downward, D. leftward,\nbackward and upward\nAnswer\n<think>1. Start by comparing the two images to identify changes in object visibility and apparent size:\n-In Image 1, a large machine is on the right side, and the left-side workbench shows fewer detailed objects. In Image 2, that machine is no longer visible, while the left-side workbench has more detailed items and\nappears more prominent. 2. Analyze the apparent size of the workbench: -The workbench in Image 2 has\nlarger-appearing objects (like the blue cylinder and tools) with more details. When a camera moves closer to\nan object, the object‚Äôs apparent size increases.", "start_char": 115066, "end_char": 117013, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_069", "doc_id": "2511.05491v1.pdf", "text": "e workbench: -The workbench in Image 2 has\nlarger-appearing objects (like the blue cylinder and tools) with more details. When a camera moves closer to\nan object, the object‚Äôs apparent size increases. So this change implies the camera moved forward towards the\nworkbench. 3. Evaluate the disappearance of the machine: -The machine that was on the right in Image 1\nis not in Image 2. For an object on the right to leave the frame, the camera must have shifted to the left. This leftward movement means the machine is no longer within the camera‚Äôs field of view. 4. Combine the\nobservations: -The increased apparent size of the workbench points to forward movement, and the machine‚Äôs\ndisappearance from the right indicates leftward movement. These two movements match option A.</think>\nA. forward and left\nTable 36 Examples of the spatial reasoning data for camera translation. Spatial Reasoning for Camera Motion\nPrompt\nTwo consecutive photos show a scene from a first-person view. When considering only horizontal rotation\n(pan), determine if the camera pans to the right or left from the first photo to the second. Options: A. Panning to the right, B. Panning to the left\nAnswer\n<think>1. First, I need to recall what pan right and pan left mean. When the camera pans right, from its\nperspective, the scene shifts to the left. This means objects on the left side of the first frame may move out of\nview, and new objects on the right side may appear. When the camera pans left, the scene shifts to the right,\nwith right-side objects moving out and left-side new objects appearing. 2. Now, I‚Äôll compare the two images. In image-1, on the left side, there‚Äôs a bright area with what seems to be a doorway or sliding door structure. In image-2, this left-side doorway structure is no longer visible. That indicates that the part of the scene\nthat was on the left in image-1 has moved out of the frame, which is a sign of a rightward pan. 3. Next, look\nat the right side.", "start_char": 116813, "end_char": 118779, "source_path": "data\\2511.05491v1.pdf"}
{"chunk_id": "2511.05491v1.pdf__chunk_070", "doc_id": "2511.05491v1.pdf", "text": "ructure is no longer visible. That indicates that the part of the scene\nthat was on the left in image-1 has moved out of the frame, which is a sign of a rightward pan. 3. Next, look\nat the right side. In image-1, the cabinet is partially visible on the right. In image-2, more of the cabinet‚Äôs\nright side is shown, including a dark sliding door and additional greenery (plants) on the far right. These\nare new elements that entered the frame, which happens when the camera pans right and reveals more of\nthe right-hand side of the scene. 4. Putting this together: the disappearance of the left-side doorway and\nthe appearance of new right-side elements match the behavior of a camera panning to the right. Because\nwhen the camera pans right, the scene shifts left from the camera‚Äôs view, causing left-side objects to exit and\nright-side objects to enter.</think> A. Panning to the right\nTable 37 Examples of the spatial reasoning data for camera rotation. 37", "start_char": 118579, "end_char": 119537, "source_path": "data\\2511.05491v1.pdf"}
